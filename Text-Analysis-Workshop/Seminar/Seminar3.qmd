---
title: '研讨会3: 语言模型A：监督式文本分类学习'

---

# 研讨会3: 语言模型A：监督式文本分类学习

讲座人：Hanxu hanxu.dong.21@ucl.ac.uk

<p>
  <a href="https://github.com/leondong98/Text-Analysis-Workshop/raw/main/Data/nhs_reviews.Rdata" 
     style="display:inline-block; background-color:#4CAF50; color:white; padding:10px 20px; text-decoration:none; margin-right:10px; border-radius:5px;">
    Seminar data
  </a>
    
## 3.1 对 NHS 评论进行分类

政府提供的医疗服务让民众有多满意/不满意？关于这个问题的一个有趣的数据来源是文本数据，因为医疗服务的使用者通常可以对他们在接受政府提供的医疗服务时的体验进行评论和反馈。

在本次研讨课中，我们将使用朴素贝叶斯模型（Naive Bayes）来预测英国患者对全科诊所（doctors’ surgeries）的评论是正面的还是负面的。为此，我们将使用一份患者对居住地附近英国国家医疗服务体系（NHS）设施的评论语料库。
    
## 3.1.1 Packages
在开始研讨会时，先下载/加载以下 R 包：
    
```r
library(tidyverse)
library(quanteda)

# 如果无法加载 quanteda.textmodels 包，请运行以下代码
# 仅适用于 Windows 用户，Mac 用户请查看下方注释说明
# remotes::install_github("quanteda/quanteda.textmodels")

library(quanteda.textmodels)

# 如果无法加载 caret 包，请运行以下代码
# install.packages("caret")

library(caret)
```
    
## 3.1.2 Mac 用户加载 ``quanteda.textmodels`` 遇到问题的说明：

对于 Mac 用户，当前使用 ``install.packages()`` 或 ``remotes::install_github()`` 安装 ``quanteda.textmodels`` 包可能会遇到问题。因此，你需要使用以下（稍显繁琐的）替代方案：

1. 第一步：安装 ``gfortran``
在你的计算机上安装 ``gfortran``，你可以从以下链接找到与你系统匹配的版本：🔗 https://github.com/fxcoudert/gfortran-for-macOS/releases 如果提示你没有完整的编译工具集，请按照提示安装 Xcode 命令行工具。
2. 第二步：在 R 中运行以下代码：
    
```r
dir.create('~/.R')
file.create('~/.R/Makevars')
```
    
3. 第三步：定位 Makevars 文件。要定位相关文件，请通过 Finder 前往你的主目录（你可以打开 Finder 然后按下 ⇧⌘H）。如果你看不到 .R 文件夹，首先需要按下 ⇧⌘.（Shift + Command + 点号），以显示你 Mac 上的隐藏文件。找到 .R 文件夹并打开，在其中应该可以看到一个名为 Makevars 的文件。

   完成后，将以下内容复制粘贴到该文件中并保存：

```swift
FC = /opt/homebrew/Cellar/gcc/14.2.0/bin/gfortran
F77 = /opt/homebrew/Cellar/gcc/14.2.0/bin/gfortran
FLIBS = -L/opt/homebrew/Cellar/gcc/14.2.0/lib/gcc/11
```
    
4. 第四步：现在重启 R。

5. 第五步：接着运行以下代码：

```r
remotes::install_github("quanteda/quanteda.textmodels")
```

## 3.1.3 数据

我们今天的研讨会将使用以下数据集：

**NHS 患者评论** – ``nhs_reviews.Rdata``
该文件包含了英国各地 2000 条关于 NHS 全科诊所的患者评论。数据中包含以下变量：


| 变量名            | 说明                                                                 |
|-------------------|----------------------------------------------------------------------|
| `review_title`     | 患者评论的标题                                                       |
| `review_text`      | 患者评论的正文                                                       |
| `star_rating`      | 患者给出的星级评分（满分为 5 星）                                     |
| `review_positive`  | 分类变量：若评分为 3 星及以上则为 `"Positive"`，否则为 `"Negative"` |
| `review_date`      | 评论的日期                                                           |
| `gp_response`      | 分类变量：表示医生是否回复了该评论。若回复则为 `"Responded"`，否则为 `"Has not responded"` |

一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：

```r
load("nhs_reviews.Rdata")
```
    
与以往一样，你可以使用 ``tidyverse`` 软件包中的 ``glimpse()`` 函数快速查看数据中的变量：

```r
glimpse(nhs_reviews)
```
```
Rows: 2,000
Columns: 6
$ review_title    <chr> "Great service", "Friendly helpful staff", "Excellent …
$ review_text     <chr> "Phoned up to book a appointment receptionist very hel…
$ star_rating     <dbl> 4, 5, 5, 1, 1, 5, 5, 5, 5, 1, 3, 5, 2, 5, 2, 5, 1, 1, …
$ review_positive <fct> Positive, Positive, Positive, Negative, Negative, Posi…
$ review_date     <date> 2021-10-13, 2021-07-26, 2021-09-18, 2021-06-19, 2021-…
$ gp_response     <chr> "Has not responded", "Responded", "Has not responded",…  
```

## 3.1.4 随机数生成

本次研讨课需要你随机生成训练集和测试集。任何涉及随机数生成的过程，都会导致你每次运行代码时可能得到不同的结果。例如，重新运行代码时，被分配到测试集和训练集的观测值可能会发生变化。

因此，为了使结果完全可重复，你需要首先使用 ``set.seed()`` 函数。该函数的参数是一个单独的整数值。一旦你设置了seed，R 每次运行时都会生成相同的随机数序列，从而保证结果一致:

```r
set.seed(12345)
```
我在这里使用了 12345，但你可以选择任何你喜欢的数值。如果你选择了不同的数字，那么你的结果可能会与我的略有不同。不过，只要你在每次运行 R 脚本时执行这一行代码，你每次都会得到相同的结果！

## 3.2 训练集与测试集

1. 在 ``nhs_reviews`` 数据中创建一个新变量，用于指示每条观测属于训练集还是测试集。确保大约 80% 的观测被分配到训练集，20% 被分配到测试集，将该变量命名为 ``train``：

<details>
<summary>Reveal Code</summary>

```r
nhs_reviews$train <- sample(x = c(TRUE, FALSE), 
                            size = nrow(nhs_reviews), 
                            replace = TRUE, 
                            prob = c(.8, .2))
```  

> ``sample()`` 函数会从提供给 ``x`` 参数的元素中随机抽样。在这里，我们是从一个只有两个元素的向量 ``TRUE`` 和 ``FALSE`` 中进行抽样。

> - ``size`` 参数指定从 ``x`` 中要随机选择的元素数量，这里我们要求抽样的数量与 ``nhs_reviews`` 数据中的观测值数量相同；
> - ``replace`` 参数指定是否放回抽样（这里我们要进行放回抽样）；
> - ``prob`` 参数接受一个向量，表示我们希望以何种概率抽取 ``x`` 中的每个值。

</details> 
    
2. 将 ``nhs_reviews`` 数据转换为语料库```corpus``，然后转换为文档-特征矩阵``DFM``, 并进行一定的特征选择:

<details>
<summary>Reveal Code</summary>

```r
# 转换为语料库
nhs_reviews_corpus <- corpus(nhs_reviews, text_field = "review_text")

# 转换为 DFM，并移除英文停用词
nhs_reviews_dfm <- nhs_reviews_corpus %>% 
  tokens() %>%
  dfm() %>%
  dfm_remove(stopwords("en"))
```  

> 请记住，特征选择没有唯一“正确”的方式。在这里，我只是使用了unigram（单词）表示，并移除了停用词。你可以根据实际任务做出不同的选择。

</details> 

3. 使用 ``dfm_subset()`` 函数将你的 DFM 拆分为训练集 DFM 和 测试集 DFM。
使用 ``dim()`` 函数确认它们是否具有你预期的行数和列数:

<details>
<summary>Reveal Code</summary>

```r
# 子集为训练集观测
nhs_reviews_dfm_train <- dfm_subset(nhs_reviews_dfm, train)

# 子集为测试集观测（使用逻辑非 !）
nhs_reviews_dfm_test <- dfm_subset(nhs_reviews_dfm, !train)
```  

> 这里我使用了逻辑运算符 ``!``，它会反转 ``train`` 向量的 ``TRUE`` 和 ``FALSE`` 值。也就是说，``train`` 中为 ``TRUE`` 的元素在加上 ``!`` 后变为 ``FALSE``，为 ``FALSE`` 的元素变为 ``TRUE``。

```r
dim(nhs_reviews_dfm_train)
``` 
```
[1] 1578 7862  
```

```r
dim(nhs_reviews_dfm_test)
``` 
```
[1]  422 7862
```

> 我们的 DFM 拆分结果中，训练集和测试集包含不同数量的文档（这是符合预期的，因为我们之前设定了 80% 用于训练，20% 用于测试），但它们包含相同数量的特征（这也是必须的，因为我们要使用这些特征来为测试集生成预测）。

</details> 
    

## 3.3 词典（Dictionaries）

1. 使用 ``dictionary()`` 函数构建一个简单的字典，其中包含一些你认为能够体现患者评论中积极情绪的词汇。将这个字典应用到你之前创建的训练集 DFM上，并创建一个二元变量：当文本中包含来自该字典的词时设为 Positive，否则设为 Negative:

<details>
<summary>Reveal Code</summary>

```r
positive_dictionary <- dictionary(list(positive = c("great", "excellent", "fantastic", "thank")))

# 将字典应用于训练集 DFM
nhs_dictionary_dfm_train <- dfm_lookup(x = nhs_reviews_dfm_train,
                                       dictionary = positive_dictionary)

# 创建一个逻辑（TRUE/FALSE）向量，对每条训练文本进行标记
predicted_positive <- as.numeric(nhs_dictionary_dfm_train[,1]) > 0

# 将逻辑向量转换为字符向量，TRUE 赋值为 "Positive"，FALSE 赋值为 "Negative"
# 并将结果赋值给训练集数据
nhs_dictionary_dfm_train$predicted_positive_dictionary <- ifelse(predicted_positive, "Positive", "Negative")
```  

</details> 

    
2. 使用 ``table()`` 函数创建一个混淆矩阵(confusion matrix)，用于比较通过字典度量预测的正面评论 (predicted_positive_dictionary) 与正面评论的真实编码（存储在 ``review_positive`` 变量中）。将 ``table()`` 函数的输出保存为一个新对象:
    
<details>
<summary>Reveal Code</summary>
    
```r
confusion_dictionary <- table(predicted_classification = nhs_dictionary_dfm_train$predicted_positive_dictionary,
                              true_classification = nhs_dictionary_dfm_train$review_positive)

confusion_dictionary
```  
```
                        true_classification
predicted_classification Negative Positive
                Negative      542      569
                Positive       36      421
```

</details> 
  
    
3. 使用 ``caret`` 包中的 ``confusionMatrix()`` 函数来计算关于你的字典分类器的多种性能统计量。（你还应设置 ``positive`` 参数为 "``Positive``"，以告知 R 哪个结果水平对应于“正面”结果。）预测的准确率是多少？灵敏度是多少？特异性是多少？解释这些数值。它们告诉我们这个字典在分类正面患者评论方面的性能如何？

<details>
<summary>Reveal Code</summary>
    
```r
confusion_dictionary_statistics <- confusionMatrix(confusion_dictionary, 
                                                   positive = "Positive")

confusion_dictionary_statistics
```  

```
Confusion Matrix and Statistics

                        true_classification
predicted_classification Negative Positive
                Negative      542      569
                Positive       36      421
                                          
               Accuracy : 0.6142          
                 95% CI : (0.5895, 0.6383)
    No Information Rate : 0.6314          
    P-Value [Acc > NIR] : 0.9247          
                                          
                  Kappa : 0.3045          
                                          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.4253          
            Specificity : 0.9377          
         Pos Pred Value : 0.9212          
         Neg Pred Value : 0.4878          
             Prevalence : 0.6314          
         Detection Rate : 0.2685          
   Detection Prevalence : 0.2915          
      Balanced Accuracy : 0.6815          
                                          
       'Positive' Class : Positive     
```

> 我们的字典预测的准确率为 61%。虽然特异性很高——达到 94%——但这是因为字典将绝大多数评论都归为非正面评论，因此正确地为真实的非正面评论分配了该标签。灵敏度则低得多，仅为 43%。也就是说，我们只正确地识别了不到一半的真实“正面”评论。

> 这是可以理解的，因为我们上面使用的字典非常简短，因此很可能漏掉了许多能表示正面情绪的词汇。
    
> 我们可以尝试通过修改字典词列表并重新测试来提升这些分数，但接下来我们将使用朴素贝叶斯分类模型，让模型学习哪些词与正面或负面患者评论最密切相关。

</details> 
    

## 3.4 朴素贝叶斯（Naive Bayes）

1. 使用你之前创建的训练集 DFM，通过 ``textmodel_nb()`` 函数来估计一个针对 ``review_positive`` 结果变量的朴素贝叶斯模型。

该模型有三个主要参数：


| 参数名    | 说明                                                                 |
|-----------|----------------------------------------------------------------------|
| `x`       | 用于训练朴素贝叶斯模型的 DFM（文档-特征矩阵）                         |
| `y`       | 要预测的结果向量（例如，此处为 `review_positive`）                  |
| `prior`   | 对结果变量 `y` 的各类别的先验分布形式。通常设为 `"docfreq"`，即每个类别的先验概率将等于该类别在训练数据中的相对频率 |

<details>
<summary>Reveal Code</summary>
    
```r
nb_train <- textmodel_nb(x = nhs_reviews_dfm_train, 
                         y = nhs_reviews_dfm_train$review_positive,
                         prior = "docfreq")
```

</details> 

2. 检查你所估计的朴素贝叶斯模型中的类别条件词概率（class-conditional word probabilities）。（此处使用 ``sort()`` 函数会很有帮助。）你可以使用 ``coef()`` 函数对估计得到的模型对象进行调用来访问这些概率。思考以下问题：在每个类别（正面 / 负面）下，哪些词的出现概率最高？请注意，如果你在研讨会一开始做了不同的特征选择决策，你得到的词列表可能会有所不同。特别是，如果你没有移除停用词（stopwords），你会发现两个类别下都对这些词赋予了高概率 —— 这也是合理的，因为这些词在所有患者评论中都很常见。

<details>
<summary>Reveal Code</summary>
    
```r
head(sort(coef(nb_train)[,"Positive"], decreasing = T), 20)
```
```
          .           ,       staff    practice     surgery appointment 
0.075829687 0.037050475 0.009988262 0.009241276 0.008921140 0.007619251 
     always          gp     service           !        time     helpful 
0.006061253 0.006061253 0.005997225 0.005890513 0.005762459 0.005719774 
      thank      doctor        care        well     doctors    friendly 
0.005463664 0.004652652 0.004545940 0.004481912 0.004183118 0.004076406 
  reception         get 
0.003670900 0.003606872    
```
    
```r
head(sort(coef(nb_train)[,"Negative"], decreasing = T), 20)
```
```
           .            ,  appointment          get         call      surgery 
 0.067243057  0.032460640  0.012294357  0.011874226  0.009021758  0.008667964 
           !         told        phone     practice         time           gp 
 0.008093048  0.007584468  0.007473908  0.006147178  0.006036618  0.005771272 
      doctor          day         back          one          can         just 
 0.004908898  0.004555103  0.004333982  0.004245533  0.004134973  0.004134973 
appointments            ? 
 0.004112860  0.004046524    
```

> 这些词大体上是合理的。在正面和负面评论中，患者都可能使用与医疗相关的词汇，例如 “staff”（工作人员）、“practice”（诊所）、“surgery”（手术/诊所）、“appointment”（预约）、“doctor”（医生）等。但在 “Positive”（正面）类别中，语言模型还对许多具有正向情感的词赋予了更高的概率，如 “thank”（感谢）、“helpful”（有帮助的）、“friendly”（友好的）。

> 相比之下，在 “Negative”（负面）类别中，模型对一些可能与不满意服务相关的词赋予了更高的概率，例如 “time”（时间）、“service”（服务）、“day”（日子），以及 “!”（感叹号）。

</details> 

3. 估计来自朴素贝叶斯模型的正面评论预测概率。使用这些概率来检查 ``review_title`` 变量：哪些患者评论标题最有可能是正面评论？哪些最有可能是负面评论？这些看起来合理吗？

<details>
<summary>Reveal Code</summary>
    
```r
nhs_reviews_dfm_train$positive_nb_probability <- predict(nb_train, type = "probability")[,2]

nhs_reviews_dfm_train$review_title[order(nhs_reviews_dfm_train$positive_nb_probability, decreasing = T)[1:10]]
```  
```
 [1] "Good GP Practice"                    "Efficient and compassionate service"
 [3] "Fantastic practice"                  "Caring and professional service"    
 [5] "Nurse was amazing"                   "Fantastic Practice"                 
 [7] "Consistently good care"              "Exemplary Practice"                 
 [9] "Fantastic practice"                  "Fantastic Practice"   
```
                                                 
```r
nhs_reviews_dfm_train$review_title[order(nhs_reviews_dfm_train$positive_nb_probability, decreasing = F)[1:10]]
```  
```
 [1] "Shambles"                                    
 [2] "Unacceptable practice"                       
 [3] "Impossible to get a service"                 
 [4] "Unprofessional, Poor and Dangerous"          
 [5] "Impossible to get an appointment!!!"         
 [6] "Honestly, you’re better off not having a GP" 
 [7] "Disappointed with appointments."             
 [8] "Not so great practice!!"                     
 [9] "Very bad practice and arrogant receptionists"
[10] "Extremely rude receptionist"   
```
> 是的！从这些列表可以非常清楚地看出，该模型在区分正面和负面评论方面表现得相当不错。但若要确切了解其表现如何，我们需要做的不仅仅是表面效度检验。

</details> 


4. 使用你拟合好的朴素贝叶斯模型对训练数据中的每条观测进行类别预测（正面或负面）。然后使用 ``table()`` 函数创建一个混淆矩阵，用于比较预测出的正面评论与真实的正面评论编码（存储在 ``review_positive`` 变量中）。与字典分析一样，将 ``table()`` 函数的输出保存为一个新对象。

<details>
<summary>Reveal Code</summary>
    
```r
## 训练数据集准确率
nhs_reviews_dfm_train$predicted_classification_nb <- predict(nb_train, type = "class")

confusion_train <- table(predicted_classification = nhs_reviews_dfm_train$predicted_classification_nb, 
                         true_classification = nhs_reviews_dfm_train$review_positive)

confusion_train
```
```
                        true_classification
predicted_classification Negative Positive
                Negative      565       59
                Positive       13      931  
```

</details> 
    
5. 使用 ``confusionMatrix()`` 函数计算你的分类器的准确率（accuracy）、灵敏度（sensitivity） 和特异性（specificity）。将这些得分与你从字典分析中获得的结果进行比较。哪种方法表现更好？
    
<details>
<summary>Reveal Code</summary>
    
```r
confusion_train_statistics <- confusionMatrix(confusion_train, 
                                              positive = "Positive")

confusion_train_statistics
```
```
Confusion Matrix and Statistics

                        true_classification
predicted_classification Negative Positive
                Negative      565       59
                Positive       13      931
                                          
               Accuracy : 0.9541          
                 95% CI : (0.9425, 0.9639)
    No Information Rate : 0.6314          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.903           
                                          
 Mcnemar's Test P-Value : 1.137e-07       
                                          
            Sensitivity : 0.9404          
            Specificity : 0.9775          
         Pos Pred Value : 0.9862          
         Neg Pred Value : 0.9054          
             Prevalence : 0.6314          
         Detection Rate : 0.5938          
   Detection Prevalence : 0.6020          
      Balanced Accuracy : 0.9590          
                                          
       'Positive' Class : Positive   
```

> 朴素贝叶斯模型显然优于字典分析。该分类器的灵敏度现在为 94%，准确率为 95%。使用朴素贝叶斯方法，我们在训练集中检测情感的表现要好得多。

</details> 
    
6. 重复评估分类器的准确性，但现在要评估测试数据集。与训练数据集相比，分类器在这些数据上的表现如何？
    
<details>
<summary>Reveal Code</summary>
    
```r
## 测试数据集准确性

nhs_reviews_dfm_test$predicted_classification_nb <- predict(nb_train, 
                                                  newdata = nhs_reviews_dfm_test, 
                                                  type = "class")

confusion_test <- table(predicted_classification = nhs_reviews_dfm_test$predicted_classification_nb, 
                        true_classification = nhs_reviews_dfm_test$review_positive)

confusion_test_statistics <- confusionMatrix(confusion_test, 
                                             positive = "Positive")

confusion_test_statistics
```
```
Confusion Matrix and Statistics

                        true_classification
predicted_classification Negative Positive
                Negative      170       35
                Positive        6      207
                                          
               Accuracy : 0.9019          
                 95% CI : (0.8693, 0.9287)
    No Information Rate : 0.5789          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.8032          
                                          
 Mcnemar's Test P-Value : 1.226e-05       
                                          
            Sensitivity : 0.8554          
            Specificity : 0.9659          
         Pos Pred Value : 0.9718          
         Neg Pred Value : 0.8293          
             Prevalence : 0.5789          
         Detection Rate : 0.4952          
   Detection Prevalence : 0.5096          
      Balanced Accuracy : 0.9106          
                                          
       'Positive' Class : Positive 
```

> 测试数据集上的准确率为 90%，灵敏度为 86%，特异度为 97%。测试数据集上的表现比训练数据集上的表现要差一些，但仍明显优于字典分析。

</details> 

## 3.5 课后思考

1. 从 ``nhs_reviews`` 数据中创建两个新的 DFM，每次都做出不同的特征选择决策（例如：词频修剪（trimming）、使用 n-gram、是否移除停用词等）。使用这些 DFM 训练两个新的朴素贝叶斯模型，并将它们在测试集数据上的表现与你之前创建的模型进行比较。
    创建一个表格，比较你所估算模型的结果，和你为每个模型所做的特征选择决策。哪个模型在准确性、灵敏度和特异性方面表现最好？

<details>
<summary>Reveal Code</summary>
    
```r
## 无标点的 DFM

# 转换为 DFM（移除标点符号）
nhs_reviews_dfm_nopunct <- nhs_reviews_corpus %>% 
  tokens(remove_punct = T) %>%
  dfm() 

# 提取训练集中的观测值
nhs_reviews_dfm_nopunct_train <- dfm_subset(nhs_reviews_dfm_nopunct, train)

# 提取测试集中的观测值
nhs_reviews_dfm_nopunct_test <- dfm_subset(nhs_reviews_dfm_nopunct, !train)

# 训练一个新的朴素贝叶斯模型
nb_train_nopunct <- textmodel_nb(x = nhs_reviews_dfm_nopunct_train, 
                         y = nhs_reviews_dfm_nopunct_train$review_positive,
                         prior = "docfreq")

## 测试集准确率

# 使用模型对测试集进行预测
nhs_reviews_dfm_nopunct_test$predicted_classification <- predict(nb_train_nopunct, 
                                                  newdata = nhs_reviews_dfm_nopunct_test, 
                                                  type = "class")

# 创建混淆矩阵：预测类别 vs 真实类别
confusion_test_nopunct <- table(predicted_classification = nhs_reviews_dfm_nopunct_test$predicted_classification, 
                        true_classification = nhs_reviews_dfm_nopunct_test$review_positive)

# 计算混淆矩阵的性能统计（准确率、灵敏度、特异性等）
confusion_test_nopunct_statistics <- confusionMatrix(confusion_test_nopunct, positive = "Positive")

# 输出统计结果
confusion_test_nopunct_statistics
```
```
Confusion Matrix and Statistics

                        true_classification
predicted_classification Negative Positive
                Negative      169       34
                Positive        7      208
                                          
               Accuracy : 0.9019          
                 95% CI : (0.8693, 0.9287)
    No Information Rate : 0.5789          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.8029          
                                          
 Mcnemar's Test P-Value : 4.896e-05       
                                          
            Sensitivity : 0.8595          
            Specificity : 0.9602          
         Pos Pred Value : 0.9674          
         Neg Pred Value : 0.8325          
             Prevalence : 0.5789          
         Detection Rate : 0.4976          
   Detection Prevalence : 0.5144          
      Balanced Accuracy : 0.9099          
                                          
       'Positive' Class : Positive  
```
        
```r
## N-gram 模型

# 转换为 DFM（使用 1 到 3 元的 n-gram 特征）
nhs_reviews_dfm_ngram <- nhs_reviews_corpus %>% 
  tokens() %>%
  tokens_ngrams(1:3) %>%
  dfm()

# 提取训练集中的观测值
nhs_reviews_dfm_ngram_train <- dfm_subset(nhs_reviews_dfm_ngram, train)

# 提取测试集中的观测值
nhs_reviews_dfm_ngram_test <- dfm_subset(nhs_reviews_dfm_ngram, !train)

# 训练一个新的朴素贝叶斯模型
nb_train_ngram <- textmodel_nb(x = nhs_reviews_dfm_ngram_train, 
                         y = nhs_reviews_dfm_ngram_train$review_positive,
                         prior = "docfreq")

## 测试集准确率

# 使用模型对测试集进行预测
nhs_reviews_dfm_ngram_test$predicted_classification <- predict(nb_train_ngram, 
                                                  newdata = nhs_reviews_dfm_ngram_test, 
                                                  type = "class")

# 创建混淆矩阵：预测类别 vs 真实类别
confusion_test_ngram <- table(predicted_classification = nhs_reviews_dfm_ngram_test$predicted_classification, 
                        true_classification = nhs_reviews_dfm_ngram_test$review_positive)

# 计算混淆矩阵的性能统计（准确率、灵敏度、特异性等）
confusion_test_ngram_statistics <- confusionMatrix(confusion_test_ngram, positive = "Positive")

# 输出统计结果
confusion_test_ngram_statistics
```
        
```
Confusion Matrix and Statistics

                        true_classification
predicted_classification Negative Positive
                Negative      169       35
                Positive        7      207
                                          
               Accuracy : 0.8995          
                 95% CI : (0.8666, 0.9266)
    No Information Rate : 0.5789          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.7983          
                                          
 Mcnemar's Test P-Value : 3.097e-05       
                                          
            Sensitivity : 0.8554          
            Specificity : 0.9602          
         Pos Pred Value : 0.9673          
         Neg Pred Value : 0.8284          
             Prevalence : 0.5789          
         Detection Rate : 0.4952          
   Detection Prevalence : 0.5120          
      Balanced Accuracy : 0.9078          
                                          
       'Positive' Class : Positive       
```

```r
accuracy <- c(confusion_test_statistics$overall[1],
              confusion_test_nopunct_statistics$overall[1],
              confusion_test_ngram_statistics$overall[1])

sensitivity <- c(confusion_test_statistics$byClass[1],
                 confusion_test_nopunct_statistics$byClass[1],
                 confusion_test_ngram_statistics$byClass[1])

specificity <- c(confusion_test_statistics$byClass[2],
                 confusion_test_nopunct_statistics$byClass[2],
                 confusion_test_ngram_statistics$byClass[2])

model_names <- c("Unigram", "No punctuation", "N-gram")

results <- data.frame(model_names,
                      accuracy,
                      sensitivity,
                      specificity)

results                                 
```

```
     model_names  accuracy sensitivity specificity
1        Unigram 0.9019139   0.8553719   0.9659091
2 No punctuation 0.9019139   0.8595041   0.9602273
3         N-gram 0.8995215   0.8553719   0.9602273  
```

> 不同特征选择方法之间的差异很小。这主要是因为预测积极性是一项非常简单的任务！表示积极性的词语非常清晰，因此我们可以从这些数据中获得强烈的信号。需要注意的是，情况并非总是如此，尤其是在我们试图对一个更细微的概念或一组更精细的类别进行分类时。

</details> 
    
2. 💪困难任务：在这个问题中，你应该实现 k 折交叉验证(k-fold cross-validation)，以评估你在上面估计的一个朴素贝叶斯模型的准确率、灵敏度和特异性。

a. 我在下面提供了一些起始代码，用于一个函数，该函数将接受一个逻辑值向量作为输入，该向量应命名为 ``held_out``。该向量应对交叉验证中每一折的保留集观测为 ``TRUE``，训练集观测为 ``FALSE``。

你的任务是填写代码中 “``你的代码在这里！``” 的部分。

<details>
<summary>Reveal Code</summary>

```r
get_performance_scores <- function(held_out){
  
  # 为该折叠设置训练集和测试集
  dfm_train <- dfm_subset(nhs_reviews_dfm, !held_out)
  dfm_test <- dfm_subset(nhs_reviews_dfm, held_out)
  
  # 在非 held-out 部分训练模型
  
      # 你的代码在这里！
  
  # 对 held-out 部分进行预测
  
      # 你的代码在这里！
  
  # 创建混淆矩阵,计算准确性、特异性和灵敏度
  
      # 你的代码在这里！

  # 将结果保存在 data.frame 中
  
  return(output)
  
}  
```
              
```r
get_performance_scores <- function(held_out){
  
  # 为该折叠设置训练集和测试集
  dfm_train <- dfm_subset(nhs_reviews_dfm, !held_out)
  dfm_test <- dfm_subset(nhs_reviews_dfm, held_out)
  
    # 在非 held-out 部分训练模型
  nb_train <- textmodel_nb(x = dfm_train, 
                         y = dfm_train$review_positive,
                         prior = "docfreq")
  
   # 对 held-out 部分进行预测
  dfm_test$predicted_classification <- predict(nb_train, 
                                             newdata = dfm_test, 
                                             type = "class")
  
  # 创建混淆矩阵,计算准确性、特异性和灵敏度
  confusion_nb <- table(predicted_classification = dfm_test$predicted_classification,
                        true_classification = dfm_test$review_positive)
  
  confusion_nb_statistics <- confusionMatrix(confusion_nb, positive = "Positive")
  
  accuracy <- confusion_nb_statistics$overall[1]
  sensitivity <- confusion_nb_statistics$byClass[1]
  specificity <- confusion_nb_statistics$byClass[2]
  
  return(data.frame(accuracy, sensitivity, specificity))
  
}         
```
    
</details> 
    
b. 完成这个函数之后，你需要创建一个向量，用来表示你将在交叉验证中测试数据所使用的各个折（K folds）。为此，你需要使用 ``sample()`` 函数生成一个向量，该向量的元素数量应与 ``nhs_reviews_dfm`` 对象中的行数相同。


<details>
<summary>Reveal Code</summary>

```r
K <- 10
folds <- sample(1:K, nrow(nhs_reviews_dfm), replace = T)
```

</details> 

c. 最后，你需要将 ``get_performance_scores()`` 函数应用到每一个折（fold）上。为此，``lapply()`` 函数可能会对你有所帮助。这个函数将一个向量作为输入，对该向量的每个值应用一个函数，并返回一个列表对象，其中包含你输入向量每个值所对应的函数运行结果。

<details>
<summary>Reveal Code</summary>

```r
all_folds <- lapply(1:K, function(k) get_performance_scores(folds==k))
```

</details> 
    
d. 输出你的结果！

<details>
<summary>Reveal Code</summary>

```r
all_folds <- lapply(1:K, function(k) get_performance_scores(folds==k))
colMeans(bind_rows(all_folds))
```
```
   accuracy sensitivity specificity 
  0.9122027   0.8881802   0.9504991 
```

</details> 


