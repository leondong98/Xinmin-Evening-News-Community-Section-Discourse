---
title: '研讨会2: 相似性、差异性与复杂性'

---

# 研讨会2: 相似性、差异性与复杂性

讲座人：Hanxu hanxu.dong.21@ucl.ac.uk

<p>
  <a href="https://github.com/leondong98/Text-Analysis-Workshop/raw/main/Data/constitutions.csv" 
     style="display:inline-block; background-color:#4CAF50; color:white; padding:10px 20px; text-decoration:none; margin-right:10px; border-radius:5px;">
    Seminar data
  </a>


## 2.1 分析各国宪法序言
美国宪法是否影响了其他国家的宪法？目前越来越多的学术观点认为，美国宪法的影响力已随着时间推移而减弱，因为它日益偏离了全球范围内关于人权对宪法秩序重要性的共识。然而，关于美国宪法在多大程度上影响了世界各国成文宪法的修订和采纳，目前仍缺乏实证和系统的认知。
    
David S. Law 和 Mila Versteeg 在2012年的一项[研究](https://www.nyulawreview.org/wp-content/uploads/2018/08/NYULawReview-87-3-Law-Versteeg_0.pdf)中，通过实证调查了美国宪法的影响力，结果表明，近几十年来，其他国家在其本国宪法中效仿美国宪法中与权利相关的条款的可能性越来越小。在本系列问题中，我们将运用本周所学的一些方法来复现其部分分析。

## 2.1.1 Packages
在开始研讨会时，先下载/加载以下 R 包：

```r
library(tidyverse)
library(quanteda)
# 如果无法加载quanteda.textplots和quanteda.textstats包，请运行以下代码：
# devtools::install_github("quanteda/quanteda.textplots")
# devtools::install_github("quanteda/quanteda.textstats")
library(quanteda.textplots)
library(quanteda.textstats)  
```

## 2.1.2 数据

我们将使用如下数据集：**宪法序言** — ``constitutions.csv``

该文件包含155部宪法（英文译本）的序言。数据包含以下变量：
    
`constituiton` 数据集中的变量说明

| **变量**   | **说明**                         |
|--------------|--------------------------------------|
| `country`    | 国家名称                             |
| `continent`  | 所在洲（大洲）                        |
| `year`       | 宪法制定的年份                        |
| `preamble`   | 宪法序言的文本内容                   |


一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：

```r
constitutions <- read_csv("constitutions.csv") 
```

你可以使用 ``tidyverse`` 包中的 ``glimpse()`` 函数快速查看数据中的变量：
                 
```r
glimpse(constitutions)
```
```
Rows: 155
Columns: 4
$ country   <chr> "afghanistan", "albania", "algeria", "andorra", "angola", "a…
$ continent <chr> "Asia", "Europe", "Africa", "Europe", "Africa", "Americas", …
$ year      <dbl> 2004, 1998, 1989, 1993, 2010, 1981, 1853, 1995, 1995, 1973, …
$ preamble  <chr> "In the name of Allah, the Most Beneficent, the Most Mercifu…
```
    
## 2.2 Tf-idf
    
1. 探索 ``constitutions`` 对象，以了解我们正在处理的数据。``preambles`` 变量中存储的文本平均长度是多少？哪个国家的序言文本最长？哪个国家的最短？这些序言的平均长度是否随着时间发生了变化？

<details>
<summary>Reveal Code</summary>
    
```r
# 序言文本平均长度：
constitutions$preamble_length <- ntoken(tokens(constitutions$preamble))
mean(constitutions$preamble_length)
```
```
[1] 324.3097                               
```
                                 
```r
# 文本最长的国家
constitutions$country[which.max(constitutions$preamble_length)]
```
```
[1] "iran_islamic_republic_of"                             
```
                                 
```r
# 文本最短的国家
constitutions$country[which.min(constitutions$preamble_length)]
```
```
[1] "greece"                            
```

```r
constitutions %>%
  ggplot(aes(x = year,
             y = preamble_length)) +
  geom_point() + 
  xlab("Year") + 
  ylab("Preamble Length") + 
  theme_bw() 
```
<img src="https://raw.githubusercontent.com/leondong98/Text-Analysis-Workshop/main/images/2_1.png" width="700"/>
    
</details>
    
2. 将 ``constitutions`` 数据框转换为 ``corpus()`` 对象，然后进一步转换为 ``dfm()`` 对象（注意：你还需要使用 ``tokens()`` 函数）。同时，进行一些合理的特征选择决策。

<details>
<summary>Reveal Code</summary>
    
```r
constitutions_dfm <- constitutions %>% 
                        corpus(text_field = "preamble") %>%
                        tokens(remove_punct = T) %>%
                        dfm() %>%
                        dfm_remove(stopwords("en"))
```   
    
</details>

    
    
3. 使用 ``topfeatures()`` 函数找出美国宪法中最常出现的 10 个特征词。将这些特征与你选择的其他三个国家的宪法中的高频特征进行比较。你注意到了什么？

<details>
<summary>Reveal Code</summary>
    
```r
topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "united_states_of_america",])
```   
```
      united    establish       states       people        order         form 
           2            2            2            1            1            1 
     justice      defense constitution      liberty 
           1            1            1            1  
```
    
    
```r
topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "argentina",])
```   
```
argentine   justice    nation   general     peace    people       god  national 
        3         2         2         2         1         1         1         1 
establish   defense 
        1         1   
```


```r
topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "cuba",])
``` 
```
          man       peoples       dignity    revolution          full 
            4             3             3             3             3 
revolutionary       victory          mart          last         human 
            3             3             3             2             2    
```

```r
topfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == "sudan",])
``` 
```
       peace    committed    agreement        sudan constitution    religious 
           4            3            3            3            2            2 
       shall   governance   agreements          end 
           2            2            2            2   
```
    
</details>
    
    
4. 使用 ``dfm_tfidf()`` 函数将 tf-idf 权重应用于你的 ``dfm``。然后使用新的矩阵重复上面的练习。你注意到了什么？ 

<details>
<summary>Reveal Code</summary>
    
```r
constitutions_dfm_tf_idf <- constitutions_dfm %>% dfm_tfidf()

topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",])
``` 
```
     insure     america     perfect      states    domestic tranquility 
   2.190332    1.889302    1.889302    1.870118    1.588272    1.588272 
     ordain   establish     defense   blessings 
   1.412180    1.292527    1.236089    1.190332  
```

```r
topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "argentina",])
``` 
```
   argentine pre-existing constituting        dwell     congress    provinces 
    6.570995     2.190332     2.190332     2.190332     1.889302     1.889302 
     compose       object     securing      general 
    1.889302     1.889302     1.889302     1.870118  
```
    
```r
topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "cuba",])
``` 
```
         mart         cuban           jos        cubans       victory 
     6.570995      4.380663      4.380663      4.380663      4.035701 
revolutionary      peasants       workers    revolution          last 
     3.446817      3.426421      3.176543      3.132611      2.824361 
```
    
```r
topfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "sudan",])
``` 
```
        sudan     agreement    agreements          2005      conflict 
     5.667905      4.236541      3.426421      3.426421      3.426421 
comprehensive           end     committed      bestowed  definitively 
     2.982723      2.380663      2.326075      2.190332      2.190332 
```

</details>
    
5. 使用 ``textplot_wordcloud()`` 函数为美国和另一个国家各制作一个词云 （你会惊叹于这些图有多么丑）。请注意：由于美国宪法的文本非常短，你可能需要将 ``min_count`` 参数设置为比默认值 3 更低的数值。
    
<details>
<summary>Reveal Code</summary>
 
```r
textplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",], min_count = 0)
```

<img src="https://raw.githubusercontent.com/leondong98/Text-Analysis-Workshop/main/images/2_2.png" width="700"/>


```r
textplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "cuba",], min_count = 2)
```

<img src="https://raw.githubusercontent.com/leondong98/Text-Analysis-Workshop/main/images/2_3.png" width="700"/>

</details>

## 2.3 余弦相似度（cosine similarity）

余弦相似度（cos(θ)）用于衡量两个向量 **a** 和 **b** 之间的相似性，其定义如下：

$$
\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}
$$

其中，θ 是向量 **a** 与 **b** 之间的夹角，$\|\mathbf{a}\|$ 和 $\|\mathbf{b}\|$ 分别表示向量 **a** 和 **b** 的模（magnitude）。

更直观的展开形式如下：

$$
\cos(\theta) = \frac{a_1b_1 + a_2b_2 + \cdots + a_Jb_J}
{\sqrt{a_1^2 + a_2^2 + \cdots + a_J^2} \times \sqrt{b_1^2 + b_2^2 + \cdots + b_J^2}}
$$

1. 编写一个用于计算两个向量之间余弦相似度的函数。我们可以使用 ``function()`` 函数来定义新函数。例如，如果我们想定义一个用于计算平均值的新函数，可以写成：``mean_func <- function(x) sum(x)/length(x)``, 其中 x 是一个向量。

<details>
<summary>Reveal Code</summary>

```r
cosine_sim <- function(a, b){
  
  # 计算两个向量的内积
  numerator <- sum(a * b)
  
  # 计算第一个向量的模
  magnitude_a <- sqrt(sum(a^2))
  
  # 计算第二个向量的模
  magnitude_b <- sqrt(sum(b^2))
  
  # 分母的计算
  denominator <- magnitude_a * magnitude_b

  # 余弦相似度
  cos_sim <- numerator/denominator
  
  return(cos_sim)
  
}
```  

</details>    


2. 使用你在上面创建的函数，计算美国宪法序言与另一个你选择的国家的宪法序言之间的余弦相似度。请使用 tf-idf 加权版本的 ``dfm`` 来完成这个任务。为确保你的函数工作正常，可将其结果与 ``textstat_simil()`` 函数计算出的余弦相似度进行比较。使用 ``textstat_simil()`` 时，需要传入 ``x`` 向量和 ``y`` 向量，并指定 ``method = "cosine"``，以计算余弦相似度（而不是其他相似度指标）。

<details>
<summary>Reveal Code</summary> 
    
```r
cosine_sim(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "cuba",],
           constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",])
``` 
```
[1] 0.02427221
```
    
```r
textstat_simil(x = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "cuba",], 
                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",],
                             method = "cosine")
``` 
```
textstat_simil object; method = "cosine"
       text149
text36  0.0243
```

> 棒极了！看来这个功能成功了。
    
</details>  
    

3. 使用 ``textstat_simil()`` 函数计算美国宪法序言与数据中所有其他序言之间的余弦相似度。(你也可以为该函数提供一个 ``x`` 矩阵和一个 ``y`` 向量，这样就能计算 ``x`` 中所有行与 ``y`` 的相似度。)使用 ``as.numeric()`` 函数将该函数的输出赋值回原始的 ``constitutions`` 数据框。哪三个宪法与美国最相似？哪三个最不相似？(使用 ``order()`` 函数来完成这项操作)

<details>
<summary>Reveal Code</summary> 
    
```r
# 计算余弦相似度
cosine_sim <- textstat_simil(x = constitutions_dfm_tf_idf, 
                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == "united_states_of_america",],
                             method = "cosine")


# 将变量赋值给 data.frame
constitutions$cosine_sim <- as.numeric(cosine_sim)

# 找出最相似和最不相似的宪法序言
constitutions$country[order(constitutions$cosine_sim, decreasing = T)][1:3]
``` 
```
[1] "united_states_of_america" "argentina"               
[3] "philippines" 
```
    
```r
constitutions$country[order(constitutions$cosine_sim, decreasing = F)][1:3]
``` 
```
[1] "greece"    "lithuania" "slovenia" 
```

</details>  
    
4. 计算从 1950 年代起撰写的所有宪法中，美国宪法与其他国家宪法之间的平均余弦相似度在各个年代的值。

要完成这个问题，你需要处理几个编码细节:
- 首先，你需要将 ``year`` 变量转换为 ``decade`` 变量。你可以使用 ``%%`` “模”运算符来实现，它用于计算两个数值变量相除后的余数。例如，``1986 %% 10`` 将返回值 ``6``。如果你从原始年份中减去这个值，就会得到正确的年代（即 1986 - 6 = 1980）。
- 其次，你需要计算上一个问题中创建的余弦相似度变量的年代平均值。为此，你应使用 ``group_by()`` 和 ``summarise()`` 函数。``group_by()`` 允许你指定进行汇总时所依据的变量，而 ``summarise()`` 函数允许你指定要使用哪种类型的汇总（即这里你应使用 ``mean()`` 函数）。
    
<details>
<summary>Reveal Code</summary>
 
```r
# 创建 decade 变量
constitutions$decade <- constitutions$year - (constitutions$year %% 10)

# 按 decade 计算平均相似度
cosine_sim_by_year <- constitutions %>%
  filter(year >= 1950) %>%
  group_by(decade) %>%
  summarise(cosine_sim = mean(cosine_sim)) 
``` 
    
</details>  
    
5. 创建一张折线图（使用 ``ggplot`` 中的 ``geom_line()``），将你上面计算的平均值放在 y 轴上，将年代（decades）放在 x 轴上。在最近几十年中，各国宪法的序言是否变得与美国宪法序言越来越不相似了？
    
<details>
<summary>Reveal Code</summary>
 
```r
cosine_sim_by_year %>%
  ggplot(aes(x = decade, y = cosine_sim)) + 
  geom_line() + 
  xlab("Decade") +
  ylab("Tf-idf cosine similarity with US constitution") + 
  theme_bw()
``` 
<img src="https://raw.githubusercontent.com/leondong98/Text-Analysis-Workshop/main/images/2_4.png" width="700"/>
    
</details>  
    
## 2.4 Fightin’ Words
    
除了计算宪法之间的相似度之外，我们或许还希望描述世界不同地区宪法之间的语言差异。为了刻画这些差异，我们将使用 Fightin’ Words 方法，该方法由 [Munroe 等人（2008）](https://www.cambridge.org/core/journals/political-analysis/article/fightin-words-lexical-feature-selection-and-evaluation-for-identifying-the-content-of-political-conflict/81B3703230D21620B81EB6E2266C7A66) 提出。
    
回顾Lecture内容，该方法首先计算**某个类别中某个词出现的概率**（此处以部门为例）：

$$
\hat{\mu}_{j,k} = \frac{W^*_{j,k} + \alpha_0}{n_k + \sum_{j=1}^{J}1 + \alpha_0}
$$

其中：

- $W^*_{j,k}$ 表示特征 $j$ 在类别 $k$ 的文档中出现的次数  
- $n_k$ 表示类别 $k$ 的文档中的总词数  
- $\alpha_0$ 是平滑参数（“smoothing”），用于将常见词的差异收缩至 0  

接着，我们计算类别 $k$ 和 $k'$ 的**对数比率（log-odds）**：

$$
\text{log-odds-ratio}_{j,k} = \log \left( \frac{\hat{\mu}_{j,k}}{1 - \hat{\mu}_{j,k}} \right) - \log \left( \frac{\hat{\mu}_{j,k'}}{1 - \hat{\mu}_{j,k'}} \right) 
$$

最后，我们用其 **方差（variance）** 进行标准化处理（再次降低稀有词的影响）：

$$
\text{Fightin' Words Score}_j =
\frac{\text{log-odds-ratio}_{j,k}}{\sqrt{\text{Var}(\text{log-odds-ratio}_{j,k})}}
$$

我在下面提供了一个函数，它可以根据数据中任意协变量所定义的一对分组，计算出 Fightin’ Words 分数：
    
```r
fightin_words <- function(dfm_input, covariate, group_1 = "Political Science", group_2 = "Economics", alpha_0 = 1){
  
  # 子集化 DFM：仅保留属于 group_1 和 group_2 的文档
  fw_dfm <- dfm_subset(dfm_input, get(covariate) %in% c(group_1, group_2)) 
  fw_dfm <- dfm_group(fw_dfm, get(covariate))
  fw_dfm <- fw_dfm[,colSums(fw_dfm)!=0]
  dfm_input_trimmed <- dfm_match(dfm_input, featnames(fw_dfm))
  
  # 计算每个词的先验频率 alpha_w
  alpha_w <- (colSums(dfm_input_trimmed))*(alpha_0/sum(dfm_input_trimmed))
   
  for(i in 1:nrow(fw_dfm)) fw_dfm[i,] <- fw_dfm[i,] + alpha_w
  fw_dfm <- as.dfm(fw_dfm)
  mu <- fw_dfm %>% dfm_weight("prop")

  # 计算每个词的 log-odds 比值
  lo_g1 <- log(as.numeric(mu[group_1,])/(1-as.numeric(mu[group_1,])))
  lo_g2 <- log(as.numeric(mu[group_2,])/(1-as.numeric(mu[group_2,])))
  fw <- lo_g1 - lo_g2
  
  # 计算每个词的方差
  fw_var <- as.numeric(1/(fw_dfm[1,])) + as.numeric(1/(fw_dfm[2,]))
  
  # 计算标准化得分（z 分数），以及每个词的频次和名称
  fw_scores <- data.frame(score = fw/sqrt(fw_var),
                          n = colSums(fw_dfm),
                          feature = featnames(fw_dfm))

  return(fw_scores)
  
}
``` 
              
### `fightin_words()` 函数的参数说明

| 参数名称     | 说明                                                                 |
|--------------|----------------------------------------------------------------------|
| `dfm_input`  | 一个 dfm（文档-词项矩阵），用于测量每篇文档中词语的频率。               |
| `covariate`  | 协变量的名称（应以字符串形式输入，如 `"covariate_name"`），其中包含分组标签（例如 “continent”）。 |
| `group_1`    | 第一个分组的名称。                                                    |
| `group_2`    | 第二个分组的名称。                                                    |
| `alpha_0`    | 正则化参数，必须是正数。                                               |

该函数返回一个 `data.frame`，其中的每一行表示在文本中出现的一个特征（词项），与两个分组相关，包含以下三个变量：

| 变量名称   | 说明                                                                 |
|------------|----------------------------------------------------------------------|
| `feature`  | 词项名称。                                                           |
| `fw`       | 每个词项对应的 Fightin' Words 分数。                                 |
| `n`        | 每个词项在两个分组的所有文档中出现的总次数。                         |

      
      
1. 使用上面介绍的的函数，计算区分非洲与欧洲国家宪法的 Fightin’ Words 分数，并回答以下问题：(a)哪 10 个词最强烈地与非洲国家的宪法相关？(b)哪 10 个词最强烈地与欧洲国家的宪法相关？
      
<details>
<summary>Reveal Code</summary>
 
```r
fw_scores_africa_europe <- fightin_words(dfm_input = constitutions_dfm, 
                                         covariate = "continent", 
                                         group_1 = "Africa", 
                                         group_2 = "Europe", 
                                         alpha_0 = 1)

fw_scores_africa_europe$feature[order(fw_scores_africa_europe$score, decreasing = T)][1:10]
``` 
```
 [1] "political"  "unity"      "man"        "rights"     "power"     
 [6] "solemnly"   "charter"    "person"     "december"   "attachment"
```
      
```r
fw_scores_africa_europe$feature[order(fw_scores_africa_europe$score, decreasing = F)][1:10]
``` 
```
 [1] "republic"           "citizens"           "responsibility"    
 [4] "state"              "self-determination" "historical"        
 [7] "statehood"          "centuries"          "basic"             
[10] "members" 
```
> 与非洲宪法相关的词汇通常涉及权利、权力与团结，而与欧洲宪法相关的词汇则更多反映出责任、自决以及历史经验。 
    
</details>  
      
2. 创建一个图表，绘制每个特征名称。图表的 x 轴应为每个词项的对数频率（log frequency），y 轴应为该特征的 Fightin’ Words 分数。使用 ``cex`` 和 ``alpha`` 函数，根据它们的 Fightin’ Words 分数调整词语的大小和透明度。
      
<details>
<summary>Reveal Code</summary>
 
```r
fw_scores_africa_europe %>%
  ggplot(aes(x = log(n),
             y = score,
             label = feature,
             cex = abs(score),
             alpha = abs(score))) + 
  geom_text() + 
  scale_alpha_continuous(guide = "none") + 
  scale_size_continuous(guide = "none") + 
  theme_bw() + 
  xlab("log(n)") + 
  ylab("Fightin' Words Score")
``` 

<img src="https://raw.githubusercontent.com/leondong98/Text-Analysis-Workshop/main/images/2_5.png" width="700"/>
    
</details>  
