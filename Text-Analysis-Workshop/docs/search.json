[
  {
    "objectID": "简单R入门.html",
    "href": "简单R入门.html",
    "title": "简单R入门",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n如果你还没有接触过 R， 或者对上次使用 R 的操作已经有些生疏（这完全可以理解：编程就像外语学习，如果不用就会遗忘），你可以先完成这里提供的一些练习。这些练习旨在帮助你初识/回顾 R 编程的基本概念，以及如何高效使用 RStudio。\n首先，请下载 R 和 RStudio\n这个介绍将涵盖以下主题： - 在控制台中使用 R - 在脚本文件中使用 R - 对象与赋值 - 向量（vectors） - 函数的使用 - 帮助文档的查询 - 数据框（data frames） - 子集提取（subsetting） - 线性回归（linear regression） - t 检验（t-test）\n\n\n让我们来熟悉一下 R 和 Rstudio。第一次启动 RStudio 时，你会看到三个面板：\n\n接下来让我们来一个个熟悉这些面板\n\n\n\n左侧是控制台（console），这是与 R 交互的最简单方式。你可以在控制台中输入代码（点击光标显示为 &gt; | 的位置），按下 ENTER 键后，R 就会执行该段代码。\n根据你输入的内容，你可能会在控制台中看到相应的输出结果；如果你输入有误，也可能会收到警告信息或错误提示。\n首先，让我们通过将 R 当作一个简单的计算器来熟悉控制台的使用：\n2 + 7\n[1] 9\n5 * 6\n[1] 30\n7 / 2\n[1] 3.5\n\n\n你可以使用键盘上的光标键或方向键在控制台中编辑你的代码： - 使用 ↑（向上） 和 ↓（向下） 键可以重新运行之前输入的代码，而无需重新输入 - 使用 ↑（向上） 和 ↓（向下） 键可以重新运行之前输入的代码，而无需重新输入\n\n\n\n\n控制台非常适合处理简单任务，但如果你正在进行一个项目，你会通常希望将工作保存在某种文档或文件中。R 中的脚本（script）就是包含 R 代码的纯文本文件。你可以像编辑任何文字处理或笔记应用中的文件一样编辑脚本。\n我们建议你在本课程中始终使用脚本文件进行操作。\n请使用菜单栏或工具栏按钮（如下图所示）来创建一个新的脚本文件:\n\n创建脚本后，请立即为其赋予一个有意义的文件名并保存。\n请注意 RStudio 中的脚本窗口，特别是其中标有 Run 和 Source 的两个按钮：\n\n💡从脚本中运行代码有几种不同的方式： | 执行方式 | 操作说明 | |—————-|————————————————————————–| | 单行执行 | 将光标放在你要运行的行上，按下 CTRL + ENTER，或点击 Run 按钮 | | 多行执行 | 选中你要运行的多行代码，按下 CTRL + ENTER，或点击 Run 按钮 | | 整个脚本执行 | 点击 Source 按钮 |\n\n\n\nR 语言中操作的基本结构称为“对象（objects）”。创建对象是将信息存储在 R 中的一种方式，我们可以为任何对象赋予任意名称。一旦创建了对象，我们就可以在后续的多种任务中使用它。\n让我们从创建一个对象开始，该对象用于存储一个简单加法的结果。我们使用赋值运算符 &lt;- 来创建或更新对象。例如，如果我们希望保存 10 + 4 的结果，可以这样写：\nmy_result &lt;- 10 + 4\n上面这一行代码创建了一个名为 my_result 的新对象，并将 10 + 4 的结果保存在其中。要查看 my_result 中的内容，只需在控制台中输入它的名字：\nmy_result\n[1] 14\n请注意，所有对象名称在 R 中都是区分大小写的，因此如果你在控制台中输入 My_Result，你将会看到如下报错信息：\nError in eval(expr, envir, enclos): object 'My_Result' not found\n对象最有用的地方在于：一旦创建，就可以在后续的计算中使用它们。例如：\nmy_result*2\n[1] 28\n你甚至可以对一个对象进行计算，并将结果赋值给一个新的对象：\nmy_new_result &lt;- my_result*2\nmy_new_result\n[1] 28\n现在，我们已经创建了两个对象，查看 RStudio 中的 Environment 面板，你会看到其中列出了 my_result 和 my_new_result 两个对象。\n\n要从环境中删除所有对象，可以使用如上图所示的扫帚按钮（broom button）。\n我们将对象命名为 my_result，但其实你可以使用任何名称，只要遵守以下几个简单的规则即可： - 对象名称可以包含大小写字母（A–Z，a–z）、数字（0–9）、下划线（_）或句点（.） - 但对象名称必须以字母开头 - 建议选择具有描述性且易于输入的名称，以便提高代码可读性与可维护性\n\n\n\n\n\n\n\n推荐的对象名\n不推荐的对象名\n\n\n\n\nresult\na\n\n\nmy_result\nx1\n\n\nmy.result\nthis.name.is.just.too.long\n\n\nmy_new_result\nsing,dance,rap,basketball\n\n\n\n\n\n\nmy_result 和 my_new_result 都是只包含单个数字的对象。但在实际操作中，我们常常需要处理相互关联的一组数字。为此，我们需要使用的基本构建模块就是向量（vectors）。\n向量就是一组信息（通常是数字，但也可以是字符字符串或逻辑值），按特定顺序组合在一起。\n在 R 中创建向量的一种方式是使用 c() 函数，它可以将多个数值“连接”（concatenate）在一起。例如，我们可以将以下数字连接成一个向量：\nmy_first_vector &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\nmy_first_vector\n [1]  0  1  1  2  3  5  8 13 21 34\n向量中数值的存储顺序是重要的，我们可以使用方括号 [ ] 来访问向量中的单个元素。例如，如果我们希望访问刚刚创建的向量中的第 3 个元素，可以这样操作：\nmy_first_vector[3]\n [1]  1\n这是一个向量子集提取（subsetting）的基本示例，它可以让我们访问向量中我们想使用的部分。我们还可以使用一个向量来提取另一个向量的子集，例如my_first_vector[c(1,3,5)] 将返回我们向量中的第 1、3 和 5 个元素。\nmy_first_vector[c(1,3,5)]\n [1] 0 1 3\n\n\n\n函数是一组用于执行特定任务的指令。函数通常需要一些输入，并生成某种输出。例如，我们可以不用加号运算符 +，而使用 sum 函数来对两个或多个数字进行求和。让我们来试试将我们上面创建的向量中的所有元素相加：\nsum(my_first_vector)\n[1] 88\n这里我们将向量作为 sum() 函数的输入，输出结果是 88。你可以手动检查这个结果是否正确，或者直接相信 R 能够正确地计算这些数字的总和。\n函数的调用必须使用圆括号 ()。传递给函数的输入称为参数（arguments），它们被放在括号内。函数的输出通常会显示在屏幕上，但我们也可以选择将其输出结果保存下来。例如：\nvec_sum &lt;- sum(my_first_vector)\nvec_sum\n[1] 88\n在这里，vec_sum 也是一个对象！所以我们对一些数据（my_first_vector）执行了一个计算（sum()），并将结果（vec_sum）存储了下来。\n尝试对我们创建的向量应用其他函数。例如，你可以尝试使用 mean()、median()、max() 和 min()。这些结果是否与你的预期一致？\n⚠️请注意，R 中的函数名称也是区分大小写的！ 这意味着，mean(my_first_vector) 可以正确计算向量的平均值，而 Mean(my_first_vector) 则会报错。\nmean(my_first_vector)\nmedian(my_first_vector)\nmax(my_first_vector)\nmin(my_first_vector)\n[1] 8.8\n[1] 4\n[1] 34\n[1] 0\n\n\n\n在控制台右下角，你会看到一个面板，包含名为 Plots、Packages、Help 和 Viewer 的多个标签页。现在我们暂时用不到大部分内容，但让我们先了解一下 Help 面板。\n在 R 中，任何函数都有对应的帮助文件。例如，如果我们想了解如何使用 sum() 函数，可以输入：\nhelp(sum)\n问号 ? 也可以作为快捷方式来访问在线帮助:\n?sum\n\n你可以使用上图所示的工具栏按钮，将帮助内容展开并在新窗口中显示。\nR 中函数的帮助页面遵循一致的结构，通常包括以下几个部分：\n\n\n\n\n\n\n\n项目\n说明\n\n\n\n\nDescription\n函数的简要描述\n\n\nUsage\n包括所有参数（输入）的完整语法或用法\n\n\nArguments\n每个参数的解释\n\n\nDetails\n关于该函数及其参数的任何相关详细信息\n\n\nValue\n函数的输出值\n\n\nExamples\n使用该函数的示例\n\n\n\n\n\n\ndata.frame 是一种以表格形式存储数据的对象，类似于电子表格的结构，其中每一列代表一个变量，每一行代表一个观测单位。\n虽然你可以手动创建一个 data.frame，但在大多数情况下，你会从文件中加载一个数据集，而在 R 中它将被表示为一个 data.frame。不过现在我们将使用一个 R 中预装的数据集。\n让我们来看一个名为 longley 的宏观经济数据集。为此，请在你的 R 脚本中运行以下代码：\ndata(longley)\nlongley 数据集是一个包含 7 个变量 和 16 个观测值 的 data.frame。\nhelp(longley)\n帮助页面对这 7 个变量中的每一个都进行了描述。现在让我们来看一下 longley 数据集中的内容:\nlongley\n\n\nClick to show output\n\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1947         83.0 234.289      235.6        159.0    107.608 1947   60.323\n1948         88.5 259.426      232.5        145.6    108.632 1948   61.122\n1949         88.2 258.054      368.2        161.6    109.773 1949   60.171\n1950         89.5 284.599      335.1        165.0    110.929 1950   61.187\n1951         96.2 328.975      209.9        309.9    112.075 1951   63.221\n1952         98.1 346.999      193.2        359.4    113.270 1952   63.639\n1953         99.0 365.385      187.0        354.7    115.094 1953   64.989\n1954        100.0 363.112      357.8        335.0    116.219 1954   63.761\n1955        101.2 397.469      290.4        304.8    117.388 1955   66.019\n1956        104.6 419.180      282.2        285.7    118.734 1956   67.857\n1957        108.4 442.769      293.6        279.8    120.445 1957   68.169\n1958        110.8 444.546      468.1        263.7    121.950 1958   66.513\n1959        112.6 482.704      381.3        255.2    123.366 1959   68.655\n1960        114.2 502.601      393.1        251.4    125.368 1960   69.564\n1961        115.7 518.173      480.6        257.2    127.852 1961   69.331\n1962        116.9 554.894      400.7        282.7    130.081 1962   70.551\n\n\n\n我们也可以使用 View 函数查看 longley 数据集，该函数会以类似电子表格的形式显示数据框:\nView(longley)\n\n\n\n在分析 data.frame 时，我们通常希望对数据进行子集提取。也就是说，我们常常只想从数据中选择某些特定的行或特定的列。\n\n\n访问 data.frame 中单个列的最简单方法是使用符号 $。例如，我们来看如何只访问 Year 这一列：\nlongley$Year\n [1] 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961\n[16] 1962\n没错，这是一个向量（vector）！也就是说，它是一组按顺序连接在一起的信息。因此，我们可以像前面示例中那样，访问该向量中的特定元素。例如：\nlongley$Year[7]\n[1] 1953\nlongley$Year 向量的第七个元素是 1953。\n现在，请你试着使用 $ 来访问 GNP 变量,请查看其第 1、2 和 10 个元素的值：\n\n\nReveal answer\n\nlongley$Year[7]\n [1] 234.289 259.426 258.054 284.599 328.975 346.999 365.385 363.112 397.469\n[10] 419.180 442.769 444.546 482.704 502.601 518.173 554.894  \nlongley$GNP[c(1,2,10)]\n[1] 234.289 259.426 419.180\n\n\n\n\n\n我们之前提到，可以使用方括号 [ ] 对向量进行子集提取。 在处理 data.frame 时，我们通常希望访问某些特定的观测值（行）、变量（列），或两者的组合，而不是一次性查看整个数据集。我们也可以使用方括号 [,] 对数据框进行子集提取。\n在方括号中，我们用逗号分隔行坐标和列坐标。行坐标写在前，列坐标写在后。例如： - longley[10, 3] 返回数据框第 10 行第 3 列 的内容。 - 如果省略列坐标，表示取该行的所有列：longley[10, ] 返回第 10 行 的所有数据。 - 如果省略行坐标，表示取该列的所有行：longley[, 3] 返回第 3 列 的全部内容。\nlongley[10, 3] # 第 10 行第 3 列中的元素\n[1] 282.2\nlongley[10, ] # 整个第 10 行的元素\n     GNP.deflator    GNP Unemployed Armed.Forces Population Year Employed\n1956        104.6 419.18      282.2        285.7    118.734 1956   67.857\nlongley[, 3] # 整个第 3 列的元素\n [1] 235.6 232.5 368.2 335.1 209.9 193.2 187.0 357.8 290.4 282.2 293.6 468.1\n[13] 381.3 393.1 480.6 400.7\n我们可以用方括号中的冒号来查看数据集的前五行，例如：longley[1:5,]。我们也可以使用方括号中的 c() 函数来显示数据集的第二列和第五列，例如：longley[, c(2,5)]。\n💻请试试查看 longley 数据集的所有列并显示第 10 到 15 行。接着显示数据集的所有列但只显示第 4 和第 7 行。\n\n\nReveal answer\n\nlongley[10:15, ] # 数据集的所有列并显示第 10 到 15 行\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1956        104.6 419.180      282.2        285.7    118.734 1956   67.857\n1957        108.4 442.769      293.6        279.8    120.445 1957   68.169\n1958        110.8 444.546      468.1        263.7    121.950 1958   66.513\n1959        112.6 482.704      381.3        255.2    123.366 1959   68.655\n1960        114.2 502.601      393.1        251.4    125.368 1960   69.564\n1961        115.7 518.173      480.6        257.2    127.852 1961   69.331\nlongley[c(4, 7), ] # 数据集的所有列但只显示第 4 和第 7 行\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1950         89.5 284.599      335.1        165.0    110.929 1950   61.187\n1953         99.0 365.385      187.0        354.7    115.094 1953   64.989\n\n\n\n\n\n我们也可以通过逻辑值和逻辑运算符进行子集提取。R 中有两个用于表示逻辑值的特殊表示：TRUE 和 FALSE。R 还有许多逻辑运算符，例如：大于&gt;、小于&lt; 和 等于==。\n当我们将逻辑运算符应用于一个对象时，返回的值应该是一个逻辑值。例如：\n2 &gt; 1\n[1] TRUE\n2 &lt; 1\n[1] FALSE\n在这里，当我们询问 R 是否 2 &gt; 1，R 返回逻辑值 TRUE；当我们询问 2 &lt; 1 时，R 返回逻辑值 FALSE。\n在子集提取中，逻辑操作非常有用，因为它们可以用来指定我们希望返回向量或数据框中的哪些元素。例如，假设我们只想使用 longley 数据集中 1955 年及以后的数据。为了将数据子集提取为这些观测值，我们可以这样做：\nlongley[longley$Year &gt; 1954,]\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1955        101.2 397.469      290.4        304.8    117.388 1955   66.019\n1956        104.6 419.180      282.2        285.7    118.734 1956   67.857\n1957        108.4 442.769      293.6        279.8    120.445 1957   68.169\n1958        110.8 444.546      468.1        263.7    121.950 1958   66.513\n1959        112.6 482.704      381.3        255.2    123.366 1959   68.655\n1960        114.2 502.601      393.1        251.4    125.368 1960   69.564\n1961        115.7 518.173      480.6        257.2    127.852 1961   69.331\n1962        116.9 554.894      400.7        282.7    130.081 1962   70.551\n这里发生了什么？让我们来慢慢分析一下这段代码： - 我们使用 $ 符号从数据框 longley 中提取变量 Year - 我们要求 R 告诉我们该变量中哪些观测值大于 1954 - 我们使用 [ , ]对数据框 longley 进行子集提取，只保留满足该条件的观测值\n如果我们只对方括号中的代码进行求值，就可以看到更详细的情况：\nlongley$Year &gt; 1954\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE\n你可以看到，R 对于 Year 大于 1954 的观测值返回 TRUE，否则返回 FALSE。正是这个逻辑向量被用在方括号中，用于对子集提取 data.frame。\n💻现在轮到你来试试！请使用逻辑运算符，将 longley 数据集子集化，仅保留那些 Population小于 115 的观测值。\n\n\nReveal answer\n\nlongley[longley$Population &lt; 115, ] \n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1947         83.0 234.289      235.6        159.0    107.608 1947   60.323\n1948         88.5 259.426      232.5        145.6    108.632 1948   61.122\n1949         88.2 258.054      368.2        161.6    109.773 1949   60.171\n1950         89.5 284.599      335.1        165.0    110.929 1950   61.187\n1951         96.2 328.975      209.9        309.9    112.075 1951   63.221\n1952         98.1 346.999      193.2        359.4    113.270 1952   63.639\n\n\n\n\n\n现在我们已经学习了函数、子集提取和对象赋值的相关内容，现在我们要将这三者结合起来使用。\n🖊️让我们试着比较一下 1955 年之前和之后美国的 GNP 平均水平。在查看下方答案前，先试试使用你上面学到的工具来完成这个任务。\n\n\nReveal answer\n\nmean_gnp_pre_55 &lt;- mean(longley[longley$Year &lt; 1955,]$GNP)\nmean_gnp_post_55 &lt;- mean(longley[longley$Year &gt;= 1955,]$GNP)\n\nmean_gnp_pre_55\nmean_gnp_post_55\n[1] 305.1049\n[1] 470.292\n这段代码中发生了什么？对于上面结果的第一行，我们可以按如下方式描述每个步骤： - longley$Year &lt; 1955 选取了 longley 中 Year 小于 1955 的那些行 - $GNP 选取了 GNP 变量 - mean() 计算了该变量的平均值 - mean_gnp_pre_55 &lt;- 将该计算结果赋值给对象 mean_gnp_pre_55\n我们在第二行中只是对不同的数据子集重复了相同的步骤，其中 longley$Year &gt;= 1955 选取了 Year 大于或等于 1955 的那些行。\n\n\n\n\n\n\nR 中的线性回归是通过 lm() 函数实现的。使用lm() 函数需要知道两件事：a) 我们要建模的关系；b) 包含观测数据的数据集。\n我们需要为 lm() 函数提供的两个参数如下所述：\n\n\n\n\n\n\n\n参数 (Argument)\n描述 (Description)\n\n\n\n\nformula\nformula 用于描述因变量与自变量之间的关系，例如 dependent.variable ~ independent.variable\n\n\ndata\n数据集中包含相关变量的名称\n\n\n\n要了解 lm() 函数的更多信息，也可以在 R 中输入 help(lm)。\n💻延续上面的示例，请运行一个线性回归，其中 GNP 是因变量，自变量是一个二元变量：当 Year ≥ 1955 时取值为 1，否则为 0（你需要自行对该变量进行编码）。 再次提醒：在查看下面的答案前，请尽量自己尝试完成这个任务。\n\n\nReveal answer\n\n# 编码二元自变量\nlongley$post_1955 &lt;- longley$Year &gt;= 1955\n\n# 运行回归\ngnp_ols &lt;- lm(GNP ~ post_1955, longley)\n\n# 概述回归结果\nsummary(gnp_ols)\nCall:\nlm(formula = GNP ~ post_1955, data = longley)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-72.823 -46.022  -4.047  43.391  84.602 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     305.10      18.67  16.341 1.63e-10 ***\npost_1955TRUE   165.19      26.40   6.256 2.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.81 on 14 degrees of freedom\nMultiple R-squared:  0.7365,    Adjusted R-squared:  0.7177 \nF-statistic: 39.14 on 1 and 14 DF,  p-value: 2.105e-05\n回归系数是 165.19 这和上面 “手动”计算的均值差相同。\n\n\n\n\n\nt 检验（t-test） 是一种常用的统计方法，用于比较两个样本或一个样本与一个已知值之间的均值差异是否显著。它主要用于样本量较小且总体标准差未知的情况下。t 检验根据样本的均值差异、样本大小和方差来计算一个 t 统计量，然后根据这个统计量与 t 分布进行比较，以判断差异是否显著（即是否可能是由于随机误差造成的）。是否显著由 p 值 决定（通常以 p &lt; 0.05 为显著性水平的标准）。\n让我们在 R 中用 t 检验 来计算均值差异，使用 t.test() 函数。该函数的语法如下：\nt.test(formula, mu, alt, conf)\n\n\n\n\n\n\n\n参数 (Arguments)\n描述 (Description)\n\n\n\n\nformula\n用于描述因变量和自变量之间的关系，例如：dependent.variable ~ independent.variable`。我们将在 t 检验中使用该公式以比较均值差异。 | |mu| 设置原假设。原假设为总体均值之间的真实差异为 0，因此我们设置mu = 0。 | |alt| 设置备择假设。假设均值差异为 0 的情况下，备择可以是差异更大或更小。若需对两者同时进行检验，设置为alt = “two.sided”。 | |conf``\n\n\n\n让我们使用 t.test() 函数比较 1955 年之前与1955 年及之后两个时期的 GNP 平均水平:\nt.test(longley$GNP ~ longley$post_1955, mu = 10000, alt = \"two.sided\") \n    Welch Two Sample t-test\n\ndata:  longley$GNP by longley$post_1955\nt = -384.98, df = 13.992, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 10000\n95 percent confidence interval:\n -221.8222 -108.5521\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           305.1049            470.2920"
  },
  {
    "objectID": "简单R入门.html#rstudio",
    "href": "简单R入门.html#rstudio",
    "title": "简单R入门",
    "section": "",
    "text": "让我们来熟悉一下 R 和 Rstudio。第一次启动 RStudio 时，你会看到三个面板：\n\n接下来让我们来一个个熟悉这些面板"
  },
  {
    "objectID": "简单R入门.html#console-控制台",
    "href": "简单R入门.html#console-控制台",
    "title": "简单R入门",
    "section": "",
    "text": "左侧是控制台（console），这是与 R 交互的最简单方式。你可以在控制台中输入代码（点击光标显示为 &gt; | 的位置），按下 ENTER 键后，R 就会执行该段代码。\n根据你输入的内容，你可能会在控制台中看到相应的输出结果；如果你输入有误，也可能会收到警告信息或错误提示。\n首先，让我们通过将 R 当作一个简单的计算器来熟悉控制台的使用：\n2 + 7\n[1] 9\n5 * 6\n[1] 30\n7 / 2\n[1] 3.5\n\n\n你可以使用键盘上的光标键或方向键在控制台中编辑你的代码： - 使用 ↑（向上） 和 ↓（向下） 键可以重新运行之前输入的代码，而无需重新输入 - 使用 ↑（向上） 和 ↓（向下） 键可以重新运行之前输入的代码，而无需重新输入"
  },
  {
    "objectID": "简单R入门.html#脚本scripts",
    "href": "简单R入门.html#脚本scripts",
    "title": "简单R入门",
    "section": "",
    "text": "控制台非常适合处理简单任务，但如果你正在进行一个项目，你会通常希望将工作保存在某种文档或文件中。R 中的脚本（script）就是包含 R 代码的纯文本文件。你可以像编辑任何文字处理或笔记应用中的文件一样编辑脚本。\n我们建议你在本课程中始终使用脚本文件进行操作。\n请使用菜单栏或工具栏按钮（如下图所示）来创建一个新的脚本文件:\n\n创建脚本后，请立即为其赋予一个有意义的文件名并保存。\n请注意 RStudio 中的脚本窗口，特别是其中标有 Run 和 Source 的两个按钮：\n\n💡从脚本中运行代码有几种不同的方式： | 执行方式 | 操作说明 | |—————-|————————————————————————–| | 单行执行 | 将光标放在你要运行的行上，按下 CTRL + ENTER，或点击 Run 按钮 | | 多行执行 | 选中你要运行的多行代码，按下 CTRL + ENTER，或点击 Run 按钮 | | 整个脚本执行 | 点击 Source 按钮 |"
  },
  {
    "objectID": "简单R入门.html#对象与赋值objects-assignment",
    "href": "简单R入门.html#对象与赋值objects-assignment",
    "title": "简单R入门",
    "section": "",
    "text": "R 语言中操作的基本结构称为“对象（objects）”。创建对象是将信息存储在 R 中的一种方式，我们可以为任何对象赋予任意名称。一旦创建了对象，我们就可以在后续的多种任务中使用它。\n让我们从创建一个对象开始，该对象用于存储一个简单加法的结果。我们使用赋值运算符 &lt;- 来创建或更新对象。例如，如果我们希望保存 10 + 4 的结果，可以这样写：\nmy_result &lt;- 10 + 4\n上面这一行代码创建了一个名为 my_result 的新对象，并将 10 + 4 的结果保存在其中。要查看 my_result 中的内容，只需在控制台中输入它的名字：\nmy_result\n[1] 14\n请注意，所有对象名称在 R 中都是区分大小写的，因此如果你在控制台中输入 My_Result，你将会看到如下报错信息：\nError in eval(expr, envir, enclos): object 'My_Result' not found\n对象最有用的地方在于：一旦创建，就可以在后续的计算中使用它们。例如：\nmy_result*2\n[1] 28\n你甚至可以对一个对象进行计算，并将结果赋值给一个新的对象：\nmy_new_result &lt;- my_result*2\nmy_new_result\n[1] 28\n现在，我们已经创建了两个对象，查看 RStudio 中的 Environment 面板，你会看到其中列出了 my_result 和 my_new_result 两个对象。\n\n要从环境中删除所有对象，可以使用如上图所示的扫帚按钮（broom button）。\n我们将对象命名为 my_result，但其实你可以使用任何名称，只要遵守以下几个简单的规则即可： - 对象名称可以包含大小写字母（A–Z，a–z）、数字（0–9）、下划线（_）或句点（.） - 但对象名称必须以字母开头 - 建议选择具有描述性且易于输入的名称，以便提高代码可读性与可维护性\n\n\n\n\n\n\n\n推荐的对象名\n不推荐的对象名\n\n\n\n\nresult\na\n\n\nmy_result\nx1\n\n\nmy.result\nthis.name.is.just.too.long\n\n\nmy_new_result\nsing,dance,rap,basketball"
  },
  {
    "objectID": "简单R入门.html#向量vectors",
    "href": "简单R入门.html#向量vectors",
    "title": "简单R入门",
    "section": "",
    "text": "my_result 和 my_new_result 都是只包含单个数字的对象。但在实际操作中，我们常常需要处理相互关联的一组数字。为此，我们需要使用的基本构建模块就是向量（vectors）。\n向量就是一组信息（通常是数字，但也可以是字符字符串或逻辑值），按特定顺序组合在一起。\n在 R 中创建向量的一种方式是使用 c() 函数，它可以将多个数值“连接”（concatenate）在一起。例如，我们可以将以下数字连接成一个向量：\nmy_first_vector &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\nmy_first_vector\n [1]  0  1  1  2  3  5  8 13 21 34\n向量中数值的存储顺序是重要的，我们可以使用方括号 [ ] 来访问向量中的单个元素。例如，如果我们希望访问刚刚创建的向量中的第 3 个元素，可以这样操作：\nmy_first_vector[3]\n [1]  1\n这是一个向量子集提取（subsetting）的基本示例，它可以让我们访问向量中我们想使用的部分。我们还可以使用一个向量来提取另一个向量的子集，例如my_first_vector[c(1,3,5)] 将返回我们向量中的第 1、3 和 5 个元素。\nmy_first_vector[c(1,3,5)]\n [1] 0 1 3"
  },
  {
    "objectID": "简单R入门.html#函数vectors",
    "href": "简单R入门.html#函数vectors",
    "title": "简单R入门",
    "section": "",
    "text": "函数是一组用于执行特定任务的指令。函数通常需要一些输入，并生成某种输出。例如，我们可以不用加号运算符 +，而使用 sum 函数来对两个或多个数字进行求和。让我们来试试将我们上面创建的向量中的所有元素相加：\nsum(my_first_vector)\n[1] 88\n这里我们将向量作为 sum() 函数的输入，输出结果是 88。你可以手动检查这个结果是否正确，或者直接相信 R 能够正确地计算这些数字的总和。\n函数的调用必须使用圆括号 ()。传递给函数的输入称为参数（arguments），它们被放在括号内。函数的输出通常会显示在屏幕上，但我们也可以选择将其输出结果保存下来。例如：\nvec_sum &lt;- sum(my_first_vector)\nvec_sum\n[1] 88\n在这里，vec_sum 也是一个对象！所以我们对一些数据（my_first_vector）执行了一个计算（sum()），并将结果（vec_sum）存储了下来。\n尝试对我们创建的向量应用其他函数。例如，你可以尝试使用 mean()、median()、max() 和 min()。这些结果是否与你的预期一致？\n⚠️请注意，R 中的函数名称也是区分大小写的！ 这意味着，mean(my_first_vector) 可以正确计算向量的平均值，而 Mean(my_first_vector) 则会报错。\nmean(my_first_vector)\nmedian(my_first_vector)\nmax(my_first_vector)\nmin(my_first_vector)\n[1] 8.8\n[1] 4\n[1] 34\n[1] 0"
  },
  {
    "objectID": "简单R入门.html#帮助help",
    "href": "简单R入门.html#帮助help",
    "title": "简单R入门",
    "section": "",
    "text": "在控制台右下角，你会看到一个面板，包含名为 Plots、Packages、Help 和 Viewer 的多个标签页。现在我们暂时用不到大部分内容，但让我们先了解一下 Help 面板。\n在 R 中，任何函数都有对应的帮助文件。例如，如果我们想了解如何使用 sum() 函数，可以输入：\nhelp(sum)\n问号 ? 也可以作为快捷方式来访问在线帮助:\n?sum\n\n你可以使用上图所示的工具栏按钮，将帮助内容展开并在新窗口中显示。\nR 中函数的帮助页面遵循一致的结构，通常包括以下几个部分：\n\n\n\n\n\n\n\n项目\n说明\n\n\n\n\nDescription\n函数的简要描述\n\n\nUsage\n包括所有参数（输入）的完整语法或用法\n\n\nArguments\n每个参数的解释\n\n\nDetails\n关于该函数及其参数的任何相关详细信息\n\n\nValue\n函数的输出值\n\n\nExamples\n使用该函数的示例"
  },
  {
    "objectID": "简单R入门.html#data.frames-数据框",
    "href": "简单R入门.html#data.frames-数据框",
    "title": "简单R入门",
    "section": "",
    "text": "data.frame 是一种以表格形式存储数据的对象，类似于电子表格的结构，其中每一列代表一个变量，每一行代表一个观测单位。\n虽然你可以手动创建一个 data.frame，但在大多数情况下，你会从文件中加载一个数据集，而在 R 中它将被表示为一个 data.frame。不过现在我们将使用一个 R 中预装的数据集。\n让我们来看一个名为 longley 的宏观经济数据集。为此，请在你的 R 脚本中运行以下代码：\ndata(longley)\nlongley 数据集是一个包含 7 个变量 和 16 个观测值 的 data.frame。\nhelp(longley)\n帮助页面对这 7 个变量中的每一个都进行了描述。现在让我们来看一下 longley 数据集中的内容:\nlongley\n\n\nClick to show output\n\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1947         83.0 234.289      235.6        159.0    107.608 1947   60.323\n1948         88.5 259.426      232.5        145.6    108.632 1948   61.122\n1949         88.2 258.054      368.2        161.6    109.773 1949   60.171\n1950         89.5 284.599      335.1        165.0    110.929 1950   61.187\n1951         96.2 328.975      209.9        309.9    112.075 1951   63.221\n1952         98.1 346.999      193.2        359.4    113.270 1952   63.639\n1953         99.0 365.385      187.0        354.7    115.094 1953   64.989\n1954        100.0 363.112      357.8        335.0    116.219 1954   63.761\n1955        101.2 397.469      290.4        304.8    117.388 1955   66.019\n1956        104.6 419.180      282.2        285.7    118.734 1956   67.857\n1957        108.4 442.769      293.6        279.8    120.445 1957   68.169\n1958        110.8 444.546      468.1        263.7    121.950 1958   66.513\n1959        112.6 482.704      381.3        255.2    123.366 1959   68.655\n1960        114.2 502.601      393.1        251.4    125.368 1960   69.564\n1961        115.7 518.173      480.6        257.2    127.852 1961   69.331\n1962        116.9 554.894      400.7        282.7    130.081 1962   70.551\n\n\n\n我们也可以使用 View 函数查看 longley 数据集，该函数会以类似电子表格的形式显示数据框:\nView(longley)"
  },
  {
    "objectID": "简单R入门.html#子集提取subsetting",
    "href": "简单R入门.html#子集提取subsetting",
    "title": "简单R入门",
    "section": "",
    "text": "在分析 data.frame 时，我们通常希望对数据进行子集提取。也就是说，我们常常只想从数据中选择某些特定的行或特定的列。\n\n\n访问 data.frame 中单个列的最简单方法是使用符号 $。例如，我们来看如何只访问 Year 这一列：\nlongley$Year\n [1] 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961\n[16] 1962\n没错，这是一个向量（vector）！也就是说，它是一组按顺序连接在一起的信息。因此，我们可以像前面示例中那样，访问该向量中的特定元素。例如：\nlongley$Year[7]\n[1] 1953\nlongley$Year 向量的第七个元素是 1953。\n现在，请你试着使用 $ 来访问 GNP 变量,请查看其第 1、2 和 10 个元素的值：\n\n\nReveal answer\n\nlongley$Year[7]\n [1] 234.289 259.426 258.054 284.599 328.975 346.999 365.385 363.112 397.469\n[10] 419.180 442.769 444.546 482.704 502.601 518.173 554.894  \nlongley$GNP[c(1,2,10)]\n[1] 234.289 259.426 419.180\n\n\n\n\n\n我们之前提到，可以使用方括号 [ ] 对向量进行子集提取。 在处理 data.frame 时，我们通常希望访问某些特定的观测值（行）、变量（列），或两者的组合，而不是一次性查看整个数据集。我们也可以使用方括号 [,] 对数据框进行子集提取。\n在方括号中，我们用逗号分隔行坐标和列坐标。行坐标写在前，列坐标写在后。例如： - longley[10, 3] 返回数据框第 10 行第 3 列 的内容。 - 如果省略列坐标，表示取该行的所有列：longley[10, ] 返回第 10 行 的所有数据。 - 如果省略行坐标，表示取该列的所有行：longley[, 3] 返回第 3 列 的全部内容。\nlongley[10, 3] # 第 10 行第 3 列中的元素\n[1] 282.2\nlongley[10, ] # 整个第 10 行的元素\n     GNP.deflator    GNP Unemployed Armed.Forces Population Year Employed\n1956        104.6 419.18      282.2        285.7    118.734 1956   67.857\nlongley[, 3] # 整个第 3 列的元素\n [1] 235.6 232.5 368.2 335.1 209.9 193.2 187.0 357.8 290.4 282.2 293.6 468.1\n[13] 381.3 393.1 480.6 400.7\n我们可以用方括号中的冒号来查看数据集的前五行，例如：longley[1:5,]。我们也可以使用方括号中的 c() 函数来显示数据集的第二列和第五列，例如：longley[, c(2,5)]。\n💻请试试查看 longley 数据集的所有列并显示第 10 到 15 行。接着显示数据集的所有列但只显示第 4 和第 7 行。\n\n\nReveal answer\n\nlongley[10:15, ] # 数据集的所有列并显示第 10 到 15 行\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1956        104.6 419.180      282.2        285.7    118.734 1956   67.857\n1957        108.4 442.769      293.6        279.8    120.445 1957   68.169\n1958        110.8 444.546      468.1        263.7    121.950 1958   66.513\n1959        112.6 482.704      381.3        255.2    123.366 1959   68.655\n1960        114.2 502.601      393.1        251.4    125.368 1960   69.564\n1961        115.7 518.173      480.6        257.2    127.852 1961   69.331\nlongley[c(4, 7), ] # 数据集的所有列但只显示第 4 和第 7 行\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1950         89.5 284.599      335.1        165.0    110.929 1950   61.187\n1953         99.0 365.385      187.0        354.7    115.094 1953   64.989\n\n\n\n\n\n我们也可以通过逻辑值和逻辑运算符进行子集提取。R 中有两个用于表示逻辑值的特殊表示：TRUE 和 FALSE。R 还有许多逻辑运算符，例如：大于&gt;、小于&lt; 和 等于==。\n当我们将逻辑运算符应用于一个对象时，返回的值应该是一个逻辑值。例如：\n2 &gt; 1\n[1] TRUE\n2 &lt; 1\n[1] FALSE\n在这里，当我们询问 R 是否 2 &gt; 1，R 返回逻辑值 TRUE；当我们询问 2 &lt; 1 时，R 返回逻辑值 FALSE。\n在子集提取中，逻辑操作非常有用，因为它们可以用来指定我们希望返回向量或数据框中的哪些元素。例如，假设我们只想使用 longley 数据集中 1955 年及以后的数据。为了将数据子集提取为这些观测值，我们可以这样做：\nlongley[longley$Year &gt; 1954,]\n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1955        101.2 397.469      290.4        304.8    117.388 1955   66.019\n1956        104.6 419.180      282.2        285.7    118.734 1956   67.857\n1957        108.4 442.769      293.6        279.8    120.445 1957   68.169\n1958        110.8 444.546      468.1        263.7    121.950 1958   66.513\n1959        112.6 482.704      381.3        255.2    123.366 1959   68.655\n1960        114.2 502.601      393.1        251.4    125.368 1960   69.564\n1961        115.7 518.173      480.6        257.2    127.852 1961   69.331\n1962        116.9 554.894      400.7        282.7    130.081 1962   70.551\n这里发生了什么？让我们来慢慢分析一下这段代码： - 我们使用 $ 符号从数据框 longley 中提取变量 Year - 我们要求 R 告诉我们该变量中哪些观测值大于 1954 - 我们使用 [ , ]对数据框 longley 进行子集提取，只保留满足该条件的观测值\n如果我们只对方括号中的代码进行求值，就可以看到更详细的情况：\nlongley$Year &gt; 1954\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE\n你可以看到，R 对于 Year 大于 1954 的观测值返回 TRUE，否则返回 FALSE。正是这个逻辑向量被用在方括号中，用于对子集提取 data.frame。\n💻现在轮到你来试试！请使用逻辑运算符，将 longley 数据集子集化，仅保留那些 Population小于 115 的观测值。\n\n\nReveal answer\n\nlongley[longley$Population &lt; 115, ] \n     GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n1947         83.0 234.289      235.6        159.0    107.608 1947   60.323\n1948         88.5 259.426      232.5        145.6    108.632 1948   61.122\n1949         88.2 258.054      368.2        161.6    109.773 1949   60.171\n1950         89.5 284.599      335.1        165.0    110.929 1950   61.187\n1951         96.2 328.975      209.9        309.9    112.075 1951   63.221\n1952         98.1 346.999      193.2        359.4    113.270 1952   63.639\n\n\n\n\n\n现在我们已经学习了函数、子集提取和对象赋值的相关内容，现在我们要将这三者结合起来使用。\n🖊️让我们试着比较一下 1955 年之前和之后美国的 GNP 平均水平。在查看下方答案前，先试试使用你上面学到的工具来完成这个任务。\n\n\nReveal answer\n\nmean_gnp_pre_55 &lt;- mean(longley[longley$Year &lt; 1955,]$GNP)\nmean_gnp_post_55 &lt;- mean(longley[longley$Year &gt;= 1955,]$GNP)\n\nmean_gnp_pre_55\nmean_gnp_post_55\n[1] 305.1049\n[1] 470.292\n这段代码中发生了什么？对于上面结果的第一行，我们可以按如下方式描述每个步骤： - longley$Year &lt; 1955 选取了 longley 中 Year 小于 1955 的那些行 - $GNP 选取了 GNP 变量 - mean() 计算了该变量的平均值 - mean_gnp_pre_55 &lt;- 将该计算结果赋值给对象 mean_gnp_pre_55\n我们在第二行中只是对不同的数据子集重复了相同的步骤，其中 longley$Year &gt;= 1955 选取了 Year 大于或等于 1955 的那些行。"
  },
  {
    "objectID": "简单R入门.html#线性回归linear-regression",
    "href": "简单R入门.html#线性回归linear-regression",
    "title": "简单R入门",
    "section": "",
    "text": "R 中的线性回归是通过 lm() 函数实现的。使用lm() 函数需要知道两件事：a) 我们要建模的关系；b) 包含观测数据的数据集。\n我们需要为 lm() 函数提供的两个参数如下所述：\n\n\n\n\n\n\n\n参数 (Argument)\n描述 (Description)\n\n\n\n\nformula\nformula 用于描述因变量与自变量之间的关系，例如 dependent.variable ~ independent.variable\n\n\ndata\n数据集中包含相关变量的名称\n\n\n\n要了解 lm() 函数的更多信息，也可以在 R 中输入 help(lm)。\n💻延续上面的示例，请运行一个线性回归，其中 GNP 是因变量，自变量是一个二元变量：当 Year ≥ 1955 时取值为 1，否则为 0（你需要自行对该变量进行编码）。 再次提醒：在查看下面的答案前，请尽量自己尝试完成这个任务。\n\n\nReveal answer\n\n# 编码二元自变量\nlongley$post_1955 &lt;- longley$Year &gt;= 1955\n\n# 运行回归\ngnp_ols &lt;- lm(GNP ~ post_1955, longley)\n\n# 概述回归结果\nsummary(gnp_ols)\nCall:\nlm(formula = GNP ~ post_1955, data = longley)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-72.823 -46.022  -4.047  43.391  84.602 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     305.10      18.67  16.341 1.63e-10 ***\npost_1955TRUE   165.19      26.40   6.256 2.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.81 on 14 degrees of freedom\nMultiple R-squared:  0.7365,    Adjusted R-squared:  0.7177 \nF-statistic: 39.14 on 1 and 14 DF,  p-value: 2.105e-05\n回归系数是 165.19 这和上面 “手动”计算的均值差相同。"
  },
  {
    "objectID": "简单R入门.html#t-tests",
    "href": "简单R入门.html#t-tests",
    "title": "简单R入门",
    "section": "",
    "text": "t 检验（t-test） 是一种常用的统计方法，用于比较两个样本或一个样本与一个已知值之间的均值差异是否显著。它主要用于样本量较小且总体标准差未知的情况下。t 检验根据样本的均值差异、样本大小和方差来计算一个 t 统计量，然后根据这个统计量与 t 分布进行比较，以判断差异是否显著（即是否可能是由于随机误差造成的）。是否显著由 p 值 决定（通常以 p &lt; 0.05 为显著性水平的标准）。\n让我们在 R 中用 t 检验 来计算均值差异，使用 t.test() 函数。该函数的语法如下：\nt.test(formula, mu, alt, conf)\n\n\n\n\n\n\n\n参数 (Arguments)\n描述 (Description)\n\n\n\n\nformula\n用于描述因变量和自变量之间的关系，例如：dependent.variable ~ independent.variable`。我们将在 t 检验中使用该公式以比较均值差异。 | |mu| 设置原假设。原假设为总体均值之间的真实差异为 0，因此我们设置mu = 0。 | |alt| 设置备择假设。假设均值差异为 0 的情况下，备择可以是差异更大或更小。若需对两者同时进行检验，设置为alt = “two.sided”。 | |conf``\n\n\n\n让我们使用 t.test() 函数比较 1955 年之前与1955 年及之后两个时期的 GNP 平均水平:\nt.test(longley$GNP ~ longley$post_1955, mu = 10000, alt = \"two.sided\") \n    Welch Two Sample t-test\n\ndata:  longley$GNP by longley$post_1955\nt = -384.98, df = 13.992, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 10000\n95 percent confidence interval:\n -221.8222 -108.5521\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           305.1049            470.2920"
  },
  {
    "objectID": "Seminar/Seminar2.html",
    "href": "Seminar/Seminar2.html",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n\n Seminar data \n\n\n美国宪法是否影响了其他国家的宪法？目前越来越多的学术观点认为，美国宪法的影响力已随着时间推移而减弱，因为它日益偏离了全球范围内关于人权对宪法秩序重要性的共识。然而，关于美国宪法在多大程度上影响了世界各国成文宪法的修订和采纳，目前仍缺乏实证和系统的认知。\nDavid S. Law 和 Mila Versteeg 在2012年的一项研究中，通过实证调查了美国宪法的影响力，结果表明，近几十年来，其他国家在其本国宪法中效仿美国宪法中与权利相关的条款的可能性越来越小。在本系列问题中，我们将运用本周所学的一些方法来复现其部分分析。\n\n\n\n在开始研讨会时，先下载/加载以下 R 包：\nlibrary(tidyverse)\nlibrary(quanteda)\n# 如果无法加载quanteda.textplots和quanteda.textstats包，请运行以下代码：\n# devtools::install_github(\"quanteda/quanteda.textplots\")\n# devtools::install_github(\"quanteda/quanteda.textstats\")\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)  \n\n\n\n我们将使用如下数据集：宪法序言 — constitutions.csv\n该文件包含155部宪法（英文译本）的序言。数据包含以下变量：\nconstituiton 数据集中的变量说明\n\n\n\n变量\n说明\n\n\n\n\ncountry\n国家名称\n\n\ncontinent\n所在洲（大洲）\n\n\nyear\n宪法制定的年份\n\n\npreamble\n宪法序言的文本内容\n\n\n\n一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：\nconstitutions &lt;- read_csv(\"constitutions.csv\") \n你可以使用 tidyverse 包中的 glimpse() 函数快速查看数据中的变量：\nglimpse(constitutions)\nRows: 155\nColumns: 4\n$ country   &lt;chr&gt; \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"a…\n$ continent &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"Europe\", \"Africa\", \"Americas\", …\n$ year      &lt;dbl&gt; 2004, 1998, 1989, 1993, 2010, 1981, 1853, 1995, 1995, 1973, …\n$ preamble  &lt;chr&gt; \"In the name of Allah, the Most Beneficent, the Most Mercifu…\n\n\n\n\n探索 constitutions 对象，以了解我们正在处理的数据。preambles 变量中存储的文本平均长度是多少？哪个国家的序言文本最长？哪个国家的最短？这些序言的平均长度是否随着时间发生了变化？\n\n\n\nReveal Code\n\n# 序言文本平均长度：\nconstitutions$preamble_length &lt;- ntoken(tokens(constitutions$preamble))\nmean(constitutions$preamble_length)\n[1] 324.3097                               \n# 文本最长的国家\nconstitutions$country[which.max(constitutions$preamble_length)]\n[1] \"iran_islamic_republic_of\"                             \n# 文本最短的国家\nconstitutions$country[which.min(constitutions$preamble_length)]\n[1] \"greece\"                            \nconstitutions %&gt;%\n  ggplot(aes(x = year,\n             y = preamble_length)) +\n  geom_point() + \n  xlab(\"Year\") + \n  ylab(\"Preamble Length\") + \n  theme_bw() \n\n\n\n将 constitutions 数据框转换为 corpus() 对象，然后进一步转换为 dfm() 对象（注意：你还需要使用 tokens() 函数）。同时，进行一些合理的特征选择决策。\n\n\n\nReveal Code\n\nconstitutions_dfm &lt;- constitutions %&gt;% \n                        corpus(text_field = \"preamble\") %&gt;%\n                        tokens(remove_punct = T) %&gt;%\n                        dfm() %&gt;%\n                        dfm_remove(stopwords(\"en\"))\n\n\n使用 topfeatures() 函数找出美国宪法中最常出现的 10 个特征词。将这些特征与你选择的其他三个国家的宪法中的高频特征进行比较。你注意到了什么？\n\n\n\nReveal Code\n\ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"united_states_of_america\",])\n      united    establish       states       people        order         form \n           2            2            2            1            1            1 \n     justice      defense constitution      liberty \n           1            1            1            1  \ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"argentina\",])\nargentine   justice    nation   general     peace    people       god  national \n        3         2         2         2         1         1         1         1 \nestablish   defense \n        1         1   \ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"cuba\",])\n          man       peoples       dignity    revolution          full \n            4             3             3             3             3 \nrevolutionary       victory          mart          last         human \n            3             3             3             2             2    \ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"sudan\",])\n       peace    committed    agreement        sudan constitution    religious \n           4            3            3            3            2            2 \n       shall   governance   agreements          end \n           2            2            2            2   \n\n\n使用 dfm_tfidf() 函数将 tf-idf 权重应用于你的 dfm。然后使用新的矩阵重复上面的练习。你注意到了什么？\n\n\n\nReveal Code\n\nconstitutions_dfm_tf_idf &lt;- constitutions_dfm %&gt;% dfm_tfidf()\n\ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",])\n     insure     america     perfect      states    domestic tranquility \n   2.190332    1.889302    1.889302    1.870118    1.588272    1.588272 \n     ordain   establish     defense   blessings \n   1.412180    1.292527    1.236089    1.190332  \ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"argentina\",])\n   argentine pre-existing constituting        dwell     congress    provinces \n    6.570995     2.190332     2.190332     2.190332     1.889302     1.889302 \n     compose       object     securing      general \n    1.889302     1.889302     1.889302     1.870118  \ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",])\n         mart         cuban           jos        cubans       victory \n     6.570995      4.380663      4.380663      4.380663      4.035701 \nrevolutionary      peasants       workers    revolution          last \n     3.446817      3.426421      3.176543      3.132611      2.824361 \ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"sudan\",])\n        sudan     agreement    agreements          2005      conflict \n     5.667905      4.236541      3.426421      3.426421      3.426421 \ncomprehensive           end     committed      bestowed  definitively \n     2.982723      2.380663      2.326075      2.190332      2.190332 \n\n\n使用 textplot_wordcloud() 函数为美国和另一个国家各制作一个词云 （你会惊叹于这些图有多么丑）。请注意：由于美国宪法的文本非常短，你可能需要将 min_count 参数设置为比默认值 3 更低的数值。\n\n\n\nReveal Code\n\ntextplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",], min_count = 0)\n\ntextplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",], min_count = 2)\n\n\n\n\n\n余弦相似度（cos(θ)）用于衡量两个向量 a 和 b 之间的相似性，其定义如下：\n\\[\n\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n\\]\n其中，θ 是向量 a 与 b 之间的夹角，\\(\\|\\mathbf{a}\\|\\) 和 \\(\\|\\mathbf{b}\\|\\) 分别表示向量 a 和 b 的模（magnitude）。\n更直观的展开形式如下：\n\\[\n\\cos(\\theta) = \\frac{a_1b_1 + a_2b_2 + \\cdots + a_Jb_J}\n{\\sqrt{a_1^2 + a_2^2 + \\cdots + a_J^2} \\times \\sqrt{b_1^2 + b_2^2 + \\cdots + b_J^2}}\n\\]\n\n编写一个用于计算两个向量之间余弦相似度的函数。我们可以使用 function() 函数来定义新函数。例如，如果我们想定义一个用于计算平均值的新函数，可以写成：mean_func &lt;- function(x) sum(x)/length(x), 其中 x 是一个向量。\n\n\n\nReveal Code\n\ncosine_sim &lt;- function(a, b){\n  \n  # 计算两个向量的内积\n  numerator &lt;- sum(a * b)\n  \n  # 计算第一个向量的模\n  magnitude_a &lt;- sqrt(sum(a^2))\n  \n  # 计算第二个向量的模\n  magnitude_b &lt;- sqrt(sum(b^2))\n  \n  # 分母的计算\n  denominator &lt;- magnitude_a * magnitude_b\n\n  # 余弦相似度\n  cos_sim &lt;- numerator/denominator\n  \n  return(cos_sim)\n  \n}\n\n\n使用你在上面创建的函数，计算美国宪法序言与另一个你选择的国家的宪法序言之间的余弦相似度。请使用 tf-idf 加权版本的 dfm 来完成这个任务。为确保你的函数工作正常，可将其结果与 textstat_simil() 函数计算出的余弦相似度进行比较。使用 textstat_simil() 时，需要传入 x 向量和 y 向量，并指定 method = \"cosine\"，以计算余弦相似度（而不是其他相似度指标）。\n\n\n\nReveal Code\n\ncosine_sim(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",],\n           constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",])\n[1] 0.02427221\ntextstat_simil(x = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",], \n                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",],\n                             method = \"cosine\")\ntextstat_simil object; method = \"cosine\"\n       text149\ntext36  0.0243\n\n棒极了！看来这个功能成功了。\n\n\n\n使用 textstat_simil() 函数计算美国宪法序言与数据中所有其他序言之间的余弦相似度。(你也可以为该函数提供一个 x 矩阵和一个 y 向量，这样就能计算 x 中所有行与 y 的相似度。)使用 as.numeric() 函数将该函数的输出赋值回原始的 constitutions 数据框。哪三个宪法与美国最相似？哪三个最不相似？(使用 order() 函数来完成这项操作)\n\n\n\nReveal Code\n\n# 计算余弦相似度\ncosine_sim &lt;- textstat_simil(x = constitutions_dfm_tf_idf, \n                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",],\n                             method = \"cosine\")\n\n\n# 将变量赋值给 data.frame\nconstitutions$cosine_sim &lt;- as.numeric(cosine_sim)\n\n# 找出最相似和最不相似的宪法序言\nconstitutions$country[order(constitutions$cosine_sim, decreasing = T)][1:3]\n[1] \"united_states_of_america\" \"argentina\"               \n[3] \"philippines\" \nconstitutions$country[order(constitutions$cosine_sim, decreasing = F)][1:3]\n[1] \"greece\"    \"lithuania\" \"slovenia\" \n\n\n计算从 1950 年代起撰写的所有宪法中，美国宪法与其他国家宪法之间的平均余弦相似度在各个年代的值。\n\n要完成这个问题，你需要处理几个编码细节: - 首先，你需要将 year 变量转换为 decade 变量。你可以使用 %% “模”运算符来实现，它用于计算两个数值变量相除后的余数。例如，1986 %% 10 将返回值 6。如果你从原始年份中减去这个值，就会得到正确的年代（即 1986 - 6 = 1980）。 - 其次，你需要计算上一个问题中创建的余弦相似度变量的年代平均值。为此，你应使用 group_by() 和 summarise() 函数。group_by() 允许你指定进行汇总时所依据的变量，而 summarise() 函数允许你指定要使用哪种类型的汇总（即这里你应使用 mean() 函数）。\n\n\nReveal Code\n\n# 创建 decade 变量\nconstitutions$decade &lt;- constitutions$year - (constitutions$year %% 10)\n\n# 按 decade 计算平均相似度\ncosine_sim_by_year &lt;- constitutions %&gt;%\n  filter(year &gt;= 1950) %&gt;%\n  group_by(decade) %&gt;%\n  summarise(cosine_sim = mean(cosine_sim)) \n\n\n创建一张折线图（使用 ggplot 中的 geom_line()），将你上面计算的平均值放在 y 轴上，将年代（decades）放在 x 轴上。在最近几十年中，各国宪法的序言是否变得与美国宪法序言越来越不相似了？\n\n\n\nReveal Code\n\ncosine_sim_by_year %&gt;%\n  ggplot(aes(x = decade, y = cosine_sim)) + \n  geom_line() + \n  xlab(\"Decade\") +\n  ylab(\"Tf-idf cosine similarity with US constitution\") + \n  theme_bw()\n\n\n\n\n\n除了计算宪法之间的相似度之外，我们或许还希望描述世界不同地区宪法之间的语言差异。为了刻画这些差异，我们将使用 Fightin’ Words 方法，该方法由 Munroe 等人（2008） 提出。\n回顾Lecture内容，该方法首先计算某个类别中某个词出现的概率（此处以部门为例）：\n\\[\n\\hat{\\mu}_{j,k} = \\frac{W^*_{j,k} + \\alpha_0}{n_k + \\sum_{j=1}^{J}1 + \\alpha_0}\n\\]\n其中：\n\n\\(W^*_{j,k}\\) 表示特征 \\(j\\) 在类别 \\(k\\) 的文档中出现的次数\n\n\\(n_k\\) 表示类别 \\(k\\) 的文档中的总词数\n\n\\(\\alpha_0\\) 是平滑参数（“smoothing”），用于将常见词的差异收缩至 0\n\n接着，我们计算类别 \\(k\\) 和 \\(k'\\) 的对数比率（log-odds）：\n\\[\n\\text{log-odds-ratio}_{j,k} = \\log \\left( \\frac{\\hat{\\mu}_{j,k}}{1 - \\hat{\\mu}_{j,k}} \\right) - \\log \\left( \\frac{\\hat{\\mu}_{j,k'}}{1 - \\hat{\\mu}_{j,k'}} \\right)\n\\]\n最后，我们用其 方差（variance） 进行标准化处理（再次降低稀有词的影响）：\n\\[\n\\text{Fightin' Words Score}_j =\n\\frac{\\text{log-odds-ratio}_{j,k}}{\\sqrt{\\text{Var}(\\text{log-odds-ratio}_{j,k})}}\n\\]\n我在下面提供了一个函数，它可以根据数据中任意协变量所定义的一对分组，计算出 Fightin’ Words 分数：\nfightin_words &lt;- function(dfm_input, covariate, group_1 = \"Political Science\", group_2 = \"Economics\", alpha_0 = 1){\n  \n  # 子集化 DFM：仅保留属于 group_1 和 group_2 的文档\n  fw_dfm &lt;- dfm_subset(dfm_input, get(covariate) %in% c(group_1, group_2)) \n  fw_dfm &lt;- dfm_group(fw_dfm, get(covariate))\n  fw_dfm &lt;- fw_dfm[,colSums(fw_dfm)!=0]\n  dfm_input_trimmed &lt;- dfm_match(dfm_input, featnames(fw_dfm))\n  \n  # 计算每个词的先验频率 alpha_w\n  alpha_w &lt;- (colSums(dfm_input_trimmed))*(alpha_0/sum(dfm_input_trimmed))\n   \n  for(i in 1:nrow(fw_dfm)) fw_dfm[i,] &lt;- fw_dfm[i,] + alpha_w\n  fw_dfm &lt;- as.dfm(fw_dfm)\n  mu &lt;- fw_dfm %&gt;% dfm_weight(\"prop\")\n\n  # 计算每个词的 log-odds 比值\n  lo_g1 &lt;- log(as.numeric(mu[group_1,])/(1-as.numeric(mu[group_1,])))\n  lo_g2 &lt;- log(as.numeric(mu[group_2,])/(1-as.numeric(mu[group_2,])))\n  fw &lt;- lo_g1 - lo_g2\n  \n  # 计算每个词的方差\n  fw_var &lt;- as.numeric(1/(fw_dfm[1,])) + as.numeric(1/(fw_dfm[2,]))\n  \n  # 计算标准化得分（z 分数），以及每个词的频次和名称\n  fw_scores &lt;- data.frame(score = fw/sqrt(fw_var),\n                          n = colSums(fw_dfm),\n                          feature = featnames(fw_dfm))\n\n  return(fw_scores)\n  \n}\n\n\n\n\n\n\n\n\n\n参数名称\n说明\n\n\n\n\ndfm_input\n一个 dfm（文档-词项矩阵），用于测量每篇文档中词语的频率。\n\n\ncovariate\n协变量的名称（应以字符串形式输入，如 \"covariate_name\"），其中包含分组标签（例如 “continent”）。\n\n\ngroup_1\n第一个分组的名称。\n\n\ngroup_2\n第二个分组的名称。\n\n\nalpha_0\n正则化参数，必须是正数。\n\n\n\n该函数返回一个 data.frame，其中的每一行表示在文本中出现的一个特征（词项），与两个分组相关，包含以下三个变量：\n\n\n\n\n\n\n\n变量名称\n说明\n\n\n\n\nfeature\n词项名称。\n\n\nfw\n每个词项对应的 Fightin’ Words 分数。\n\n\nn\n每个词项在两个分组的所有文档中出现的总次数。\n\n\n\n\n使用上面介绍的的函数，计算区分非洲与欧洲国家宪法的 Fightin’ Words 分数，并回答以下问题：(a)哪 10 个词最强烈地与非洲国家的宪法相关？(b)哪 10 个词最强烈地与欧洲国家的宪法相关？\n\n\n\nReveal Code\n\nfw_scores_africa_europe &lt;- fightin_words(dfm_input = constitutions_dfm, \n                                         covariate = \"continent\", \n                                         group_1 = \"Africa\", \n                                         group_2 = \"Europe\", \n                                         alpha_0 = 1)\n\nfw_scores_africa_europe$feature[order(fw_scores_africa_europe$score, decreasing = T)][1:10]\n [1] \"political\"  \"unity\"      \"man\"        \"rights\"     \"power\"     \n [6] \"solemnly\"   \"charter\"    \"person\"     \"december\"   \"attachment\"\nfw_scores_africa_europe$feature[order(fw_scores_africa_europe$score, decreasing = F)][1:10]\n [1] \"republic\"           \"citizens\"           \"responsibility\"    \n [4] \"state\"              \"self-determination\" \"historical\"        \n [7] \"statehood\"          \"centuries\"          \"basic\"             \n[10] \"members\" \n\n与非洲宪法相关的词汇通常涉及权利、权力与团结，而与欧洲宪法相关的词汇则更多反映出责任、自决以及历史经验。\n\n\n\n创建一个图表，绘制每个特征名称。图表的 x 轴应为每个词项的对数频率（log frequency），y 轴应为该特征的 Fightin’ Words 分数。使用 cex 和 alpha 函数，根据它们的 Fightin’ Words 分数调整词语的大小和透明度。\n\n\n\nReveal Code\n\nfw_scores_africa_europe %&gt;%\n  ggplot(aes(x = log(n),\n             y = score,\n             label = feature,\n             cex = abs(score),\n             alpha = abs(score))) + \n  geom_text() + \n  scale_alpha_continuous(guide = \"none\") + \n  scale_size_continuous(guide = \"none\") + \n  theme_bw() + \n  xlab(\"log(n)\") + \n  ylab(\"Fightin' Words Score\")"
  },
  {
    "objectID": "Seminar/Seminar2.html#分析各国宪法序言",
    "href": "Seminar/Seminar2.html#分析各国宪法序言",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "美国宪法是否影响了其他国家的宪法？目前越来越多的学术观点认为，美国宪法的影响力已随着时间推移而减弱，因为它日益偏离了全球范围内关于人权对宪法秩序重要性的共识。然而，关于美国宪法在多大程度上影响了世界各国成文宪法的修订和采纳，目前仍缺乏实证和系统的认知。\nDavid S. Law 和 Mila Versteeg 在2012年的一项研究中，通过实证调查了美国宪法的影响力，结果表明，近几十年来，其他国家在其本国宪法中效仿美国宪法中与权利相关的条款的可能性越来越小。在本系列问题中，我们将运用本周所学的一些方法来复现其部分分析。"
  },
  {
    "objectID": "Seminar/Seminar2.html#packages",
    "href": "Seminar/Seminar2.html#packages",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "在开始研讨会时，先下载/加载以下 R 包：\nlibrary(tidyverse)\nlibrary(quanteda)\n# 如果无法加载quanteda.textplots和quanteda.textstats包，请运行以下代码：\n# devtools::install_github(\"quanteda/quanteda.textplots\")\n# devtools::install_github(\"quanteda/quanteda.textstats\")\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)"
  },
  {
    "objectID": "Seminar/Seminar2.html#数据",
    "href": "Seminar/Seminar2.html#数据",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "我们将使用如下数据集：宪法序言 — constitutions.csv\n该文件包含155部宪法（英文译本）的序言。数据包含以下变量：\nconstituiton 数据集中的变量说明\n\n\n\n变量\n说明\n\n\n\n\ncountry\n国家名称\n\n\ncontinent\n所在洲（大洲）\n\n\nyear\n宪法制定的年份\n\n\npreamble\n宪法序言的文本内容\n\n\n\n一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：\nconstitutions &lt;- read_csv(\"constitutions.csv\") \n你可以使用 tidyverse 包中的 glimpse() 函数快速查看数据中的变量：\nglimpse(constitutions)\nRows: 155\nColumns: 4\n$ country   &lt;chr&gt; \"afghanistan\", \"albania\", \"algeria\", \"andorra\", \"angola\", \"a…\n$ continent &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"Europe\", \"Africa\", \"Americas\", …\n$ year      &lt;dbl&gt; 2004, 1998, 1989, 1993, 2010, 1981, 1853, 1995, 1995, 1973, …\n$ preamble  &lt;chr&gt; \"In the name of Allah, the Most Beneficent, the Most Mercifu…"
  },
  {
    "objectID": "Seminar/Seminar2.html#tf-idf",
    "href": "Seminar/Seminar2.html#tf-idf",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "探索 constitutions 对象，以了解我们正在处理的数据。preambles 变量中存储的文本平均长度是多少？哪个国家的序言文本最长？哪个国家的最短？这些序言的平均长度是否随着时间发生了变化？\n\n\n\nReveal Code\n\n# 序言文本平均长度：\nconstitutions$preamble_length &lt;- ntoken(tokens(constitutions$preamble))\nmean(constitutions$preamble_length)\n[1] 324.3097                               \n# 文本最长的国家\nconstitutions$country[which.max(constitutions$preamble_length)]\n[1] \"iran_islamic_republic_of\"                             \n# 文本最短的国家\nconstitutions$country[which.min(constitutions$preamble_length)]\n[1] \"greece\"                            \nconstitutions %&gt;%\n  ggplot(aes(x = year,\n             y = preamble_length)) +\n  geom_point() + \n  xlab(\"Year\") + \n  ylab(\"Preamble Length\") + \n  theme_bw() \n\n\n\n将 constitutions 数据框转换为 corpus() 对象，然后进一步转换为 dfm() 对象（注意：你还需要使用 tokens() 函数）。同时，进行一些合理的特征选择决策。\n\n\n\nReveal Code\n\nconstitutions_dfm &lt;- constitutions %&gt;% \n                        corpus(text_field = \"preamble\") %&gt;%\n                        tokens(remove_punct = T) %&gt;%\n                        dfm() %&gt;%\n                        dfm_remove(stopwords(\"en\"))\n\n\n使用 topfeatures() 函数找出美国宪法中最常出现的 10 个特征词。将这些特征与你选择的其他三个国家的宪法中的高频特征进行比较。你注意到了什么？\n\n\n\nReveal Code\n\ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"united_states_of_america\",])\n      united    establish       states       people        order         form \n           2            2            2            1            1            1 \n     justice      defense constitution      liberty \n           1            1            1            1  \ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"argentina\",])\nargentine   justice    nation   general     peace    people       god  national \n        3         2         2         2         1         1         1         1 \nestablish   defense \n        1         1   \ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"cuba\",])\n          man       peoples       dignity    revolution          full \n            4             3             3             3             3 \nrevolutionary       victory          mart          last         human \n            3             3             3             2             2    \ntopfeatures(constitutions_dfm[docvars(constitutions_dfm)$country == \"sudan\",])\n       peace    committed    agreement        sudan constitution    religious \n           4            3            3            3            2            2 \n       shall   governance   agreements          end \n           2            2            2            2   \n\n\n使用 dfm_tfidf() 函数将 tf-idf 权重应用于你的 dfm。然后使用新的矩阵重复上面的练习。你注意到了什么？\n\n\n\nReveal Code\n\nconstitutions_dfm_tf_idf &lt;- constitutions_dfm %&gt;% dfm_tfidf()\n\ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",])\n     insure     america     perfect      states    domestic tranquility \n   2.190332    1.889302    1.889302    1.870118    1.588272    1.588272 \n     ordain   establish     defense   blessings \n   1.412180    1.292527    1.236089    1.190332  \ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"argentina\",])\n   argentine pre-existing constituting        dwell     congress    provinces \n    6.570995     2.190332     2.190332     2.190332     1.889302     1.889302 \n     compose       object     securing      general \n    1.889302     1.889302     1.889302     1.870118  \ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",])\n         mart         cuban           jos        cubans       victory \n     6.570995      4.380663      4.380663      4.380663      4.035701 \nrevolutionary      peasants       workers    revolution          last \n     3.446817      3.426421      3.176543      3.132611      2.824361 \ntopfeatures(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"sudan\",])\n        sudan     agreement    agreements          2005      conflict \n     5.667905      4.236541      3.426421      3.426421      3.426421 \ncomprehensive           end     committed      bestowed  definitively \n     2.982723      2.380663      2.326075      2.190332      2.190332 \n\n\n使用 textplot_wordcloud() 函数为美国和另一个国家各制作一个词云 （你会惊叹于这些图有多么丑）。请注意：由于美国宪法的文本非常短，你可能需要将 min_count 参数设置为比默认值 3 更低的数值。\n\n\n\nReveal Code\n\ntextplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",], min_count = 0)\n\ntextplot_wordcloud(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",], min_count = 2)"
  },
  {
    "objectID": "Seminar/Seminar2.html#余弦相似度cosine-similarity",
    "href": "Seminar/Seminar2.html#余弦相似度cosine-similarity",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "余弦相似度（cos(θ)）用于衡量两个向量 a 和 b 之间的相似性，其定义如下：\n\\[\n\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n\\]\n其中，θ 是向量 a 与 b 之间的夹角，\\(\\|\\mathbf{a}\\|\\) 和 \\(\\|\\mathbf{b}\\|\\) 分别表示向量 a 和 b 的模（magnitude）。\n更直观的展开形式如下：\n\\[\n\\cos(\\theta) = \\frac{a_1b_1 + a_2b_2 + \\cdots + a_Jb_J}\n{\\sqrt{a_1^2 + a_2^2 + \\cdots + a_J^2} \\times \\sqrt{b_1^2 + b_2^2 + \\cdots + b_J^2}}\n\\]\n\n编写一个用于计算两个向量之间余弦相似度的函数。我们可以使用 function() 函数来定义新函数。例如，如果我们想定义一个用于计算平均值的新函数，可以写成：mean_func &lt;- function(x) sum(x)/length(x), 其中 x 是一个向量。\n\n\n\nReveal Code\n\ncosine_sim &lt;- function(a, b){\n  \n  # 计算两个向量的内积\n  numerator &lt;- sum(a * b)\n  \n  # 计算第一个向量的模\n  magnitude_a &lt;- sqrt(sum(a^2))\n  \n  # 计算第二个向量的模\n  magnitude_b &lt;- sqrt(sum(b^2))\n  \n  # 分母的计算\n  denominator &lt;- magnitude_a * magnitude_b\n\n  # 余弦相似度\n  cos_sim &lt;- numerator/denominator\n  \n  return(cos_sim)\n  \n}\n\n\n使用你在上面创建的函数，计算美国宪法序言与另一个你选择的国家的宪法序言之间的余弦相似度。请使用 tf-idf 加权版本的 dfm 来完成这个任务。为确保你的函数工作正常，可将其结果与 textstat_simil() 函数计算出的余弦相似度进行比较。使用 textstat_simil() 时，需要传入 x 向量和 y 向量，并指定 method = \"cosine\"，以计算余弦相似度（而不是其他相似度指标）。\n\n\n\nReveal Code\n\ncosine_sim(constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",],\n           constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",])\n[1] 0.02427221\ntextstat_simil(x = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"cuba\",], \n                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",],\n                             method = \"cosine\")\ntextstat_simil object; method = \"cosine\"\n       text149\ntext36  0.0243\n\n棒极了！看来这个功能成功了。\n\n\n\n使用 textstat_simil() 函数计算美国宪法序言与数据中所有其他序言之间的余弦相似度。(你也可以为该函数提供一个 x 矩阵和一个 y 向量，这样就能计算 x 中所有行与 y 的相似度。)使用 as.numeric() 函数将该函数的输出赋值回原始的 constitutions 数据框。哪三个宪法与美国最相似？哪三个最不相似？(使用 order() 函数来完成这项操作)\n\n\n\nReveal Code\n\n# 计算余弦相似度\ncosine_sim &lt;- textstat_simil(x = constitutions_dfm_tf_idf, \n                             y = constitutions_dfm_tf_idf[docvars(constitutions_dfm_tf_idf)$country == \"united_states_of_america\",],\n                             method = \"cosine\")\n\n\n# 将变量赋值给 data.frame\nconstitutions$cosine_sim &lt;- as.numeric(cosine_sim)\n\n# 找出最相似和最不相似的宪法序言\nconstitutions$country[order(constitutions$cosine_sim, decreasing = T)][1:3]\n[1] \"united_states_of_america\" \"argentina\"               \n[3] \"philippines\" \nconstitutions$country[order(constitutions$cosine_sim, decreasing = F)][1:3]\n[1] \"greece\"    \"lithuania\" \"slovenia\" \n\n\n计算从 1950 年代起撰写的所有宪法中，美国宪法与其他国家宪法之间的平均余弦相似度在各个年代的值。\n\n要完成这个问题，你需要处理几个编码细节: - 首先，你需要将 year 变量转换为 decade 变量。你可以使用 %% “模”运算符来实现，它用于计算两个数值变量相除后的余数。例如，1986 %% 10 将返回值 6。如果你从原始年份中减去这个值，就会得到正确的年代（即 1986 - 6 = 1980）。 - 其次，你需要计算上一个问题中创建的余弦相似度变量的年代平均值。为此，你应使用 group_by() 和 summarise() 函数。group_by() 允许你指定进行汇总时所依据的变量，而 summarise() 函数允许你指定要使用哪种类型的汇总（即这里你应使用 mean() 函数）。\n\n\nReveal Code\n\n# 创建 decade 变量\nconstitutions$decade &lt;- constitutions$year - (constitutions$year %% 10)\n\n# 按 decade 计算平均相似度\ncosine_sim_by_year &lt;- constitutions %&gt;%\n  filter(year &gt;= 1950) %&gt;%\n  group_by(decade) %&gt;%\n  summarise(cosine_sim = mean(cosine_sim)) \n\n\n创建一张折线图（使用 ggplot 中的 geom_line()），将你上面计算的平均值放在 y 轴上，将年代（decades）放在 x 轴上。在最近几十年中，各国宪法的序言是否变得与美国宪法序言越来越不相似了？\n\n\n\nReveal Code\n\ncosine_sim_by_year %&gt;%\n  ggplot(aes(x = decade, y = cosine_sim)) + \n  geom_line() + \n  xlab(\"Decade\") +\n  ylab(\"Tf-idf cosine similarity with US constitution\") + \n  theme_bw()"
  },
  {
    "objectID": "Seminar/Seminar2.html#fightin-words",
    "href": "Seminar/Seminar2.html#fightin-words",
    "title": "研讨会2: 相似性、差异性与复杂性",
    "section": "",
    "text": "除了计算宪法之间的相似度之外，我们或许还希望描述世界不同地区宪法之间的语言差异。为了刻画这些差异，我们将使用 Fightin’ Words 方法，该方法由 Munroe 等人（2008） 提出。\n回顾Lecture内容，该方法首先计算某个类别中某个词出现的概率（此处以部门为例）：\n\\[\n\\hat{\\mu}_{j,k} = \\frac{W^*_{j,k} + \\alpha_0}{n_k + \\sum_{j=1}^{J}1 + \\alpha_0}\n\\]\n其中：\n\n\\(W^*_{j,k}\\) 表示特征 \\(j\\) 在类别 \\(k\\) 的文档中出现的次数\n\n\\(n_k\\) 表示类别 \\(k\\) 的文档中的总词数\n\n\\(\\alpha_0\\) 是平滑参数（“smoothing”），用于将常见词的差异收缩至 0\n\n接着，我们计算类别 \\(k\\) 和 \\(k'\\) 的对数比率（log-odds）：\n\\[\n\\text{log-odds-ratio}_{j,k} = \\log \\left( \\frac{\\hat{\\mu}_{j,k}}{1 - \\hat{\\mu}_{j,k}} \\right) - \\log \\left( \\frac{\\hat{\\mu}_{j,k'}}{1 - \\hat{\\mu}_{j,k'}} \\right)\n\\]\n最后，我们用其 方差（variance） 进行标准化处理（再次降低稀有词的影响）：\n\\[\n\\text{Fightin' Words Score}_j =\n\\frac{\\text{log-odds-ratio}_{j,k}}{\\sqrt{\\text{Var}(\\text{log-odds-ratio}_{j,k})}}\n\\]\n我在下面提供了一个函数，它可以根据数据中任意协变量所定义的一对分组，计算出 Fightin’ Words 分数：\nfightin_words &lt;- function(dfm_input, covariate, group_1 = \"Political Science\", group_2 = \"Economics\", alpha_0 = 1){\n  \n  # 子集化 DFM：仅保留属于 group_1 和 group_2 的文档\n  fw_dfm &lt;- dfm_subset(dfm_input, get(covariate) %in% c(group_1, group_2)) \n  fw_dfm &lt;- dfm_group(fw_dfm, get(covariate))\n  fw_dfm &lt;- fw_dfm[,colSums(fw_dfm)!=0]\n  dfm_input_trimmed &lt;- dfm_match(dfm_input, featnames(fw_dfm))\n  \n  # 计算每个词的先验频率 alpha_w\n  alpha_w &lt;- (colSums(dfm_input_trimmed))*(alpha_0/sum(dfm_input_trimmed))\n   \n  for(i in 1:nrow(fw_dfm)) fw_dfm[i,] &lt;- fw_dfm[i,] + alpha_w\n  fw_dfm &lt;- as.dfm(fw_dfm)\n  mu &lt;- fw_dfm %&gt;% dfm_weight(\"prop\")\n\n  # 计算每个词的 log-odds 比值\n  lo_g1 &lt;- log(as.numeric(mu[group_1,])/(1-as.numeric(mu[group_1,])))\n  lo_g2 &lt;- log(as.numeric(mu[group_2,])/(1-as.numeric(mu[group_2,])))\n  fw &lt;- lo_g1 - lo_g2\n  \n  # 计算每个词的方差\n  fw_var &lt;- as.numeric(1/(fw_dfm[1,])) + as.numeric(1/(fw_dfm[2,]))\n  \n  # 计算标准化得分（z 分数），以及每个词的频次和名称\n  fw_scores &lt;- data.frame(score = fw/sqrt(fw_var),\n                          n = colSums(fw_dfm),\n                          feature = featnames(fw_dfm))\n\n  return(fw_scores)\n  \n}\n\n\n\n\n\n\n\n\n\n参数名称\n说明\n\n\n\n\ndfm_input\n一个 dfm（文档-词项矩阵），用于测量每篇文档中词语的频率。\n\n\ncovariate\n协变量的名称（应以字符串形式输入，如 \"covariate_name\"），其中包含分组标签（例如 “continent”）。\n\n\ngroup_1\n第一个分组的名称。\n\n\ngroup_2\n第二个分组的名称。\n\n\nalpha_0\n正则化参数，必须是正数。\n\n\n\n该函数返回一个 data.frame，其中的每一行表示在文本中出现的一个特征（词项），与两个分组相关，包含以下三个变量：\n\n\n\n\n\n\n\n变量名称\n说明\n\n\n\n\nfeature\n词项名称。\n\n\nfw\n每个词项对应的 Fightin’ Words 分数。\n\n\nn\n每个词项在两个分组的所有文档中出现的总次数。\n\n\n\n\n使用上面介绍的的函数，计算区分非洲与欧洲国家宪法的 Fightin’ Words 分数，并回答以下问题：(a)哪 10 个词最强烈地与非洲国家的宪法相关？(b)哪 10 个词最强烈地与欧洲国家的宪法相关？\n\n\n\nReveal Code\n\nfw_scores_africa_europe &lt;- fightin_words(dfm_input = constitutions_dfm, \n                                         covariate = \"continent\", \n                                         group_1 = \"Africa\", \n                                         group_2 = \"Europe\", \n                                         alpha_0 = 1)\n\nfw_scores_africa_europe$feature[order(fw_scores_africa_europe$score, decreasing = T)][1:10]\n [1] \"political\"  \"unity\"      \"man\"        \"rights\"     \"power\"     \n [6] \"solemnly\"   \"charter\"    \"person\"     \"december\"   \"attachment\"\nfw_scores_africa_europe$feature[order(fw_scores_africa_europe$score, decreasing = F)][1:10]\n [1] \"republic\"           \"citizens\"           \"responsibility\"    \n [4] \"state\"              \"self-determination\" \"historical\"        \n [7] \"statehood\"          \"centuries\"          \"basic\"             \n[10] \"members\" \n\n与非洲宪法相关的词汇通常涉及权利、权力与团结，而与欧洲宪法相关的词汇则更多反映出责任、自决以及历史经验。\n\n\n\n创建一个图表，绘制每个特征名称。图表的 x 轴应为每个词项的对数频率（log frequency），y 轴应为该特征的 Fightin’ Words 分数。使用 cex 和 alpha 函数，根据它们的 Fightin’ Words 分数调整词语的大小和透明度。\n\n\n\nReveal Code\n\nfw_scores_africa_europe %&gt;%\n  ggplot(aes(x = log(n),\n             y = score,\n             label = feature,\n             cex = abs(score),\n             alpha = abs(score))) + \n  geom_text() + \n  scale_alpha_continuous(guide = \"none\") + \n  scale_size_continuous(guide = \"none\") + \n  theme_bw() + \n  xlab(\"log(n)\") + \n  ylab(\"Fightin' Words Score\")"
  },
  {
    "objectID": "Seminar/Seminar5.html",
    "href": "Seminar/Seminar5.html",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n\n Seminar data \n\n\n在前几周中，当我们将词语表示为数据时，我们只是简单地计算它们在文档之间和文档内部出现的频率。我们大体上将词语视为独立的单位：作为唯一标识某一特定含义的文本字符串，而我们并没有一个自然的相似性概念来将相似的词汇进行归类。\n相比之下，词嵌入（word embeddings） 方法将语料库中的每个唯一词汇表示为一个密集的实数向量。正如我们在讲座中所讨论的，这些向量实际编码了大量关于词语用法的信息，这些信息可以在社会科学中的各种问题上加以有效利用。\n在今天的研讨课中，我们将熟悉 GloVe 项目中一些预训练的词向量。我们将使用这些向量来探索词语之间的相似性，执行基于类比的任务，并补充我们在讲座二中学习过的基于词典的测量方法。\n\n\n\n在开始研讨会时，先下载/加载以下 R 包：\nlibrary(tidyverse)\nlibrary(quanteda)\n# install.packages(\"text2vec\")\nlibrary(text2vec)\n\n\n\n今天我们将使用预训练的 GloVe 词嵌入，这些文件可以从研讨课页面顶部的链接下载。请注意，包含词嵌入的文件非常大！因此，建议你可以提前一天先下载好。\n尽管文件体积很大，我们实际上使用的是 GloVe 词嵌入中较小的一个版本，它是在维基百科和新闻数据的组合上训练而成的。这些词嵌入的维度为 300，涵盖约 40 万个词汇。请注意，你也可以从 GloVe 项目官网中下载更大版本的词嵌入来复现本次练习中的所有任务，但对于我们这里的应用而言，使用更大的模型所带来的差异可能是微乎其微的。\n\n\n\n\n使用 load() 函数将 GloVe 词嵌入加载到 R 中。\n\n\n\nReveal Code\n\nload(\"glove_embeddings.Rdata\")\n\n\n看看 Glove 嵌入对象的大小。这个对象有多少行？有多少列？分别表示了什么？\n\n\n\nReveal Code\n\ndim(glove)\n[1] 400000    300   \n\n行代表单词。列代表嵌入的维度。\n\n\n\n写一个函数，用于计算给定词汇与 GloVe 词嵌入对象中所有其他词之间的余弦相似度。你可以使用 text2vec 包中的 sim2() 函数来实现此操作。你的函数应包含两个输入参数：\n\n\ntarget_word：你希望计算相似度的目标词\nn：你希望返回的最近邻词的数量（按相似度排序） 该函数将帮助你快速发现与任一词语最相近的词汇，有助于理解词语在向量空间中的语义邻近性。\n\n\n\nReveal Code\n\nsimilarities &lt;- function(target_word, n){\n\n  # 提取目标词的词向量\n  target_vector &lt;- glove[which(rownames(glove) %in% target_word),]  \n  \n  # 计算目标词与其他词之间的余弦相似度\n  target_sim &lt;- sim2(glove, matrix(target_vector, nrow = 1))\n  \n  # 返回与目标词最相近的前 n 个词\n  names(sort(target_sim[,1], decreasing = T))[1:n]\n\n}\n\n\n根据你写好的这个函数，找出与下列三个词最相似的前七个词: “quantitative”; “text”; “analysis”(你也可以尝试一些别的词)\n\n\n\nReveal Code\n\nsimilarities(\"quantitative\", n = 7)\n[1] \"quantitative\" \"qualitative\"  \"empirical\"    \"measurement\"  \"analysis\"    \n[6] \"methodology\"  \"analytical\"     \nsimilarities(\"text\", n = 7)\n[1] \"text\"     \"texts\"    \"document\" \"read\"     \"messages\" \"written\"  \"copy\"       \nsimilarities(\"analysis\", n = 7)\n[1] \"analysis\"    \"analyses\"    \"study\"       \"data\"        \"studies\"    \n[6] \"analyzed\"    \"methodology\"      \n\n\n\n\n\n编写一个函数，用于计算形式为 “a is to b as c is to ___” 的类比关系。例如，如果 b 是 “king”，a 是 “man”，c 是 “woman”，那么缺失的词应该是 “queen”。 你的函数需要接受四个参数。前三个参数应对应于类比中包含的词。第四个参数应是一个指定返回最近邻词数量的参数。\n\n\n\nReveal Code\n\nanalogies &lt;- function(a, b, c, n){\n  \n  # 提取类比任务中三个词的词向量\n  a_vec &lt;- glove[which(rownames(glove) == a),]\n  b_vec &lt;- glove[which(rownames(glove) == b),]\n  c_vec &lt;- glove[which(rownames(glove) == c),]\n  \n  # 生成类比向量（向量(c) - 向量(a) + 向量(b)）\n  target &lt;- c_vec - a_vec + b_vec\n  \n  # 计算类比向量与所有其他向量之间的余弦相似度\n  target_sim &lt;- sim2(glove, matrix(target, nrow = 1))\n  \n  # 返回与类比向量最相似的前 n 个词\n  names(sort(target_sim[,1], decreasing = T))[1:n]\n\n}     \n\n\n使用你上面创建的函数来求解以下类比填空任务的词嵌入答案。\n\n\nEinstein is to scientist as Picasso is to ___?\nArsenal is to football as Yankees is to ___?\nActor is to theatre as doctor is to ___?\n\n\n\nReveal Code\n\nanalogies(\"einstein\", \"scientist\", \"picasso\", 6)\n[1] \"picasso\"   \"painter\"   \"painting\"  \"artist\"    \"paintings\" \"scientist\"  \nanalogies(\"arsenal\", \"football\", \"yankees\", 6)\n[1] \"baseball\"   \"yankees\"    \"sox\"        \"football\"   \"basketball\"\n[6] \"braves\"   \nanalogies(\"actor\", \"theatre\", \"doctor\", 6)\n[1] \"theatre\"  \"doctor\"   \"hospital\" \"medical\"  \"theater\"  \"doctors\"   \n\n\n尝试一些其他词的类比！\n\n\n\n\n在研讨会2中，我们使用了道德基础词典（Moral Foundations Dictionary, MFD）来评估 Reddit 帖子中的道德内容。在本次研讨会中，我们将使用 GloVe 词嵌入来扩展 MFD 中的 “care” 类别。\n为完成这一部分任务，你需要再次访问我们在研讨课 2 中使用的文件： - mft_dictionary.csv - mft_texts.csv 现在请找到这些文件（如果需要，可以重新下载）一旦找到了这些文件，请将它们加载到 R 中。\nmft_dictionary_words &lt;- read_csv(\"mft_dictionary.csv\")\nmft_texts &lt;- read_csv(\"mft_texts.csv\")\n\n创建一个包含 MFT 词典中 “Care” 类别词汇的向量。\n\n\n\nReveal Code\n\ncare_words &lt;- mft_dictionary_words$word[mft_dictionary_words$foundation == \"care\"] \n\n\n从 glove 中提取与 “care” 类别词汇相关的词嵌入向量。\n\n\n\nReveal Code\n\ncare_embeddings &lt;- glove[rownames(glove) %in% care_words,]\n\n\n计算 “care” 词汇的平均嵌入向量。请使用 colMeans 函数，它可以计算矩阵每一列的均值。\n\n\n\nReveal Code\n\ncare_embeddings_mean &lt;- colMeans(care_embeddings)\n\n\n计算 care 平均向量与 glove 嵌入对象中所有其他词之间的相似度。为此，再次使用 sim2() 函数。\n\n\n\nReveal Code\n\ntarget_sim &lt;- sim2(x = glove,\n                   y = matrix(care_embeddings_mean, nrow = 1))\n\n\n哪些是与 care 平均向量具有最高余弦相似度的 500 个词？这些词中有多少在原始词典中？\n\n\n\nReveal Code\n\ntop500 &lt;- names(sort(target_sim[,1], decreasing = T))[1:500]\n\ntable(top500%in%care_words)\nFALSE  TRUE \n  386   114        \n\n\n检查你在上一步中计算出的前 500 个词中不在原始 care 词典中的词。这些词是否体现了 “care” 的概念？\n\n\n\nReveal Code\n\ntop500[!top500%in%care_words]\n  [1] \"traumatized\"     \"victimized\"      \"cruelly\"         \"helpless\"       \n  [5] \"innocent\"        \"unborn\"          \"sufferings\"      \"endure\"         \n  [9] \"terrified\"       \"frightened\"      \"sick\"            \"callous\"        \n [13] \"wronged\"         \"civilians\"       \"viciously\"       \"senseless\"      \n [17] \"terrorizing\"     \"frighten\"        \"beatings\"        \"horrific\"       \n [21] \"risking\"         \"inhumane\"        \"unspeakable\"     \"confess\"        \n [25] \"humiliation\"     \"defenseless\"     \"loneliness\"      \"terrorize\"      \n [29] \"grief\"           \"bereaved\"        \"injustice\"       \"subjecting\"     \n [33] \"vengeful\"        \"innocents\"       \"misfortune\"      \"abuse\"          \n [37] \"neglect\"         \"plight\"          \"treating\"        \"traumatised\"    \n [41] \"stigmatized\"     \"oppression\"      \"starving\"        \"humiliate\"      \n [45] \"oftentimes\"      \"grievous\"        \"unjustly\"        \"grieving\"       \n [49] \"humiliated\"      \"betrayed\"        \"spared\"          \"mutilation\"     \n [53] \"lest\"            \"ashamed\"         \"heinous\"         \"enemies\"        \n [57] \"abusive\"         \"barbaric\"        \"elderly\"         \"starve\"         \n [61] \"fear\"            \"neglecting\"      \"terrible\"        \"deprived\"       \n [65] \"dignity\"         \"deprive\"         \"beings\"          \"dying\"          \n [69] \"mercilessly\"     \"hatred\"          \"grievously\"      \"oneself\"        \n [73] \"vicious\"         \"succumb\"         \"feelings\"        \"schoolmates\"    \n [77] \"fearful\"         \"horrible\"        \"horrifying\"      \"sparing\"        \n [81] \"hardships\"       \"afraid\"          \"risked\"          \"indiscriminate\" \n [85] \"unnecessarily\"   \"bystanders\"      \"heartless\"       \"shame\"          \n [89] \"ordeal\"          \"offends\"         \"subjected\"       \"enslavement\"    \n [93] \"handicapped\"     \"disturbed\"       \"humanity\"        \"sacrificing\"    \n [97] \"degrading\"       \"oppressed\"       \"treat\"           \"retribution\"    \n[101] \"maiming\"         \"injure\"          \"terrify\"         \"terminally\"     \n[105] \"unbearable\"      \"betray\"          \"suicidal\"        \"pretending\"     \n[109] \"seriously\"       \"perpetrated\"     \"destitute\"       \"maimed\"         \n[113] \"perpetrator\"     \"insult\"          \"demeaning\"       \"depraved\"       \n[117] \"betraying\"       \"selfishness\"     \"aiding\"          \"traumas\"        \n[121] \"taunt\"           \"brutal\"          \"belittling\"      \"sickening\"      \n[125] \"despair\"         \"needless\"        \"orphans\"         \"repress\"        \n[129] \"terrorized\"      \"hardship\"        \"scared\"          \"infirm\"         \n[133] \"husbands\"        \"savagery\"        \"townspeople\"     \"mentally\"       \n[137] \"punishing\"       \"disciplining\"    \"sins\"            \"savagely\"       \n[141] \"escaping\"        \"teasing\"         \"sickness\"        \"distraught\"     \n[145] \"captors\"         \"pregnant\"        \"oppress\"         \"spouse\"         \n[149] \"indignity\"       \"intolerable\"     \"intimidated\"     \"forgiveness\"    \n[153] \"countrymen\"      \"cursed\"          \"conscience\"      \"knowing\"        \n[157] \"trauma\"          \"deserve\"         \"punishes\"        \"strangling\"     \n[161] \"wickedness\"      \"sinful\"          \"punished\"        \"cruelties\"      \n[165] \"helplessness\"    \"punish\"          \"disrespect\"      \"perceived\"      \n[169] \"appalling\"       \"hysterical\"      \"bystander\"       \"disfigurement\"  \n[173] \"treachery\"       \"homosexuals\"     \"debilitating\"    \"evils\"          \n[177] \"taunts\"          \"vindictive\"      \"betrayal\"        \"tolerate\"       \n[181] \"provokes\"        \"thoughtless\"     \"avenge\"          \"intimidate\"     \n[185] \"feel\"            \"offend\"          \"despicable\"      \"needlessly\"     \n[189] \"malnourished\"    \"sexually\"        \"tolerating\"      \"bodily\"         \n[193] \"terrifying\"      \"passivity\"       \"abduct\"          \"betrays\"        \n[197] \"trusting\"        \"frightening\"     \"expose\"          \"misguided\"      \n[201] \"barbarity\"       \"severely\"        \"perceive\"        \"imprison\"       \n[205] \"horrendous\"      \"depriving\"       \"sadistic\"        \"committing\"     \n[209] \"unworthy\"        \"treated\"         \"unjust\"          \"slaughtering\"   \n[213] \"painful\"         \"debilitated\"     \"scolding\"        \"enslave\"        \n[217] \"aftereffects\"    \"disrespectful\"   \"victimised\"      \"awaken\"         \n[221] \"humankind\"       \"resentful\"       \"suffocate\"       \"hypocritical\"   \n[225] \"cowardly\"        \"fleeing\"         \"judgmental\"      \"ridicule\"       \n[229] \"insecurities\"    \"complicit\"       \"misery\"          \"caretakers\"     \n[233] \"detriment\"       \"physically\"      \"tenderly\"        \"habitually\"     \n[237] \"malnutrition\"    \"punishment\"      \"misbehaving\"     \"villagers\"      \n[241] \"emotionally\"     \"relatives\"       \"selfish\"         \"deserving\"      \n[245] \"destitution\"     \"gravely\"         \"exposing\"        \"companionship\"  \n[249] \"squeamish\"       \"brutally\"        \"guilt\"           \"imprisoning\"    \n[253] \"orphaned\"        \"criminals\"       \"injustices\"      \"adolescents\"    \n[257] \"stigma\"          \"disobedient\"     \"sacrificed\"      \"survivors\"      \n[261] \"remorse\"         \"empathetic\"      \"wanton\"          \"insults\"        \n[265] \"unsuspecting\"    \"enslaving\"       \"aggression\"      \"involuntarily\"  \n[269] \"outweighs\"       \"ignorance\"       \"screams\"         \"addicted\"       \n[273] \"ostracized\"      \"subordinates\"    \"immoral\"         \"caregivers\"     \n[277] \"merciless\"       \"perpetrators\"    \"powerlessness\"   \"mutilating\"     \n[281] \"ills\"            \"depredations\"    \"stab\"            \"affection\"      \n[285] \"ill-treatment\"   \"ill\"             \"blinded\"         \"callously\"      \n[289] \"insulted\"        \"manipulative\"    \"verbally\"        \"unintentionally\"\n[293] \"distract\"        \"illness\"         \"ungrateful\"      \"deprivation\"    \n[297] \"pretend\"         \"undermines\"      \"disfigured\"      \"impotent\"       \n[301] \"cursing\"         \"inconvenience\"   \"motivates\"       \"witnessing\"     \n[305] \"aggressors\"      \"jealousy\"        \"suffocating\"     \"children\"       \n[309] \"sacrifices\"      \"jealous\"         \"hideous\"         \"ostracism\"      \n[313] \"perverted\"       \"horrified\"       \"insensitive\"     \"confronting\"    \n[317] \"illnesses\"       \"befriend\"        \"scourge\"         \"willful\"        \n[321] \"repression\"      \"revenge\"         \"annoy\"           \"scarred\"        \n[325] \"curses\"          \"throats\"         \"jailers\"         \"horribly\"       \n[329] \"repressed\"       \"bigotry\"         \"excruciating\"    \"befriending\"    \n[333] \"coerce\"          \"traumatic\"       \"ugliness\"        \"revulsion\"      \n[337] \"exposes\"         \"patronizing\"     \"stepmother\"      \"newborns\"       \n[341] \"indulging\"       \"promiscuous\"     \"confront\"        \"ministering\"    \n[345] \"unconscionable\"  \"insidious\"       \"tenderness\"      \"indifference\"   \n[349] \"gruesome\"        \"trampling\"       \"embittered\"      \"despise\"        \n[353] \"cowardice\"       \"hopelessness\"    \"underlings\"      \"horrors\"        \n[357] \"scold\"           \"atrocious\"       \"banish\"          \"persecutions\"   \n[361] \"brainwashed\"     \"indignities\"     \"banishing\"       \"pretended\"      \n[365] \"abhorrent\"       \"wrongs\"          \"genitals\"        \"counseled\"      \n[369] \"atrocities\"      \"susceptible\"     \"ferocity\"        \"indifferent\"    \n[373] \"autistic\"        \"deprivations\"    \"demoralizing\"    \"unfaithful\"     \n[377] \"transgressions\"  \"brainwashing\"    \"ghastly\"         \"insufferable\"   \n[381] \"taunting\"        \"pillage\"         \"believing\"       \"unconscious\"    \n[385] \"afflicting\"      \"muggers\"        \n\n\n你对上一个问题的回答对这种词典扩展方法有何启示？\n\n\n\nReveal Code\n\n\n这里的核心思想是，使用词嵌入（word embeddings）使我们能够自动扩展与某一概念相关的词汇集合，而此前我们是通过手动选定的一组词典词语来进行测量的。\n\n\n这意味着我们可以利用词语之间的相似性信息来补充现有的测量方法。至于这是否会在分类任务中带来性能上的提升，将是下面课后思考中要探讨的主题。\n\n\n\n\n\nmft_texts 对象中包含了一系列变量，这些变量记录了每条文本由人工标注归属到的道德类别。在今天的课后思考中，你将再次使用基于词典的方法对文本进行打分，并将这些词典分数与人工编码进行比较。如果你忘记了如何应用词典，可以回顾研讨会2的材料。\n\n请创建一个新的词典对象，其中应包含两个类别：\n\n\ncare_original_words：只包含原始 care 词典中的词语；\ncare_embedding_words：包含原始 care 词典词语以及你在上一部分中提取出的前 500 个最相似词语。\n\n\n\nReveal Code\n\n# 创建 dfm\nmft_dfm &lt;- mft_texts %&gt;% \n  corpus(text_field = \"text\") %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 5)\n\n# 创建 care 词典\ncare_dictionary &lt;- dictionary(list(care_original = care_words,\n                                   care_embedding = c(top500[!top500%in%care_words], care_words)))\n\n\n使用你刚刚构建的词典对 mft_texts 对象中的文本进行打分。在该对象中创建变量，指示每条文本是否被相应的词典分类为 care 文本（即如果文本中包含该类别词典中的任意词语，则将其分类为 care 文本）。\n\n\n\nReveal Code\n\n# 使用词典对文本进行打分\ncare_dfm_dictionary &lt;- dfm_lookup(mft_dfm, care_dictionary)\nmft_texts$care_original &lt;- as.numeric(care_dfm_dictionary[,1]) &gt; 0\nmft_texts$care_embedding &lt;- as.numeric(care_dfm_dictionary[,2]) &gt; 0\n\n\n创建一个混淆矩阵，用于将人工标注与词典打分结果进行比较。你需要分别比较以下两个变量与人工标注的匹配情况：\n\n\nmft_texts$care_original：使用原始 care 词典的分类结果\nmft_texts$care_embedding：使用词嵌入扩展后的 care 词典分类结果 然后，通过混淆矩阵评估：哪种方法表现最好 —— 原始词典，还是词嵌入扩展的方法？\n\n\n\nReveal Code\n\n# 计算混淆矩阵：原始 care 词典\ncare_original_confusion &lt;- table( \n  dictionary = mft_texts$care_original &gt; 0,\n  human_coding = mft_texts$care)\n\n# 计算混淆矩阵：嵌入扩展后的 care 词典\ncare_embedding_confusion &lt;- table( \n  dictionary = mft_texts$care_embedding &gt; 0,\n  human_coding = mft_texts$care)\n\n# 输出原始词典的性能统计\nlibrary(caret)\nconfusionMatrix(care_original_confusion, positive = \"TRUE\")\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 11303  2621\n     TRUE   1843  2119\n                                         \n               Accuracy : 0.7504         \n                 95% CI : (0.744, 0.7567)\n    No Information Rate : 0.735          \n    P-Value [Acc &gt; NIR] : 1.326e-06      \n                                         \n                  Kappa : 0.3238         \n                                         \n Mcnemar's Test P-Value : &lt; 2.2e-16      \n                                         \n            Sensitivity : 0.4470         \n            Specificity : 0.8598         \n         Pos Pred Value : 0.5348         \n         Neg Pred Value : 0.8118         \n             Prevalence : 0.2650         \n         Detection Rate : 0.1185         \n   Detection Prevalence : 0.2215         \n      Balanced Accuracy : 0.6534         \n                                         \n       'Positive' Class : TRUE        \nconfusionMatrix(care_embedding_confusion, positive = \"TRUE\")\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 10114  1763\n     TRUE   3032  2977\n                                          \n               Accuracy : 0.7319          \n                 95% CI : (0.7254, 0.7384)\n    No Information Rate : 0.735           \n    P-Value [Acc &gt; NIR] : 0.8265          \n                                          \n                  Kappa : 0.3661          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6281          \n            Specificity : 0.7694          \n         Pos Pred Value : 0.4954          \n         Neg Pred Value : 0.8516          \n             Prevalence : 0.2650          \n         Detection Rate : 0.1664          \n   Detection Prevalence : 0.3360          \n      Balanced Accuracy : 0.6987          \n                                          \n       'Positive' Class : TRUE    \n\n在本例中，当我们引入基于词嵌入相似度扩展出的词语后，预测的准确率略有下降，但分类的敏感性显著提高。这表明，新增的词语在很大程度上提升了我们识别表达“care”概念文本的能力，尽管这也带来了更多的假阳性结果（即误将非 care 文本分类为 care）。"
  },
  {
    "objectID": "Seminar/Seminar5.html#相似性类比与词典扩展",
    "href": "Seminar/Seminar5.html#相似性类比与词典扩展",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "在前几周中，当我们将词语表示为数据时，我们只是简单地计算它们在文档之间和文档内部出现的频率。我们大体上将词语视为独立的单位：作为唯一标识某一特定含义的文本字符串，而我们并没有一个自然的相似性概念来将相似的词汇进行归类。\n相比之下，词嵌入（word embeddings） 方法将语料库中的每个唯一词汇表示为一个密集的实数向量。正如我们在讲座中所讨论的，这些向量实际编码了大量关于词语用法的信息，这些信息可以在社会科学中的各种问题上加以有效利用。\n在今天的研讨课中，我们将熟悉 GloVe 项目中一些预训练的词向量。我们将使用这些向量来探索词语之间的相似性，执行基于类比的任务，并补充我们在讲座二中学习过的基于词典的测量方法。"
  },
  {
    "objectID": "Seminar/Seminar5.html#packages",
    "href": "Seminar/Seminar5.html#packages",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "在开始研讨会时，先下载/加载以下 R 包：\nlibrary(tidyverse)\nlibrary(quanteda)\n# install.packages(\"text2vec\")\nlibrary(text2vec)"
  },
  {
    "objectID": "Seminar/Seminar5.html#数据集",
    "href": "Seminar/Seminar5.html#数据集",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "今天我们将使用预训练的 GloVe 词嵌入，这些文件可以从研讨课页面顶部的链接下载。请注意，包含词嵌入的文件非常大！因此，建议你可以提前一天先下载好。\n尽管文件体积很大，我们实际上使用的是 GloVe 词嵌入中较小的一个版本，它是在维基百科和新闻数据的组合上训练而成的。这些词嵌入的维度为 300，涵盖约 40 万个词汇。请注意，你也可以从 GloVe 项目官网中下载更大版本的词嵌入来复现本次练习中的所有任务，但对于我们这里的应用而言，使用更大的模型所带来的差异可能是微乎其微的。"
  },
  {
    "objectID": "Seminar/Seminar5.html#词语相似性",
    "href": "Seminar/Seminar5.html#词语相似性",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "使用 load() 函数将 GloVe 词嵌入加载到 R 中。\n\n\n\nReveal Code\n\nload(\"glove_embeddings.Rdata\")\n\n\n看看 Glove 嵌入对象的大小。这个对象有多少行？有多少列？分别表示了什么？\n\n\n\nReveal Code\n\ndim(glove)\n[1] 400000    300   \n\n行代表单词。列代表嵌入的维度。\n\n\n\n写一个函数，用于计算给定词汇与 GloVe 词嵌入对象中所有其他词之间的余弦相似度。你可以使用 text2vec 包中的 sim2() 函数来实现此操作。你的函数应包含两个输入参数：\n\n\ntarget_word：你希望计算相似度的目标词\nn：你希望返回的最近邻词的数量（按相似度排序） 该函数将帮助你快速发现与任一词语最相近的词汇，有助于理解词语在向量空间中的语义邻近性。\n\n\n\nReveal Code\n\nsimilarities &lt;- function(target_word, n){\n\n  # 提取目标词的词向量\n  target_vector &lt;- glove[which(rownames(glove) %in% target_word),]  \n  \n  # 计算目标词与其他词之间的余弦相似度\n  target_sim &lt;- sim2(glove, matrix(target_vector, nrow = 1))\n  \n  # 返回与目标词最相近的前 n 个词\n  names(sort(target_sim[,1], decreasing = T))[1:n]\n\n}\n\n\n根据你写好的这个函数，找出与下列三个词最相似的前七个词: “quantitative”; “text”; “analysis”(你也可以尝试一些别的词)\n\n\n\nReveal Code\n\nsimilarities(\"quantitative\", n = 7)\n[1] \"quantitative\" \"qualitative\"  \"empirical\"    \"measurement\"  \"analysis\"    \n[6] \"methodology\"  \"analytical\"     \nsimilarities(\"text\", n = 7)\n[1] \"text\"     \"texts\"    \"document\" \"read\"     \"messages\" \"written\"  \"copy\"       \nsimilarities(\"analysis\", n = 7)\n[1] \"analysis\"    \"analyses\"    \"study\"       \"data\"        \"studies\"    \n[6] \"analyzed\"    \"methodology\""
  },
  {
    "objectID": "Seminar/Seminar5.html#词类比",
    "href": "Seminar/Seminar5.html#词类比",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "编写一个函数，用于计算形式为 “a is to b as c is to ___” 的类比关系。例如，如果 b 是 “king”，a 是 “man”，c 是 “woman”，那么缺失的词应该是 “queen”。 你的函数需要接受四个参数。前三个参数应对应于类比中包含的词。第四个参数应是一个指定返回最近邻词数量的参数。\n\n\n\nReveal Code\n\nanalogies &lt;- function(a, b, c, n){\n  \n  # 提取类比任务中三个词的词向量\n  a_vec &lt;- glove[which(rownames(glove) == a),]\n  b_vec &lt;- glove[which(rownames(glove) == b),]\n  c_vec &lt;- glove[which(rownames(glove) == c),]\n  \n  # 生成类比向量（向量(c) - 向量(a) + 向量(b)）\n  target &lt;- c_vec - a_vec + b_vec\n  \n  # 计算类比向量与所有其他向量之间的余弦相似度\n  target_sim &lt;- sim2(glove, matrix(target, nrow = 1))\n  \n  # 返回与类比向量最相似的前 n 个词\n  names(sort(target_sim[,1], decreasing = T))[1:n]\n\n}     \n\n\n使用你上面创建的函数来求解以下类比填空任务的词嵌入答案。\n\n\nEinstein is to scientist as Picasso is to ___?\nArsenal is to football as Yankees is to ___?\nActor is to theatre as doctor is to ___?\n\n\n\nReveal Code\n\nanalogies(\"einstein\", \"scientist\", \"picasso\", 6)\n[1] \"picasso\"   \"painter\"   \"painting\"  \"artist\"    \"paintings\" \"scientist\"  \nanalogies(\"arsenal\", \"football\", \"yankees\", 6)\n[1] \"baseball\"   \"yankees\"    \"sox\"        \"football\"   \"basketball\"\n[6] \"braves\"   \nanalogies(\"actor\", \"theatre\", \"doctor\", 6)\n[1] \"theatre\"  \"doctor\"   \"hospital\" \"medical\"  \"theater\"  \"doctors\"   \n\n\n尝试一些其他词的类比！"
  },
  {
    "objectID": "Seminar/Seminar5.html#词典扩展",
    "href": "Seminar/Seminar5.html#词典扩展",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "在研讨会2中，我们使用了道德基础词典（Moral Foundations Dictionary, MFD）来评估 Reddit 帖子中的道德内容。在本次研讨会中，我们将使用 GloVe 词嵌入来扩展 MFD 中的 “care” 类别。\n为完成这一部分任务，你需要再次访问我们在研讨课 2 中使用的文件： - mft_dictionary.csv - mft_texts.csv 现在请找到这些文件（如果需要，可以重新下载）一旦找到了这些文件，请将它们加载到 R 中。\nmft_dictionary_words &lt;- read_csv(\"mft_dictionary.csv\")\nmft_texts &lt;- read_csv(\"mft_texts.csv\")\n\n创建一个包含 MFT 词典中 “Care” 类别词汇的向量。\n\n\n\nReveal Code\n\ncare_words &lt;- mft_dictionary_words$word[mft_dictionary_words$foundation == \"care\"] \n\n\n从 glove 中提取与 “care” 类别词汇相关的词嵌入向量。\n\n\n\nReveal Code\n\ncare_embeddings &lt;- glove[rownames(glove) %in% care_words,]\n\n\n计算 “care” 词汇的平均嵌入向量。请使用 colMeans 函数，它可以计算矩阵每一列的均值。\n\n\n\nReveal Code\n\ncare_embeddings_mean &lt;- colMeans(care_embeddings)\n\n\n计算 care 平均向量与 glove 嵌入对象中所有其他词之间的相似度。为此，再次使用 sim2() 函数。\n\n\n\nReveal Code\n\ntarget_sim &lt;- sim2(x = glove,\n                   y = matrix(care_embeddings_mean, nrow = 1))\n\n\n哪些是与 care 平均向量具有最高余弦相似度的 500 个词？这些词中有多少在原始词典中？\n\n\n\nReveal Code\n\ntop500 &lt;- names(sort(target_sim[,1], decreasing = T))[1:500]\n\ntable(top500%in%care_words)\nFALSE  TRUE \n  386   114        \n\n\n检查你在上一步中计算出的前 500 个词中不在原始 care 词典中的词。这些词是否体现了 “care” 的概念？\n\n\n\nReveal Code\n\ntop500[!top500%in%care_words]\n  [1] \"traumatized\"     \"victimized\"      \"cruelly\"         \"helpless\"       \n  [5] \"innocent\"        \"unborn\"          \"sufferings\"      \"endure\"         \n  [9] \"terrified\"       \"frightened\"      \"sick\"            \"callous\"        \n [13] \"wronged\"         \"civilians\"       \"viciously\"       \"senseless\"      \n [17] \"terrorizing\"     \"frighten\"        \"beatings\"        \"horrific\"       \n [21] \"risking\"         \"inhumane\"        \"unspeakable\"     \"confess\"        \n [25] \"humiliation\"     \"defenseless\"     \"loneliness\"      \"terrorize\"      \n [29] \"grief\"           \"bereaved\"        \"injustice\"       \"subjecting\"     \n [33] \"vengeful\"        \"innocents\"       \"misfortune\"      \"abuse\"          \n [37] \"neglect\"         \"plight\"          \"treating\"        \"traumatised\"    \n [41] \"stigmatized\"     \"oppression\"      \"starving\"        \"humiliate\"      \n [45] \"oftentimes\"      \"grievous\"        \"unjustly\"        \"grieving\"       \n [49] \"humiliated\"      \"betrayed\"        \"spared\"          \"mutilation\"     \n [53] \"lest\"            \"ashamed\"         \"heinous\"         \"enemies\"        \n [57] \"abusive\"         \"barbaric\"        \"elderly\"         \"starve\"         \n [61] \"fear\"            \"neglecting\"      \"terrible\"        \"deprived\"       \n [65] \"dignity\"         \"deprive\"         \"beings\"          \"dying\"          \n [69] \"mercilessly\"     \"hatred\"          \"grievously\"      \"oneself\"        \n [73] \"vicious\"         \"succumb\"         \"feelings\"        \"schoolmates\"    \n [77] \"fearful\"         \"horrible\"        \"horrifying\"      \"sparing\"        \n [81] \"hardships\"       \"afraid\"          \"risked\"          \"indiscriminate\" \n [85] \"unnecessarily\"   \"bystanders\"      \"heartless\"       \"shame\"          \n [89] \"ordeal\"          \"offends\"         \"subjected\"       \"enslavement\"    \n [93] \"handicapped\"     \"disturbed\"       \"humanity\"        \"sacrificing\"    \n [97] \"degrading\"       \"oppressed\"       \"treat\"           \"retribution\"    \n[101] \"maiming\"         \"injure\"          \"terrify\"         \"terminally\"     \n[105] \"unbearable\"      \"betray\"          \"suicidal\"        \"pretending\"     \n[109] \"seriously\"       \"perpetrated\"     \"destitute\"       \"maimed\"         \n[113] \"perpetrator\"     \"insult\"          \"demeaning\"       \"depraved\"       \n[117] \"betraying\"       \"selfishness\"     \"aiding\"          \"traumas\"        \n[121] \"taunt\"           \"brutal\"          \"belittling\"      \"sickening\"      \n[125] \"despair\"         \"needless\"        \"orphans\"         \"repress\"        \n[129] \"terrorized\"      \"hardship\"        \"scared\"          \"infirm\"         \n[133] \"husbands\"        \"savagery\"        \"townspeople\"     \"mentally\"       \n[137] \"punishing\"       \"disciplining\"    \"sins\"            \"savagely\"       \n[141] \"escaping\"        \"teasing\"         \"sickness\"        \"distraught\"     \n[145] \"captors\"         \"pregnant\"        \"oppress\"         \"spouse\"         \n[149] \"indignity\"       \"intolerable\"     \"intimidated\"     \"forgiveness\"    \n[153] \"countrymen\"      \"cursed\"          \"conscience\"      \"knowing\"        \n[157] \"trauma\"          \"deserve\"         \"punishes\"        \"strangling\"     \n[161] \"wickedness\"      \"sinful\"          \"punished\"        \"cruelties\"      \n[165] \"helplessness\"    \"punish\"          \"disrespect\"      \"perceived\"      \n[169] \"appalling\"       \"hysterical\"      \"bystander\"       \"disfigurement\"  \n[173] \"treachery\"       \"homosexuals\"     \"debilitating\"    \"evils\"          \n[177] \"taunts\"          \"vindictive\"      \"betrayal\"        \"tolerate\"       \n[181] \"provokes\"        \"thoughtless\"     \"avenge\"          \"intimidate\"     \n[185] \"feel\"            \"offend\"          \"despicable\"      \"needlessly\"     \n[189] \"malnourished\"    \"sexually\"        \"tolerating\"      \"bodily\"         \n[193] \"terrifying\"      \"passivity\"       \"abduct\"          \"betrays\"        \n[197] \"trusting\"        \"frightening\"     \"expose\"          \"misguided\"      \n[201] \"barbarity\"       \"severely\"        \"perceive\"        \"imprison\"       \n[205] \"horrendous\"      \"depriving\"       \"sadistic\"        \"committing\"     \n[209] \"unworthy\"        \"treated\"         \"unjust\"          \"slaughtering\"   \n[213] \"painful\"         \"debilitated\"     \"scolding\"        \"enslave\"        \n[217] \"aftereffects\"    \"disrespectful\"   \"victimised\"      \"awaken\"         \n[221] \"humankind\"       \"resentful\"       \"suffocate\"       \"hypocritical\"   \n[225] \"cowardly\"        \"fleeing\"         \"judgmental\"      \"ridicule\"       \n[229] \"insecurities\"    \"complicit\"       \"misery\"          \"caretakers\"     \n[233] \"detriment\"       \"physically\"      \"tenderly\"        \"habitually\"     \n[237] \"malnutrition\"    \"punishment\"      \"misbehaving\"     \"villagers\"      \n[241] \"emotionally\"     \"relatives\"       \"selfish\"         \"deserving\"      \n[245] \"destitution\"     \"gravely\"         \"exposing\"        \"companionship\"  \n[249] \"squeamish\"       \"brutally\"        \"guilt\"           \"imprisoning\"    \n[253] \"orphaned\"        \"criminals\"       \"injustices\"      \"adolescents\"    \n[257] \"stigma\"          \"disobedient\"     \"sacrificed\"      \"survivors\"      \n[261] \"remorse\"         \"empathetic\"      \"wanton\"          \"insults\"        \n[265] \"unsuspecting\"    \"enslaving\"       \"aggression\"      \"involuntarily\"  \n[269] \"outweighs\"       \"ignorance\"       \"screams\"         \"addicted\"       \n[273] \"ostracized\"      \"subordinates\"    \"immoral\"         \"caregivers\"     \n[277] \"merciless\"       \"perpetrators\"    \"powerlessness\"   \"mutilating\"     \n[281] \"ills\"            \"depredations\"    \"stab\"            \"affection\"      \n[285] \"ill-treatment\"   \"ill\"             \"blinded\"         \"callously\"      \n[289] \"insulted\"        \"manipulative\"    \"verbally\"        \"unintentionally\"\n[293] \"distract\"        \"illness\"         \"ungrateful\"      \"deprivation\"    \n[297] \"pretend\"         \"undermines\"      \"disfigured\"      \"impotent\"       \n[301] \"cursing\"         \"inconvenience\"   \"motivates\"       \"witnessing\"     \n[305] \"aggressors\"      \"jealousy\"        \"suffocating\"     \"children\"       \n[309] \"sacrifices\"      \"jealous\"         \"hideous\"         \"ostracism\"      \n[313] \"perverted\"       \"horrified\"       \"insensitive\"     \"confronting\"    \n[317] \"illnesses\"       \"befriend\"        \"scourge\"         \"willful\"        \n[321] \"repression\"      \"revenge\"         \"annoy\"           \"scarred\"        \n[325] \"curses\"          \"throats\"         \"jailers\"         \"horribly\"       \n[329] \"repressed\"       \"bigotry\"         \"excruciating\"    \"befriending\"    \n[333] \"coerce\"          \"traumatic\"       \"ugliness\"        \"revulsion\"      \n[337] \"exposes\"         \"patronizing\"     \"stepmother\"      \"newborns\"       \n[341] \"indulging\"       \"promiscuous\"     \"confront\"        \"ministering\"    \n[345] \"unconscionable\"  \"insidious\"       \"tenderness\"      \"indifference\"   \n[349] \"gruesome\"        \"trampling\"       \"embittered\"      \"despise\"        \n[353] \"cowardice\"       \"hopelessness\"    \"underlings\"      \"horrors\"        \n[357] \"scold\"           \"atrocious\"       \"banish\"          \"persecutions\"   \n[361] \"brainwashed\"     \"indignities\"     \"banishing\"       \"pretended\"      \n[365] \"abhorrent\"       \"wrongs\"          \"genitals\"        \"counseled\"      \n[369] \"atrocities\"      \"susceptible\"     \"ferocity\"        \"indifferent\"    \n[373] \"autistic\"        \"deprivations\"    \"demoralizing\"    \"unfaithful\"     \n[377] \"transgressions\"  \"brainwashing\"    \"ghastly\"         \"insufferable\"   \n[381] \"taunting\"        \"pillage\"         \"believing\"       \"unconscious\"    \n[385] \"afflicting\"      \"muggers\"        \n\n\n你对上一个问题的回答对这种词典扩展方法有何启示？\n\n\n\nReveal Code\n\n\n这里的核心思想是，使用词嵌入（word embeddings）使我们能够自动扩展与某一概念相关的词汇集合，而此前我们是通过手动选定的一组词典词语来进行测量的。\n\n\n这意味着我们可以利用词语之间的相似性信息来补充现有的测量方法。至于这是否会在分类任务中带来性能上的提升，将是下面课后思考中要探讨的主题。"
  },
  {
    "objectID": "Seminar/Seminar5.html#课后思考",
    "href": "Seminar/Seminar5.html#课后思考",
    "title": "研讨会5：文本作为数据B：词嵌入",
    "section": "",
    "text": "mft_texts 对象中包含了一系列变量，这些变量记录了每条文本由人工标注归属到的道德类别。在今天的课后思考中，你将再次使用基于词典的方法对文本进行打分，并将这些词典分数与人工编码进行比较。如果你忘记了如何应用词典，可以回顾研讨会2的材料。\n\n请创建一个新的词典对象，其中应包含两个类别：\n\n\ncare_original_words：只包含原始 care 词典中的词语；\ncare_embedding_words：包含原始 care 词典词语以及你在上一部分中提取出的前 500 个最相似词语。\n\n\n\nReveal Code\n\n# 创建 dfm\nmft_dfm &lt;- mft_texts %&gt;% \n  corpus(text_field = \"text\") %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 5)\n\n# 创建 care 词典\ncare_dictionary &lt;- dictionary(list(care_original = care_words,\n                                   care_embedding = c(top500[!top500%in%care_words], care_words)))\n\n\n使用你刚刚构建的词典对 mft_texts 对象中的文本进行打分。在该对象中创建变量，指示每条文本是否被相应的词典分类为 care 文本（即如果文本中包含该类别词典中的任意词语，则将其分类为 care 文本）。\n\n\n\nReveal Code\n\n# 使用词典对文本进行打分\ncare_dfm_dictionary &lt;- dfm_lookup(mft_dfm, care_dictionary)\nmft_texts$care_original &lt;- as.numeric(care_dfm_dictionary[,1]) &gt; 0\nmft_texts$care_embedding &lt;- as.numeric(care_dfm_dictionary[,2]) &gt; 0\n\n\n创建一个混淆矩阵，用于将人工标注与词典打分结果进行比较。你需要分别比较以下两个变量与人工标注的匹配情况：\n\n\nmft_texts$care_original：使用原始 care 词典的分类结果\nmft_texts$care_embedding：使用词嵌入扩展后的 care 词典分类结果 然后，通过混淆矩阵评估：哪种方法表现最好 —— 原始词典，还是词嵌入扩展的方法？\n\n\n\nReveal Code\n\n# 计算混淆矩阵：原始 care 词典\ncare_original_confusion &lt;- table( \n  dictionary = mft_texts$care_original &gt; 0,\n  human_coding = mft_texts$care)\n\n# 计算混淆矩阵：嵌入扩展后的 care 词典\ncare_embedding_confusion &lt;- table( \n  dictionary = mft_texts$care_embedding &gt; 0,\n  human_coding = mft_texts$care)\n\n# 输出原始词典的性能统计\nlibrary(caret)\nconfusionMatrix(care_original_confusion, positive = \"TRUE\")\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 11303  2621\n     TRUE   1843  2119\n                                         \n               Accuracy : 0.7504         \n                 95% CI : (0.744, 0.7567)\n    No Information Rate : 0.735          \n    P-Value [Acc &gt; NIR] : 1.326e-06      \n                                         \n                  Kappa : 0.3238         \n                                         \n Mcnemar's Test P-Value : &lt; 2.2e-16      \n                                         \n            Sensitivity : 0.4470         \n            Specificity : 0.8598         \n         Pos Pred Value : 0.5348         \n         Neg Pred Value : 0.8118         \n             Prevalence : 0.2650         \n         Detection Rate : 0.1185         \n   Detection Prevalence : 0.2215         \n      Balanced Accuracy : 0.6534         \n                                         \n       'Positive' Class : TRUE        \nconfusionMatrix(care_embedding_confusion, positive = \"TRUE\")\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 10114  1763\n     TRUE   3032  2977\n                                          \n               Accuracy : 0.7319          \n                 95% CI : (0.7254, 0.7384)\n    No Information Rate : 0.735           \n    P-Value [Acc &gt; NIR] : 0.8265          \n                                          \n                  Kappa : 0.3661          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.6281          \n            Specificity : 0.7694          \n         Pos Pred Value : 0.4954          \n         Neg Pred Value : 0.8516          \n             Prevalence : 0.2650          \n         Detection Rate : 0.1664          \n   Detection Prevalence : 0.3360          \n      Balanced Accuracy : 0.6987          \n                                          \n       'Positive' Class : TRUE    \n\n在本例中，当我们引入基于词嵌入相似度扩展出的词语后，预测的准确率略有下降，但分类的敏感性显著提高。这表明，新增的词语在很大程度上提升了我们识别表达“care”概念文本的能力，尽管这也带来了更多的假阳性结果（即误将非 care 文本分类为 care）。"
  },
  {
    "objectID": "Seminar/Seminar6.html",
    "href": "Seminar/Seminar6.html",
    "title": "研讨会6: 文本作为数据C：词序列",
    "section": "",
    "text": "研讨会6: 文本作为数据C：词序列\n（Coming Soon）"
  },
  {
    "objectID": "Tidyverse包介绍.html",
    "href": "Tidyverse包介绍.html",
    "title": "Tidyverse 包介绍",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n在本课程模块中，我们将使用多个来自 “Tidyverse” 的程序包 —— 这是一个为简化 R 中数据管理、处理与可视化而开发的数据科学套件集合。我们将从安装 Tidyverse 并加载它开始：\ninstall.packages(\"tidyverse\") # 只需安装一次！如果你已经安装了 Tidyverse 软件包，则无需再次运行此行。\nlibrary(tidyverse) # 每当你想使用本页描述的功能时，都需要运行这一行代码\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n为了说明的目的，我们将在本页中使用 mtcars 数据集。mtcars 是 R 中的一个内置数据集，包含了关于 32 种不同汽车模型的信息。它包含 11 个变量，例如燃油效率（mpg）、发动机排量（disp）以及汽缸数量（cyl）等。\n由于其简单性以及包含多种数值和分类变量，mtcars 数据集常被用于教授讲解基础的数据操作、编程和可视化。不过它本身非常无聊，在此表示歉意（我们将在课程中处理非常有趣的数据集！）\n\n\n管道操作符 %&gt;% 是 Tidyverse 中最重要的工具之一。它允许你将一个函数的输出直接传递给下一个函数作为输入，从而使代码更加简洁且易读。\n\n\n# 如果没有 %&gt;%\nsummarise(group_by(mtcars, cyl), avg_mpg = mean(mpg))\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n# 有了 %&gt;%\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg))\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\n\n\ndplyr 包提供了一系列用于数据操作的函数。以下是一些你将经常使用的核心函数：\n\n\n用 filter() 根据条件筛选数据框中的行:\nmtcars %&gt;%\n  filter(cyl == 4)  # 选择 'cyl' 为 4 的行\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n使用 select() 选择特定的列:\nmtcars %&gt;%\n  select(mpg, cyl, gear)  # 只选择 'mpg', 'cyl', and 'gear' 列\n                     mpg cyl gear\nMazda RX4           21.0   6    4\nMazda RX4 Wag       21.0   6    4\nDatsun 710          22.8   4    4\nHornet 4 Drive      21.4   6    3\nHornet Sportabout   18.7   8    3\nValiant             18.1   6    3\nDuster 360          14.3   8    3\nMerc 240D           24.4   4    4\nMerc 230            22.8   4    4\nMerc 280            19.2   6    4\nMerc 280C           17.8   6    4\nMerc 450SE          16.4   8    3\nMerc 450SL          17.3   8    3\nMerc 450SLC         15.2   8    3\nCadillac Fleetwood  10.4   8    3\nLincoln Continental 10.4   8    3\nChrysler Imperial   14.7   8    3\nFiat 128            32.4   4    4\nHonda Civic         30.4   4    4\nToyota Corolla      33.9   4    4\nToyota Corona       21.5   4    3\nDodge Challenger    15.5   8    3\nAMC Javelin         15.2   8    3\nCamaro Z28          13.3   8    3\nPontiac Firebird    19.2   8    3\nFiat X1-9           27.3   4    4\nPorsche 914-2       26.0   4    5\nLotus Europa        30.4   4    5\nFord Pantera L      15.8   8    5\nFerrari Dino        19.7   6    5\nMaserati Bora       15.0   8    5\nVolvo 142E          21.4   4    4\n\n\n\n使用 group_by() 按一个或多个变量对数据进行分组，以便进行聚合操作:\nmtcars %&gt;%\n  group_by(cyl)  # 按汽缸(cylinders)数量进行分组\n# A tibble: 32 × 11\n# Groups:   cyl [3]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\n\n\n使用聚合函数如 mean()、sum() 或 n() 对数据进行汇总:\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg))  # 计算每个气缸组的 'mpg' 平均值\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\n\n使用 mutate() 添加新列或转换已有列:\nmtcars %&gt;%\n  mutate(weight_kg = wt * 453.592)  # 将 '重量' 从 1000 磅转换为千克\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                    weight_kg\nMazda RX4           1188.4110\nMazda RX4 Wag       1304.0770\nDatsun 710          1052.3334\nHornet 4 Drive      1458.2983\nHornet Sportabout   1560.3565\nValiant             1569.4283\nDuster 360          1619.3234\nMerc 240D           1446.9585\nMerc 230            1428.8148\nMerc 280            1560.3565\nMerc 280C           1560.3565\nMerc 450SE          1846.1194\nMerc 450SL          1691.8982\nMerc 450SLC         1714.5778\nCadillac Fleetwood  2381.3580\nLincoln Continental 2460.2830\nChrysler Imperial   2424.4492\nFiat 128             997.9024\nHonda Civic          732.5511\nToyota Corolla       832.3413\nToyota Corona       1118.1043\nDodge Challenger    1596.6438\nAMC Javelin         1558.0885\nCamaro Z28          1741.7933\nPontiac Firebird    1744.0612\nFiat X1-9            877.7005\nPorsche 914-2        970.6869\nLotus Europa         686.2847\nFord Pantera L      1437.8866\nFerrari Dino        1256.4498\nMaserati Bora       1619.3234\nVolvo 142E          1260.9858\n\n\n\n在长格式（long）和宽格式（wide）之间转换数据：\n# Example: pivot_longer\ndata &lt;- tibble(id = 1:3, a = c(10, 20, 30), b = c(40, 50, 60))\ndata %&gt;%\n  pivot_longer(cols = a:b, names_to = \"variable\", values_to = \"value\")\n# A tibble: 6 × 3\n     id variable value\n  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 a           10\n2     1 b           40\n3     2 a           20\n4     2 b           50\n5     3 a           30\n6     3 b           60\n# Example: pivot_wider\ndata_long &lt;- tibble(id = c(1, 1, 2, 2), variable = c(\"a\", \"b\", \"a\", \"b\"), value = c(10, 40, 20, 50))\ndata_long %&gt;%\n  pivot_wider(names_from = variable, values_from = value)\n# A tibble: 2 × 3\n     id     a     b\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    10    40\n2     2    20    50\n\n\n\n\ndplyr 提供了按行匹配合并数据集的函数。常见的连接（join）类型包括：\n\n\n包括两个数据集中的所有行:\ndf1 &lt;- tibble(id = 1:3, value = c(\"A\", \"B\", \"C\"))\ndf2 &lt;- tibble(id = 2:4, value2 = c(\"X\", \"Y\", \"Z\"))\nfull_join(df1, df2, by = \"id\")\n# A tibble: 4 × 3\n     id value value2\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1     1 A     &lt;NA&gt;  \n2     2 B     X     \n3     3 C     Y     \n4     4 &lt;NA&gt;  Z \n\n\n\n包括第二个数据集中的所有行和第一个数据集中的匹配行:\nright_join(df1, df2, by = \"id\")\n# A tibble: 3 × 3\n     id value value2\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1     2 B     X     \n2     3 C     Y     \n3     4 &lt;NA&gt;  Z  \n\n\n\n包括第一个数据集中的所有行和第二个数据集中的匹配行:\nleft_join(df1, df2, by = \"id\")\n# A tibble: 3 × 3\n     id value value2\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1     1 A     &lt;NA&gt;  \n2     2 B     X     \n3     3 C     Y   \n\n\n\nggplot2 是 Tidyverse 中用于数据可视化的主要工具, 它允许你创建具有图层结构且可高度自定义的图形:\n示例：基本散点图\nmtcars %&gt;%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Weight vs MPG\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\n\n示例：分组条形图\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg)) %&gt;%\n  ggplot(aes(x = factor(cyl), y = avg_mpg)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average MPG by Cylinder\",\n       x = \"Cylinders\",\n       y = \"Average MPG\")\n\n\n这并不是 Tidyverse 的完整教程！我会在课程中尽量解释所用函数，但如果你希望更深入学习 Tidyverse，以下资源可能会对你有所帮助：\n\n\n\n\n\nTidyverse Packages Overview: https://www.tidyverse.org 一个集中展示所有 Tidyverse 包、其文档与更新的核心平台。\ndplyr Documentation: https://dplyr.tidyverse.org 了解更多关于数据操作函数的信息，如 filter()、mutate() 和 group_by()。\nggplot2 Documentation: https://ggplot2.tidyverse.org 浏览详细的可视化函数参考资料，深入探索图形创建方法。"
  },
  {
    "objectID": "Tidyverse包介绍.html#the-pipe-operator",
    "href": "Tidyverse包介绍.html#the-pipe-operator",
    "title": "Tidyverse 包介绍",
    "section": "",
    "text": "管道操作符 %&gt;% 是 Tidyverse 中最重要的工具之一。它允许你将一个函数的输出直接传递给下一个函数作为输入，从而使代码更加简洁且易读。\n\n\n# 如果没有 %&gt;%\nsummarise(group_by(mtcars, cyl), avg_mpg = mean(mpg))\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n# 有了 %&gt;%\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg))\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1"
  },
  {
    "objectID": "Tidyverse包介绍.html#dplyr",
    "href": "Tidyverse包介绍.html#dplyr",
    "title": "Tidyverse 包介绍",
    "section": "",
    "text": "dplyr 包提供了一系列用于数据操作的函数。以下是一些你将经常使用的核心函数：\n\n\n用 filter() 根据条件筛选数据框中的行:\nmtcars %&gt;%\n  filter(cyl == 4)  # 选择 'cyl' 为 4 的行\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n使用 select() 选择特定的列:\nmtcars %&gt;%\n  select(mpg, cyl, gear)  # 只选择 'mpg', 'cyl', and 'gear' 列\n                     mpg cyl gear\nMazda RX4           21.0   6    4\nMazda RX4 Wag       21.0   6    4\nDatsun 710          22.8   4    4\nHornet 4 Drive      21.4   6    3\nHornet Sportabout   18.7   8    3\nValiant             18.1   6    3\nDuster 360          14.3   8    3\nMerc 240D           24.4   4    4\nMerc 230            22.8   4    4\nMerc 280            19.2   6    4\nMerc 280C           17.8   6    4\nMerc 450SE          16.4   8    3\nMerc 450SL          17.3   8    3\nMerc 450SLC         15.2   8    3\nCadillac Fleetwood  10.4   8    3\nLincoln Continental 10.4   8    3\nChrysler Imperial   14.7   8    3\nFiat 128            32.4   4    4\nHonda Civic         30.4   4    4\nToyota Corolla      33.9   4    4\nToyota Corona       21.5   4    3\nDodge Challenger    15.5   8    3\nAMC Javelin         15.2   8    3\nCamaro Z28          13.3   8    3\nPontiac Firebird    19.2   8    3\nFiat X1-9           27.3   4    4\nPorsche 914-2       26.0   4    5\nLotus Europa        30.4   4    5\nFord Pantera L      15.8   8    5\nFerrari Dino        19.7   6    5\nMaserati Bora       15.0   8    5\nVolvo 142E          21.4   4    4\n\n\n\n使用 group_by() 按一个或多个变量对数据进行分组，以便进行聚合操作:\nmtcars %&gt;%\n  group_by(cyl)  # 按汽缸(cylinders)数量进行分组\n# A tibble: 32 × 11\n# Groups:   cyl [3]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\n\n\n使用聚合函数如 mean()、sum() 或 n() 对数据进行汇总:\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg))  # 计算每个气缸组的 'mpg' 平均值\n# A tibble: 3 × 2\n    cyl avg_mpg\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\n\n使用 mutate() 添加新列或转换已有列:\nmtcars %&gt;%\n  mutate(weight_kg = wt * 453.592)  # 将 '重量' 从 1000 磅转换为千克\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n                    weight_kg\nMazda RX4           1188.4110\nMazda RX4 Wag       1304.0770\nDatsun 710          1052.3334\nHornet 4 Drive      1458.2983\nHornet Sportabout   1560.3565\nValiant             1569.4283\nDuster 360          1619.3234\nMerc 240D           1446.9585\nMerc 230            1428.8148\nMerc 280            1560.3565\nMerc 280C           1560.3565\nMerc 450SE          1846.1194\nMerc 450SL          1691.8982\nMerc 450SLC         1714.5778\nCadillac Fleetwood  2381.3580\nLincoln Continental 2460.2830\nChrysler Imperial   2424.4492\nFiat 128             997.9024\nHonda Civic          732.5511\nToyota Corolla       832.3413\nToyota Corona       1118.1043\nDodge Challenger    1596.6438\nAMC Javelin         1558.0885\nCamaro Z28          1741.7933\nPontiac Firebird    1744.0612\nFiat X1-9            877.7005\nPorsche 914-2        970.6869\nLotus Europa         686.2847\nFord Pantera L      1437.8866\nFerrari Dino        1256.4498\nMaserati Bora       1619.3234\nVolvo 142E          1260.9858\n\n\n\n在长格式（long）和宽格式（wide）之间转换数据：\n# Example: pivot_longer\ndata &lt;- tibble(id = 1:3, a = c(10, 20, 30), b = c(40, 50, 60))\ndata %&gt;%\n  pivot_longer(cols = a:b, names_to = \"variable\", values_to = \"value\")\n# A tibble: 6 × 3\n     id variable value\n  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 a           10\n2     1 b           40\n3     2 a           20\n4     2 b           50\n5     3 a           30\n6     3 b           60\n# Example: pivot_wider\ndata_long &lt;- tibble(id = c(1, 1, 2, 2), variable = c(\"a\", \"b\", \"a\", \"b\"), value = c(10, 40, 20, 50))\ndata_long %&gt;%\n  pivot_wider(names_from = variable, values_from = value)\n# A tibble: 2 × 3\n     id     a     b\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    10    40\n2     2    20    50"
  },
  {
    "objectID": "Tidyverse包介绍.html#连接join",
    "href": "Tidyverse包介绍.html#连接join",
    "title": "Tidyverse 包介绍",
    "section": "",
    "text": "dplyr 提供了按行匹配合并数据集的函数。常见的连接（join）类型包括：\n\n\n包括两个数据集中的所有行:\ndf1 &lt;- tibble(id = 1:3, value = c(\"A\", \"B\", \"C\"))\ndf2 &lt;- tibble(id = 2:4, value2 = c(\"X\", \"Y\", \"Z\"))\nfull_join(df1, df2, by = \"id\")\n# A tibble: 4 × 3\n     id value value2\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1     1 A     &lt;NA&gt;  \n2     2 B     X     \n3     3 C     Y     \n4     4 &lt;NA&gt;  Z \n\n\n\n包括第二个数据集中的所有行和第一个数据集中的匹配行:\nright_join(df1, df2, by = \"id\")\n# A tibble: 3 × 3\n     id value value2\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1     2 B     X     \n2     3 C     Y     \n3     4 &lt;NA&gt;  Z  \n\n\n\n包括第一个数据集中的所有行和第二个数据集中的匹配行:\nleft_join(df1, df2, by = \"id\")\n# A tibble: 3 × 3\n     id value value2\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1     1 A     &lt;NA&gt;  \n2     2 B     X     \n3     3 C     Y   \n\n\n\nggplot2 是 Tidyverse 中用于数据可视化的主要工具, 它允许你创建具有图层结构且可高度自定义的图形:\n示例：基本散点图\nmtcars %&gt;%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Weight vs MPG\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\n\n示例：分组条形图\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg_mpg = mean(mpg)) %&gt;%\n  ggplot(aes(x = factor(cyl), y = avg_mpg)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average MPG by Cylinder\",\n       x = \"Cylinders\",\n       y = \"Average MPG\")\n\n\n这并不是 Tidyverse 的完整教程！我会在课程中尽量解释所用函数，但如果你希望更深入学习 Tidyverse，以下资源可能会对你有所帮助："
  },
  {
    "objectID": "Tidyverse包介绍.html#官方的-tidyverse-介绍",
    "href": "Tidyverse包介绍.html#官方的-tidyverse-介绍",
    "title": "Tidyverse 包介绍",
    "section": "",
    "text": "Tidyverse Packages Overview: https://www.tidyverse.org 一个集中展示所有 Tidyverse 包、其文档与更新的核心平台。\ndplyr Documentation: https://dplyr.tidyverse.org 了解更多关于数据操作函数的信息，如 filter()、mutate() 和 group_by()。\nggplot2 Documentation: https://ggplot2.tidyverse.org 浏览详细的可视化函数参考资料，深入探索图形创建方法。"
  },
  {
    "objectID": "代码帮助.html",
    "href": "代码帮助.html",
    "title": "代码帮助",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n\n\n\n\n在 R 中，有多种方法可以保存和导出图形。其中最常见的一种是使用 png() 函数，它会创建一个 PNG 文件并打开一个绘图设备。该函数的语法如下：\npng(filename, width, height, units = \"px\", res = 72)\n\nfilename 是 PNG 文件应保存的文件名及路径。\nwidth 和 height 分别表示 PNG 文件的宽度和高度（以像素为单位）。\nunits 指定宽度和高度的单位，默认值为 “px”（像素）。\nres 是 PNG 文件的分辨率，以每英寸点数（DPI）计，默认值为 72 DPI。\n\n一旦绘图设备被打开，你就可以使用任何标准的 R 绘图函数（如 plot()、barplot()、hist() 等）来创建图形。一旦图形绘制完成，你可以通过调用 dev.off() 函数将其保存为 PNG 文件。\n例如，假设我们有一个名为 data_for_plots 的数据框：\nhead(data_for_plots)\n  variable_a variable_b\n1  0.1703222  0.6700134\n2  0.1612494  1.5436437\n3 -2.0870488  1.1083646\n4  0.6901655  0.9038989\n5 -1.0462657 -0.3617654\n6 -0.1199232 -0.4250342\n我们可以创建一个图表，并保存如下：\npng(\"path_to_folder/file_name.png\", width = 500, height = 500)\nplot(x = data_for_plots$variable_a,\n     y = data_for_plots$variable_b,\n     xlab = \"X-axis variable name\",\n     ylab = \"Y-axis variable name\")\ndev.off()\n你也可以将图形保存为其他文件类型，例如 pdf、jpeg、bmp 等，通过使用对应的函数，如 pdf()、jpeg()、bmp()。需要特别注意的是，在图形创建并保存之后，始终应调用 dev.off() 函数，以确保绘图设备被正确关闭。\n\n\n\n另一种保存图形的方法是使用 ggsave() 函数（来自 ggplot2 包）。ggsave() 会保存你最近创建的那一个 ggplot 图形:\nggsave(filename, width = 7, height = 7, units = c(\"in\", \"cm\", \"mm\"), dpi = 300)\n\nfilename 是图形应保存的文件名及路径。\nwidth 和 height 分别表示图形的宽度和高度，单位由 units 参数指定。\nunits 是宽度和高度的单位，默认值为 “in”（英寸）。\ndpi 是图形的分辨率，以每英寸点数（DPI）表示，默认值为 300 DPI。\n\n例如：\nggplot(data_for_plots,\n       aes(x = variable_a,\n           y = variable_b)) + \n  geom_point() + \n  xlab(\"X-axis variable name\") + \n  ylab(\"Y-axis variable name\")\nggsave(\"path_to_folder/file_name.png\", width = 6, height = 6)\n\n\n\ngroup_by() 和 summarise() 函数是 dplyr 包的一部分，当你调用 library(tidyverse) 时会加载该包，它们通常用于执行数据聚合和汇总。\ngroup_by() 函数用于按一个或多个变量对数据框进行分组。例如，如果你有一个名为 data 的数据框，包含列 “A”、“B” 和 “C”，你可以使用以下代码按列 “A” 对数据进行分组：\ndata %&gt;%\n  group_by(A)\n你也可以通过传递多个列名作为参数，按多个列进行分组。例如：\ndata %&gt;% \n  group_by(A, B)\n然后可以使用 summarise() 函数对分组后的数据进行汇总。该函数接受一个或多个参数，这些参数是你希望计算的汇总统计量。例如，你可以使用以下代码计算每个分组中列 “C” 的平均值：\ndata %&gt;% \n  group_by(A) %&gt;% \n  summarise(mean_C = mean(C))\n你也可以根据需要使用多个不同的汇总统计量，例如 sum、max、min 等。例如：\ndata %&gt;% group_by(A) %&gt;% \n  summarise(mean_C = mean(C),\n            median_B = median(B))\n需要注意的是，group_by() 函数必须在 summarise() 函数之前调用，且 summarise() 应是操作链中的最后一个函数。summarise() 返回的是一个新的数据框，因此将结果赋值给一个新对象是一个良好的实践：\nmy_summary_object &lt;- data %&gt;% \n  group_by(A) %&gt;% \n  summarise(mean_C = mean(C),\n            median_B = median(B))"
  },
  {
    "objectID": "代码帮助.html#保存图表",
    "href": "代码帮助.html#保存图表",
    "title": "代码帮助",
    "section": "",
    "text": "在 R 中，有多种方法可以保存和导出图形。其中最常见的一种是使用 png() 函数，它会创建一个 PNG 文件并打开一个绘图设备。该函数的语法如下：\npng(filename, width, height, units = \"px\", res = 72)\n\nfilename 是 PNG 文件应保存的文件名及路径。\nwidth 和 height 分别表示 PNG 文件的宽度和高度（以像素为单位）。\nunits 指定宽度和高度的单位，默认值为 “px”（像素）。\nres 是 PNG 文件的分辨率，以每英寸点数（DPI）计，默认值为 72 DPI。\n\n一旦绘图设备被打开，你就可以使用任何标准的 R 绘图函数（如 plot()、barplot()、hist() 等）来创建图形。一旦图形绘制完成，你可以通过调用 dev.off() 函数将其保存为 PNG 文件。\n例如，假设我们有一个名为 data_for_plots 的数据框：\nhead(data_for_plots)\n  variable_a variable_b\n1  0.1703222  0.6700134\n2  0.1612494  1.5436437\n3 -2.0870488  1.1083646\n4  0.6901655  0.9038989\n5 -1.0462657 -0.3617654\n6 -0.1199232 -0.4250342\n我们可以创建一个图表，并保存如下：\npng(\"path_to_folder/file_name.png\", width = 500, height = 500)\nplot(x = data_for_plots$variable_a,\n     y = data_for_plots$variable_b,\n     xlab = \"X-axis variable name\",\n     ylab = \"Y-axis variable name\")\ndev.off()\n你也可以将图形保存为其他文件类型，例如 pdf、jpeg、bmp 等，通过使用对应的函数，如 pdf()、jpeg()、bmp()。需要特别注意的是，在图形创建并保存之后，始终应调用 dev.off() 函数，以确保绘图设备被正确关闭。\n\n\n\n另一种保存图形的方法是使用 ggsave() 函数（来自 ggplot2 包）。ggsave() 会保存你最近创建的那一个 ggplot 图形:\nggsave(filename, width = 7, height = 7, units = c(\"in\", \"cm\", \"mm\"), dpi = 300)\n\nfilename 是图形应保存的文件名及路径。\nwidth 和 height 分别表示图形的宽度和高度，单位由 units 参数指定。\nunits 是宽度和高度的单位，默认值为 “in”（英寸）。\ndpi 是图形的分辨率，以每英寸点数（DPI）表示，默认值为 300 DPI。\n\n例如：\nggplot(data_for_plots,\n       aes(x = variable_a,\n           y = variable_b)) + \n  geom_point() + \n  xlab(\"X-axis variable name\") + \n  ylab(\"Y-axis variable name\")\nggsave(\"path_to_folder/file_name.png\", width = 6, height = 6)\n\n\n\ngroup_by() 和 summarise() 函数是 dplyr 包的一部分，当你调用 library(tidyverse) 时会加载该包，它们通常用于执行数据聚合和汇总。\ngroup_by() 函数用于按一个或多个变量对数据框进行分组。例如，如果你有一个名为 data 的数据框，包含列 “A”、“B” 和 “C”，你可以使用以下代码按列 “A” 对数据进行分组：\ndata %&gt;%\n  group_by(A)\n你也可以通过传递多个列名作为参数，按多个列进行分组。例如：\ndata %&gt;% \n  group_by(A, B)\n然后可以使用 summarise() 函数对分组后的数据进行汇总。该函数接受一个或多个参数，这些参数是你希望计算的汇总统计量。例如，你可以使用以下代码计算每个分组中列 “C” 的平均值：\ndata %&gt;% \n  group_by(A) %&gt;% \n  summarise(mean_C = mean(C))\n你也可以根据需要使用多个不同的汇总统计量，例如 sum、max、min 等。例如：\ndata %&gt;% group_by(A) %&gt;% \n  summarise(mean_C = mean(C),\n            median_B = median(B))\n需要注意的是，group_by() 函数必须在 summarise() 函数之前调用，且 summarise() 应是操作链中的最后一个函数。summarise() 返回的是一个新的数据框，因此将结果赋值给一个新对象是一个良好的实践：\nmy_summary_object &lt;- data %&gt;% \n  group_by(A) %&gt;% \n  summarise(mean_C = mean(C),\n            median_B = median(B))"
  },
  {
    "objectID": "简单python入门.html",
    "href": "简单python入门.html",
    "title": "Python 🐍",
    "section": "",
    "text": "Python 支持多种编程范式, 包括过程式编程(procedural)、函数式编程(functional)和面向对象编程(object-oriented programming, OOP)。其中, 理解 Python 的面向对象特性是高效使用这门语言的关键。\n\n在 Python 中，一切皆对象 —— 无论是整数、字符串，还是函数，甚至类本身，都是对象。对象是类(classes)的实例, 封装了属性(data/state)和行为(methods/functions)。这种结构使得对象之间能够以可预测、模块化的方式进行交互，从而更容易设计、扩展和维护复杂的系统。\n\n\n\n\n\n\n\n\n\n\n\n\n\n运算符\n描述说明\n示例\n输出结果\n\n\n\n\n=\n赋值运算符\nx = 5\nx 的值现在是 5\n\n\n+\n加法\n2 + 3\n5\n\n\n-\n减法\n5 - 2\n3\n\n\n*\n乘法\n4 * 2\n8\n\n\n/\n除法（返回浮点数）\n5 / 2\n2.5\n\n\n//\n整除（向下取整）\n5 // 2\n2\n\n\n%\n取余（模运算）\n5 % 2\n1\n\n\n**\n幂运算（指数）\n2 ** 3\n8\n\n\n@\n矩阵乘法（如用于 NumPy 矩阵对象）\nA @ B\n矩阵结果\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n运算符\n类型\n描述说明\n示例\n输出结果\n\n\n\n\n==\n比较运算符\n判断两个值是否相等\n5 == 5\nTrue\n\n\n!=\n比较运算符\n判断两个值是否不相等\n5 != 3\nTrue\n\n\n&gt;\n比较运算符\n判断左边是否大于右边\n5 &gt; 3\nTrue\n\n\n&lt;\n比较运算符\n判断左边是否小于右边\n2 &lt; 4\nTrue\n\n\n&gt;=\n比较运算符\n判断左边是否大于等于右边\n5 &gt;= 5\nTrue\n\n\n&lt;=\n比较运算符\n判断左边是否小于等于右边\n3 &lt;= 3\nTrue\n\n\nand\n逻辑运算符\n逻辑与，两个条件都为 True 时返回 True\nTrue and False\nFalse\n\n\nor\n逻辑运算符\n逻辑或，任一条件为 True 时返回 True\nFalse or True\nTrue\n\n\nnot\n逻辑运算符\n逻辑非，将 True 变为 False，反之亦然\nnot True\nFalse\n\n\n\n\n#@title 做些尝试！\n\nmy_var = 5 + 6\nanother_var = \"Hello World！！！\"\n\nnext_var = my_var ** 2\nnext_var += my_var\n\n# next_var + another_var\nanother_var + \"UCL is better than LSE\"\n\nmany_things = [x for x in range(0, 10, 2)]\nmany_things\n\ntype(my_var)\nisinstance(another_var, str)\n\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\nOutput\n\n\n\n\n==\nEqual to\n3 == 3\nTrue\n\n\n!=\nNot equal to\n3 != 2\nTrue\n\n\n&gt;\nGreater than\n3 &gt; 2\nTrue\n\n\n&lt;\nLess than\n2 &lt; 3\nTrue\n\n\n&gt;=\nGreater than or equal to\n3 &gt;= 3\nTrue\n\n\n&lt;=\nLess than or equal to\n2 &lt;= 3\nTrue\n\n\n\n\n\n3 == \"3\" #False\n3 == 3.000 #Ture\n\n\n\n\nPython 提供了几种内建的数据容器, 帮助你高效地存储和组织数据。对于初学者来说, 最常见的四种是: 列表(list)、字典(dictionary)、元组(tuple)和集合(set)。\n\n列表 ([]): 有序的、可变的元素集合。 列表可以包含不同类型的元素，创建后可以修改、添加或删除其中的内容。 示例: my_list = [1, 'apple', True]\n字典 ({}): 无序的键-值对集合。 每个键(key)必须唯一，字典用于通过键快速查找或存储数据. 示例: my_dict = {'name': 'Trump', 'age': 78}\n元组 (()): 有序的、不可变的元素集合。 与列表类似，元组可以存储多种类型的值，但创建后内容不可更改。 示例: my_tuple = (10, 'banana', False)\n集合 ({}): 无序的、唯一元素的集合。 集合自动去除重复值，适用于进行成员测试或消除冗余数据的场景。 示例: my_set = {1, 1, 2, 3} → {1, 2, 3}\n\n这些数据容器各有优势，掌握它们的使用场景是编写简洁高效 Python 代码的关键一步。\n\n\n\n\n\n\n\n\n\n\n\n\n数据容器\n符号\n是否有序\n是否可变\n是否支持重复\n典型用途\n示例\n\n\n\n\n列表 (List)\n[]\n是\n可变\n支持\n存储可变的有序数据集合\n[1, 'apple', True]\n\n\n字典 (Dictionary)\n{}\n否\n可变\n键唯一\n以键访问数据，查找快\n{'name': 'Trump', 'age': 78}\n\n\n元组 (Tuple)\n()\n是\n不可变\n支持\n存储不可变的有序数据\n(10, 'banana', False)\n\n\n集合 (Set)\n{}\n否\n可变\n不支持\n去重、测试成员资格\n{1, 1, 2, 3} → {1, 2, 3}\n\n\n\n\n#@title Example List\nmy_list = [1, 1, 2, 3, 5, 8]\n\nmy_list.append(6)\nmy_list\n\nmy_list_sqrd = [x**2 for x in my_list]\nmy_list_sqrd\n\n\nmy_list = [el for el in range (0, 20)]\nmy_list = [el for el in range (0, 20, 2)] #以2为间隔（即print 偶数）\nmy_list\n\nmy_list.append(\"Hello World\")\nmy_list\n\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 'Hello World']\n\n\n\n#@title Example Dictionary\nex_dict = {\n    \"a\": 5,\n    \"b\": 6,\n    \"c\": [1, 2, 3]\n}\n\nex_dict.items()\nex_dict.keys()\nex_dict.values()\n\nex_dict.get(\"a\", 0)\nex_dict.get(\"d\", 0)\n\nex_dict['a']\nex_dict['d'] = 4.5\n\n\n\n\n在 Python 中, 索引是访问数据容器中单个元素的方式, 适用于列表(list)、元组(tuple)、字符串(string)和字典(dictionary)等数据结构。\nPython 使用从 0 开始的索引(zero-based indexing), 也就是说:\n\n第一个元素的位置是 0, 第二个是 1, 依此类推。\n同时，你也可以使用负索引(negative indexing)从序列的末尾开始计数，其中 -1 表示最后一个元素。\n\n列表、元组、字符串 等顺序类型(sequences)使用位置索引(positional indexing), 索引必须是整数。\n\n如果访问一个不存在的索引位置，将会引发索引错误 (IndexError)。\n\n与此不同, 字典(dictionary)使用的是键索引(key-based indexing), 而不是位置索引。 你通过键来访问对应的值，而不是通过数字位置。\n \n\n\n\n\n\n\n\n\n\n\n\n数据类型\n索引类型\n是否有顺序\n支持负索引\n索引是否为整数\n示例\n\n\n\n\n列表 (List)\n位置索引\n是\n是\n是\nmy_list[0], my_list[-1]\n\n\n元组 (Tuple)\n位置索引\n是\n是\n是\nmy_tuple[1], my_tuple[-2]\n\n\n字符串 (String)\n位置索引\n是\n是\n是\n'hello'[0], 'hello'[-1]\n\n\n字典 (Dict)\n键索引（key）\n否\n否\n否（使用键）\nmy_dict['name']\n\n\n集合 (Set)\n不支持索引\n否\n否\n否\n❌ 不支持直接索引访问\n\n\n\n\n#@title Indexing Example\n\nmy_list[0] # 第一个元素\nmy_list[1] # 第二个元素\nmy_list[-1] # 最后一个元素\n\nex_dict['a'] # 索引键 “a” 对应的值\n\nmy_list = ['a', 'b', 'c']\nprint(my_list[0])   # 输出: 'a'\nprint(my_list[-1])  # 输出: 'c'\n\nmy_dict = {'name': 'Trump', 'age': 78}\nprint(my_dict['name'])  # 输出: 'Trump'\n\na\nc\nTrump\n\n\n\n\n\n切片是从序列中访问一部分元素的方式 适用于列表(list)、元组(tuple)和字符串(string)等序列类型。\n基本语法是: sequence[start:stop]\n\n返回从 start 索引开始、到 stop 索引之前(不包括 stop)的一个新子序列。\n注意：切片中的 stop 是“左闭右开”的，即包括起始，不包括结束。\n\n你还可以加入步长参数, 语法是: sequence[start:stop:step]\n\nstep 表示每次移动多少个索引单位，默认是 1。\nstep 可以为负值，这样就可以实现反向切片。\n\n\n#@title Slicing Example\nmy_list = [0, 1, 2, 3, 4, 5]\n\nprint(my_list[1:4])      # 输出 [1, 2, 3]\nprint(my_list[:3])       # 输出 [0, 1, 2]\nprint(my_list[::2])      # 输出 [0, 2, 4]\nprint(my_list[::-1])     # 输出 [5, 4, 3, 2, 1, 0]，反转列表\n\n[1, 2, 3]\n[0, 1, 2]\n[0, 2, 4]\n[5, 4, 3, 2, 1, 0]\n\n\n\n\n\n在 Python 中，我们经常使用以下两个运算符来判断对象之间的关系或内存身份：\n\n\n\n\n\n\n\n\n\n运算符\n作用说明\n示例\n输出结果\n\n\n\n\nin\n成员运算符, 判断一个元素是否存在于容器中\n'a' in 'cat'\nTrue\n\n\nis\n身份运算符, 判断两个变量是否引用同一个对象(内存地址)\na is b\nTrue or False\n\n\n\n🔍 拓展说明：\n\nin 可以用于列表、字符串、元组、字典（对键）等容器类型：\n\n3 in [1, 2, 3]       # True\n'key' in {'key': 1}  # True\n\nis 是检查两个变量是否是同一个对象 (即 id(a) == id(b))\n\na = [1, 2]\nb = a\nc = [1, 2]\n\na is b  # True\na is c  # False（内容相同，但不是同一个对象）\n\n\n\n循环语句让你可以重复执行一段代码块, 是实现自动化、遍历数据和控制流程的重要工具。Python 中最常用的两种循环结构是: for 循环 和 while 循环。\n \n🔁 for 循环\n当你需要遍历 (iterate)一个序列（如列表、字符串或数字范围）时使用 for 循环。\nfor item in [1, 2, 3]:\n    print(item)\noutput:\n1\n2\n3  \n \n🔁 while 循环\n当你不确定需要循环多少次，仅知道某个条件为真时要循环时，使用 while 循环。\nx = 0\nwhile x &lt; 3:\n    print(x)\n    x += 1\noutput:\n0\n1\n2\n \n🔑 Python 中的循环关键字(保留字):\nfor, while, in, break等，是 Python 语言的保留关键字(reserved keywords), 具有特殊含义, 不能用作变量名:\n \n\n\n\n\n\n\n\n\n特性\nfor 循环\nwhile 循环\n\n\n\n\n用途\n遍历序列（list、tuple、string、range 等）\n在某个条件为真时反复执行\n\n\n是否需要初始化变量\n否（自动从序列中取值）\n是（通常需要初始化计数器变量）\n\n\n适用场景\n已知迭代次数、处理集合数据\n条件控制循环，如等待用户输入、监测状态等\n\n\n是否需指定终止条件\n否（由序列长度决定）\n是（显式指定条件）\n\n\n可用关键词\nfor, in, break, continue\nwhile, break, continue\n\n\n可否无限循环\n否（除非使用 while True 的等价写法）\n可以 (while True: 创建死循环)\n\n\n\n\n\n\n条件语句允许程序根据条件是否成立来决定执行哪一段代码。这让你的程序能够进行“判断”和“分支”。\n \n🔒 基本结构说明：\n\nif判断一个条件是否为真,若为真，则执行对应代码块。\nelif (“else if”的缩写) 如果前面的 if 条件为假，尝试判断另一个条件。\nelse 如果以上所有条件都为假,则执行这一部分代码。\n\n \n\n\n\n\n\n\n\n\n\n关键词\n中文释义\n用途说明\n示例\n\n\n\n\nif\n如果\n判断条件是否为真，执行代码块\nif x &gt; 0:\n\n\nelif\n否则如果\n上一个条件为假时，检查另一个条件\nelif x == 0:\n\n\nelse\n否则\n所有条件都为假时执行\nelse:\n\n\n\n\n#@title Example Conditional Statement\nx = 10\n\nif x &gt; 15:\n    print(\"x is greater than 15\")\nelif x &gt; 5:\n    print(\"x is greater than 5 but not more than 15\")\nelse:\n    print(\"x is 5 or less\")\n\nx is greater than 5 but not more than 15\n\n\n🌟小拓展：嵌套条件语句 (Nested if)与三元表达式 (One-liner if)\n嵌套条件语句 (Nested if): 是指在一个 if 或 else 的代码块中，再写一个 if 判断，用于处理更复杂的决策逻辑。\nx = 15\n\nif x &gt; 0:\n    if x &lt; 10:\n        print(\"x 是一个小的正数\")\n    else:\n        print(\"x 是一个较大的正数\")\nelse:\n    print(\"x 不是正数\")\noutput:\nx 是一个较大的正数  \n \n三元表达式 (One-liner if): 也叫条件表达式 (Ternary Expression), 是一种简洁写法，适合在一行中进行简单判断和赋值。\n#语法格式：\n变量 = 表达式1 if 条件 else 表达式2\n#示例\nx = 5\nresult = \"正数\" if x &gt; 0 else \"非正数\"\nprint(result)   # 输出：正数\n\n\n\n\n\n\n运算符\n描述说明\n示例\n输出结果\n\n\n\n\nand\n逻辑与（AND）\nTrue and False\nFalse\n\n\nor\n逻辑或（OR）\nTrue or False\nTrue\n\n\nnot\n逻辑非（NOT）\nnot True\nFalse\n\n\n\n \n真值表（True Table）举例\n# AND 示例\nprint(True and True)     # True\nprint(True and False)    # False\n\n# OR 示例\nprint(False or True)     # True\nprint(False or False)    # False\n\n# NOT 示例\nprint(not False)         # True\n\n#@title Example Logical Operators\n\nage = 20\nhas_id = True\n\nif age &gt;= 18 and has_id:\n    print(\"Access granted\")\nelse:\n    print(\"Access denied\")\n\nAccess granted\n\n\n\n\n\n函数是可以重复使用的代码块，用于执行特定任务。使用函数可以帮助你更好地组织代码、实现复用、提升可读性和维护性。\n# define function\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# call function\ngreet(\"Leon\")  # Output: 'Hello, Leon!'\n\n#@title Example Function\n\nstudents = {\n    \"John\": {\"age\": 20,\n            \"has_id\": False},\n    \"Cook\": {\"age\": 17,\n            \"has_id\": True},\n    \"Ternus\": {\"age\": 31,\n               \"has_id\": True}\n}\n\n# IMPORTANT: INDENTATION!!!\ndef check_id(students: dict):\n    for student in students.keys():\n        age = students[student].get(\"age\")\n        id = students[student].get(\"has_id\")\n\n        if age &gt;= 18 and id:\n            print(f\"{student}: access granted\")\n        else:\n            print(f\"{student}: no \")\n\ncheck_id(students)\n\nJohn: no \nCook: no \nTernus: access granted\n\n\n小练习\n请编写代码完成以下任务：\n\n创建一个包含从 10 到 40 的整数列表;\n对列表中的每个元素, 如果能被 5 整除，输出 “YAAY”;\n修改原列表，将可整除的元素替换为 “YAAY” 字符串\n封装成函数，适用于任何包含整数或浮点数的列表\n\n\n### Your Exercising Code ###\n\n\n##\n\n\n\n\n继承（Inherence）是面向对象编程中的核心概念之一，允许一个类（称为子类或派生类）继承另一个类（称为父类或基类）的属性和方法。\n通过继承，子类可以重用父类已有的功能，同时添加或修改其行为，避免重复编写代码。\n在 Python 中，通过在类定义的括号中传入父类名称来创建一个子类。如果子类需要自定义初始化过程，可以使用 super() 函数调用父类的 __init__方法，然后添加自己的属性或行为。\n继承有助于保持代码的组织性，减少重复，并实现灵活、可扩展的程序设计。\n\n#@title Example Inheretance\n\nclass GraduateStudent(Student):\n    def __init__(self, name, subject, grade, has_thesis):\n        # Use super() to call the parent class's __init__ method\n        super().__init__(name, subject, grade)\n        self.has_thesis = has_thesis\n\n    def is_doctor(self):\n        # Graduate students need at least 70 to pass\n        return self.grade &gt;= 70 and self.has_thesis\n\nanna = Student(\"Anna\", \"Python\", 67)\nbob = GraduateStudent(\"Bob\", \"AI Ethics\", 72, True)\nchris = GraduateStudent(\"Chris\", \"AI Ethics\", 68, True)\n\nprint(anna.has_passed())   # True (Student needs 51)\nprint(bob.has_passed())    # True (Student needs 51)\nprint(chris.has_passed())  # True (Student needs 51)\n\nprint(bob.is_doctor())    # True (Graduate needs 70 + thesis)\nprint(chris.is_doctor())  # False (68 is not enough)\n\n\n\n\n\n在 Python 中，模块是包含可重用代码的文件 —— 通常包括函数、变量或类 —— 可以在其他 Python 脚本中导入并使用。模块有助于你从逻辑上组织代码，使程序保持简洁和模块化。你可以使用内建模块（如 math、random 或 datetime），也可以创建你自己的模块。\n \n📐数学模块\nimport math\n\nprint(math.sqrt(16))        # 输出：4.0\nprint(math.pi)              # 输出：3.141592653589793\n🔢随机数模块\nimport random\n\nprint(random.randint(1, 1000))    # 输出：1 到 1000 之间的随机整数\nprint(random.choice(['A', 'B', 'C']))  # 从列表中随机选择一个元素\n \n \n\n\n\n\n\n\nnumpy.png.webp\n\n\n在使用大型数据集或进行数值计算时，性能至关重要。Python 中的标准数据容器（如列表）虽然灵活且易于使用，但在进行大量数学运算时会变得低效。\n这时就要引入 NumPy —— 一个强大的库，它引入了一种高性能的数据容器 ndarray（n维数组）。 与 Python 的列表不同，ndarray 是专为数值计算设计的。它提供了更快的计算速度、更低的内存占用，并具备便捷的语法来进行数组操作、线性代数和统计计算。\n数据科学和机器学习领域中的大多数主流库 —— 包括 Pandas、Matplotlib 和 Scikit-learn —— 都是建立在 NumPy 之上的。因此，学习 NumPy 是进入更高级工具和工作流的基础步骤。\n\n\nPython 是动态类型语言（dynamically typed），意味着你不需要声明变量的类型 —— Python 会自动推断。虽然这让 Python 非常易读且灵活，但也带来了额外的开销（Overhead）：列表中的每个元素都是一个完整的 Python 对象，携带着如类型、内存引用等元数据。\n对于小型任务，这种开销可以忽略不计。但在进行大规模数值运算时，这种开销会成为性能瓶颈 —— 导致运算更慢、内存占用更高。\n\nNumPy 通过其 ndarray 解决了这个问题，它是静态类型的数组容器 —— 所有元素都以相同的数据类型（如 float64）连续存储在内存中。这意味着：\n\n\n不需要多余的类型检查；\n每个元素不再单独存储类型元数据（type metadata）；\n访问和运算速度更快（得益于底层的 C 语言实现）。\n\n简而言之：NumPy 用牺牲一部分灵活性换来了巨大的速度与效率提升，使其成为 Python 中进行高性能数值计算的首选工具。\n\nimport numpy as np # np 是 NumPy 的标准别名（alias）\n\n\nnp.array([1, 2, 3])\n\narray([1, 2, 3])\n\n\n\n#@title Difference between python list and an np ndarray\n\nexample_list = list(range(1_000_000))\n\nexample_array = np.arange(1_000_000)\n\n# mutiply each element by 2\n%timeit example_list_multipled = [e * 2 for e in example_list]\n%timeit example_array_multiplied = example_array * 2\n\n65.1 ms ± 13.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n1.2 ms ± 79.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\n\n\n\n数组是一种用于存储和访问数据的结构。 你可以把数组想象成一个空间中的网格，每个“格子”中存储一个数据元素。\n\n一个 ndarray 拥有以下属性： - ndim: 数组的维度数量。 - shape: 每个维度上的元素个数（即数组的形状）。 - size: 数组中所有元素的总数。 - dtype: 数组中所有元素的数据类型。\n\n#@title Example Creating Arrays\n\n\n# NumPy 创建不同类型的数组（arrays）的基本方法\n\n# 从列表创建数组\nmy_list = [1, 2, 3]\nmy_arr = np.array(my_list)\nmy_arr\n\n# 使用 np.arange() 创建范围数组\nmy_arr2 = np.arange(4)                    # similar to python list: my_list = list(range(4))\nmy_arr2                                   # 生成 [0, 1, 2, 3] 的数组（不包括上限 4），类似于 range() 函数\n\n#固定值数组：全为 0\nzeros = np.zeros((2, 5, 5))\nzeros                                     # 创建2组 5x5的数组，所有值均为0，默认数据类型是 float64\n\n#固定值数组：全为 1（指定形状 & 类型）\nones = np.ones((3, 3, 3), dtype=np.int16) # dtype=np.int16 指定数据类型为 16 位整数（可以省略）\nones                                      # 创建一个 3x3x3 的三维数组，所有元素都是 1\n\n#固定值数组：自定义值\narr_custom = np.full((2), 7.645)          # 创建一个 长度为 2 的数组，所有元素都是 7.645\narr_custom\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])\n\n\n\n\n\n\n\n要从数组中选择某个元素，使用方括号并通过索引指定所需元素的位置（从 0 开始计数）\n一维数组的索引方式与 Python 列表类似:\narray[index]\n\n\n\n对于多维数组，要获取某个元素，需要为数组的每一维提供一个索引。\n对于二维数组（matrix），使用一对索引 (row, column) 来访问：\narray[row, column]\n\n\n\n对于维度更高的数组，可以将“列索引”视为最后一维，“行索引”为倒数第二维。\narray[:, :, row, column] # 示例：用于一个 4D 数组\n\n#@title Example Array Indexing\n\narr_2d = np.arange(27).reshape((9,3))  #创建一个数字从0到27，9行三列的矩阵\narr_2d\n\narr_2d[0, 0] # get first row first column value\narr_2d[0, -1] # last column of first row\narr_2d[1, ] # all of second row\n\narr_3d = np.arange(27).reshape((3,3,3))\narr_3d\n\narr_3d[1] # get second element of first dimension (matrix)\narr_3d[1, 1] # get second matrix, second row\narr_3d[:, -1, -1] # get second matrix, second row, third column value\n\n\n\n\n\n要获取多个元素或子数组，我们仍然使用方括号（square brakets）和切片语法：\narray[start:stop:step]\n\nstart 起始索引（默认值为 0）\ntop 终止索引（默认是该维度的长度，不包含该索引）\nstep 步长（默认是 1）\n\n如果将 step 设置为负值，start 和 stop 的默认值会被交换，这是一种反转数组的常见技巧。\n\nNumPy 的切片操作会创建数组的一个视图（view），而不是副本（copy）。这意味着对切片后的数组进行更改，会影响原数组中的对应部分。\n\n\n\n\n\n\n\n\n\n\n操作\n含义\n示例\n说明/备注\n\n\n\n\narray[i, j]\n访问第 i 行第 j 列的元素\na[1, 2]\n用于二维或更高维数组索引\n\n\n::2\n步长为 2，隔一个取一个元素\na[::2]\n用于切片，返回偶数索引的元素\n\n\n[::-1]\n倒序\na[::-1]\n快速反转数组顺序\n\n\n.copy()\n显式复制数组\nb = a.copy()\n防止修改视图影响原数组\n\n\n.reshape()\n改变数组形状，不改变数据内容\na.reshape(2, 3)\n新形状必须与元素数量匹配\n\n\n\n\n#@title Example Slicing Arrays\n\narr_2d = np.arange(27).reshape((3, 9))     # 创建一个从 0 到 26 的一维数组，重塑成 3 行 9 列的二维数组\narr_2d\n\narr_2d[1, :4] # 第 2 行（索引为 1），前 4 个元素\narr_2d[1, ::2] # 第 2 行，每隔一个取一个（步长为 2）\narr_2d[::2, ::2] # 每隔一行 & 每隔一列取一个，起点都是第 0 行/列\n\narr_2d[::-1, ] # 所有行反转（上下颠倒）\n\n# view vs copy\nsecond_row = arr_2d[1]  # 这并不是复制了第 2 行，而是创建了 一个对原数组的“视图”（view）\nsecond_row\n\nsecond_row[:2] = 999 # 注意！把视图中的前两个值改成了 999，原数组也会被修改！\nsecond_row\narr_2d\n\n#⚠️注意⚠️ NumPy 默认用“视图”而不是“复制”，这可以节省内存，但如果你要避免影响原数据，还是需使用 .copy()\n\n\n# 创建三维数组\narr_3d = np.arange(27).reshape((3,3,3)) #创建一个形状为 (3,3,3) 的三维数组（3 个 3x3 的小矩阵）\narr_3d\n\narr_3d[1, 1, ::2] # 取第 2 个“矩阵”（索引为 1）它的第 2 行（索引为 1），这一行中，每隔一列取一个元素（步长为 2）\n\narray([12, 14])\n\n\n\n#@title Exercises Array Manipulation\n\n#### Your Code Here ####\n\n# 1: 所有矩阵、所有行、第3列（索引2）\nthird_col = arr_3d[:, :, 2]\n\n# 2: 第3个矩阵（索引2）、第2行（索引1）、所有列\nsecond_row_third_matrix = arr_3d[2, 1, :]\n\n# 3: 第1个矩阵（索引0）、最后一行（索引2）、中间列（索引1）\nmid_val = arr_3d[0, 2, 1]\n\n#输出结果\nprint(\"1️⃣ Third column of all matrices:\\n\", third_col)\nprint(\"2️⃣ Second row of third matrix:\\n\", second_row_third_matrix)\nprint(\"3️⃣ First matrix, last row, middle column:\\n\", mid_val)\n\n1️⃣ Third column of all matrices:\n [[ 2  5  8]\n [11 14 17]\n [20 23 26]]\n2️⃣ Second row of third matrix:\n [21 22 23]\n3️⃣ First matrix, last row, middle column:\n 7\n\n\n\n\n\n“高级索引”（也称为“花式索引” fancy indexing）指的是使用数组对另一个数组进行索引的所有情况。高级索引总是返回一个副本（copy），而不是原始数据的视图（view）。\n高级索引使你能够快速地访问和修改数组中某些值的子集，特别适用于按条件或特定索引位置提取多个元素。\n\n#@title Example Fancy Indexing\narr_2d = np.arange(15).reshape(5,3)\narr_2d\n\nidx = [1,2]\n\narr_2d[idx] # select indexed rows\narr_2d[:, idx] # select indexed columns\n\nrows = [0, 1]\ncols = [1, 2]\n\narr_2d[rows, cols]\n\narray([1, 5])\n\n\nNumPy 中的布尔索引操作符（Boolean Indexing Operators）\n\n\n\n\n\n\n\n\n\n操作符\n操作类型\n示例\n描述说明\n\n\n\n\n&\n按位与（AND）\n(arr &gt; 5) & (arr &lt; 10)\n元素级的逻辑与操作\n\n\n\\|\n按位或（OR）\n(arr &lt; 3) \\| (arr &gt; 8)\n元素级的逻辑或操作\n\n\n~\n按位非（NOT）\n~(arr &gt; 5)\n反转布尔掩码（mask）\n\n\n!=\n不等于\narr != 0\n筛选不等于某值的元素\n\n\n==\n等于\narr == 10\n筛选等于某值的元素\n\n\n&gt;\n大于\narr &gt; 5\n筛选大于某值的元素\n\n\n&lt;\n小于\narr &lt; 5\n筛选小于某值的元素\n\n\n&gt;=\n大于或等于\narr &gt;= 5\n筛选大于或等于某值的元素\n\n\n&lt;=\n小于或等于\narr &lt;= 5\n筛选小于或等于某值的元素\n\n\n\n\n#@title Example Fancy Idx (Boolean)\nmask = arr_2d &gt; 7\nmask\n# arr_2d[arr_2d &gt; 7]\n\narray([[False, False, False, False, False, False, False, False,  True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n\n\n\n\n数组非常强大，因为它们允许你不使用 for 循环就能进行批量操作。\n💡 通用函数（ufunc）\n通用函数（universal functions）是 NumPy 中的快速函数，可对数组中的数据逐元素执行操作。\n\n向量化（Vectorisation）是 NumPy 中的一种计算技巧，能在不显式写循环的情况下，对整个数组同时应用运算。\n\n\n\n\n\n\n\n\n\n\n函数名\n操作类型\n示例\n描述说明\n\n\n\n\nnp.add()\n加法\narr1 + arr2\n对应元素相加\n\n\nnp.subtract()\n减法\narr1 - arr2\n对应元素相减\n\n\nnp.multiply()\n乘法\narr1 * arr2\n对应元素相乘\n\n\nnp.divide()\n除法\narr1 / arr2\n对应元素相除\n\n\nnp.power()\n幂运算\narr**2\n每个元素的平方（或次方）\n\n\nnp.sqrt()\n开方\nnp.sqrt(arr)\n每个元素开平方\n\n\nnp.exp()\n指数\nnp.exp(arr)\ne 的每个元素次幂\n\n\nnp.abs()\n绝对值\nnp.abs(arr)\n每个元素取绝对值\n\n\n\n \n汇总统计（Summary Statistics）：\n\n\n\n\n\n\n\n\n\n函数名\n操作类型\n示例\n描述说明\n\n\n\n\nnp.mean()\n平均值\narr.mean()\n计算算术平均值\n\n\nnp.median()\n中位数\nnp.median(arr)\n排序后取中间的值\n\n\nnp.std()\n标准差\narr.std()\n数据的离散程度（波动）\n\n\nnp.var()\n方差\narr.var()\n每个元素与平均值的偏差平方的平均\n\n\nnp.sum()\n总和\narr.sum()\n所有元素相加\n\n\nnp.min()\n最小值\narr.min()\n数组中最小的元素\n\n\nnp.max()\n最大值\narr.max()\n数组中最大的元素\n\n\n\n\n📚 想要了解更多？ - NumPy ufuncs basics - NumPy ufuncs\n\n\n#@title Example Operations on Arrays\n\n#1. 创建数组 & 简单加法\narr = np.arange(10)\narr + 5\n\n#2. 反转数组并做对应元素相乘\narr2 = arr[::-1]\narr2\n\narr * arr2 # 将相应元素相乘\n\n#3. 统计方法：平均值 & 标准差\narr.mean() # 均值\n\narr.std() # 标准差（std）：默认是总体标准差（denominator = n）\n# arr.std(ddof=1) # 带贝塞尔修正的std\n\narr = arr.reshape(5,2)\narr\n\n# arr.sum() # 所有元素求和\n# arr.sum(axis = 1) # 沿着第1轴（列）求和，即对“每一行”求和\narr.sum(axis = 0) # 沿着第0轴（行）求和，即对“每一列”求和\n\narray([20, 25])\n\n\n\n\n\nnp_matrix_aggregation_row.png\n\n\nImg source: NumPy basics\n\n\n\n假设你有两个形状不同的数组，但你想对它们执行某种操作（例如相加）。通常你需要将它们“变为相同形状”才能进行逐元素运算。\n\n广播（Broadcasting） 是 NumPy 用来扩展或复制较小数组形状的机制，从而使它们在逐元素操作（element-wise operations）时形状相容。\n\n✅ 两个维度在以下情况下被认为是兼容的： 1. 两个维度相等，或 2. 其中一个维度为 1\n🔄 广播的工作方式：从右向左比较数组的形状维度 （即从最后一个维度开始，逐步向前比较）\n📢 广播的步骤详解：\n\n对齐维度（Align Dims）: 如果两个数组的维度数量不同，NumPy 会在较小数组的形状前补上 1，直到两者维度数一致。\n\na.shape = (3, 4, 5)\nb.shape = (4, 5) → prepend 1 to make (1, 4, 5)\n\n\n检查兼容性（Check Compatibility）: 对于每一个对应维度，满足以下任一条件即可广播： 1. 两个维度值相等 2. 其中一个维度为 1（会自动“扩展”）\n\na.shape = (3, 4, 5)\nb.shape = (1, 4, 5) broadcast to (3, 4, 5)\n\n完成操作（Element-wise Operation）：如果所有维度都满足条件，两个数组就被广播为相同形状，可以进行逐元素操作（如加法、乘法等）。\n\n参考: - NumPy Broadcasting Explained (mCoding) - NumPy Website on Broadcasting\n\na = np.array([1,2,3])\nb = 2\nprint(a.shape)\n\n(3,)\n\n\n\na * b\n\narray([2, 4, 6])\n\n\n\n\n\nbroadcasting_1.png\n\n\nImg source: NumPy broadcasting\n\na = np.array([[0, 0, 0],\n            [10, 10, 10],\n            [20, 20, 20],\n            [30, 30, 30]])\n\nb = np.array([1, 2, 3])\n\nprint(a.shape, b.shape)\n\n(4, 3) (3,)\n\n\n\na + b # shapes: (4, 3) + (1, 3)\n\narray([[ 1,  2,  3],\n       [11, 12, 13],\n       [21, 22, 23],\n       [31, 32, 33]])\n\n\n\n\n\nbroadcasting_2.png\n\n\nImg source: NumPy broadcasting\n\na = np.array([[0, 0, 0],\n            [10, 10, 10],\n            [20, 20, 20],\n            [30, 30, 30]])\n\nb = np.array([1, 2, 3, 4]) # added one more element!\n\nprint(a.shape, b.shape)\n\n(4, 3) (4,)\n\n\n\na + b # shapes: (4, 3) + (1, 4) will give error bc 3 and 4 not match\n# a + b.reshape(-1, 1)\n\n\n\n\nbroadcasting_3.png\n\n\nImg source: NumPy broadcasting\n\na = np.array([[0], [10], [20], [30]])\n\nb = np.array([1, 2, 3])\n\nprint(a.shape, b.shape)\n\n(4, 1) (3,)\n\n\n\n\n\nbroadcasting_4.png\n\n\nImg source: NumPy broadcasting\n\n#@title Exercises NumPy\n\narr_2d = np.arange(27).reshape(3, -1)\nprint(arr_2d)\n\n# 任务 1：对每列进行中心化（减去列的均值）\ncol_means = arr_2d.mean(axis=0)                    # 沿 axis=0（即列）计算均值\ncentered_by_column = arr_2d - col_means            # 对每一列的每个元素减去该列的均值\nprint(\"\\n任务1：\\n\", centered_by_column)\n\n\n# 任务 2：将每一行归一化（总和为 1）\nrow_sums = arr_2d.sum(axis=1, keepdims=True)       # 计算每一行的总和，保持列结构（用于广播）\nnormalized_by_row = arr_2d / row_sums              # 每行除以它自己的和，实现归一化\nprint(\"\\n任务2：\\n\", normalized_by_row)\n\n\n# 任务 3：将每个元素替换为它与数组整体均值的距离\noverall_mean = arr_2d.mean()                       # 计算整个数组的均值\ndistance_from_mean = np.abs(arr_2d - overall_mean) # 使用 np.abs 计算每个元素与整体均值的绝对距离\nprint(\"\\n任务3：\\n\", distance_from_mean)\n\n[[ 0  1  2  3  4  5  6  7  8]\n [ 9 10 11 12 13 14 15 16 17]\n [18 19 20 21 22 23 24 25 26]]\n\n任务1：\n [[-9. -9. -9. -9. -9. -9. -9. -9. -9.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 9.  9.  9.  9.  9.  9.  9.  9.  9.]]\n\n任务2：\n [[0.         0.02777778 0.05555556 0.08333333 0.11111111 0.13888889\n  0.16666667 0.19444444 0.22222222]\n [0.07692308 0.08547009 0.09401709 0.1025641  0.11111111 0.11965812\n  0.12820513 0.13675214 0.14529915]\n [0.09090909 0.0959596  0.1010101  0.10606061 0.11111111 0.11616162\n  0.12121212 0.12626263 0.13131313]]\n\n任务3：\n [[13. 12. 11. 10.  9.  8.  7.  6.  5.]\n [ 4.  3.  2.  1.  0.  1.  2.  3.  4.]\n [ 5.  6.  7.  8.  9. 10. 11. 12. 13.]]"
  },
  {
    "objectID": "简单python入门.html#numpy",
    "href": "简单python入门.html#numpy",
    "title": "Python 🐍",
    "section": "",
    "text": "numpy.png.webp\n\n\n在使用大型数据集或进行数值计算时，性能至关重要。Python 中的标准数据容器（如列表）虽然灵活且易于使用，但在进行大量数学运算时会变得低效。\n这时就要引入 NumPy —— 一个强大的库，它引入了一种高性能的数据容器 ndarray（n维数组）。 与 Python 的列表不同，ndarray 是专为数值计算设计的。它提供了更快的计算速度、更低的内存占用，并具备便捷的语法来进行数组操作、线性代数和统计计算。\n数据科学和机器学习领域中的大多数主流库 —— 包括 Pandas、Matplotlib 和 Scikit-learn —— 都是建立在 NumPy 之上的。因此，学习 NumPy 是进入更高级工具和工作流的基础步骤。\n\n\nPython 是动态类型语言（dynamically typed），意味着你不需要声明变量的类型 —— Python 会自动推断。虽然这让 Python 非常易读且灵活，但也带来了额外的开销（Overhead）：列表中的每个元素都是一个完整的 Python 对象，携带着如类型、内存引用等元数据。\n对于小型任务，这种开销可以忽略不计。但在进行大规模数值运算时，这种开销会成为性能瓶颈 —— 导致运算更慢、内存占用更高。\n\nNumPy 通过其 ndarray 解决了这个问题，它是静态类型的数组容器 —— 所有元素都以相同的数据类型（如 float64）连续存储在内存中。这意味着：\n\n\n不需要多余的类型检查；\n每个元素不再单独存储类型元数据（type metadata）；\n访问和运算速度更快（得益于底层的 C 语言实现）。\n\n简而言之：NumPy 用牺牲一部分灵活性换来了巨大的速度与效率提升，使其成为 Python 中进行高性能数值计算的首选工具。\n\nimport numpy as np # np 是 NumPy 的标准别名（alias）\n\n\nnp.array([1, 2, 3])\n\narray([1, 2, 3])\n\n\n\n#@title Difference between python list and an np ndarray\n\nexample_list = list(range(1_000_000))\n\nexample_array = np.arange(1_000_000)\n\n# mutiply each element by 2\n%timeit example_list_multipled = [e * 2 for e in example_list]\n%timeit example_array_multiplied = example_array * 2\n\n65.1 ms ± 13.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n1.2 ms ± 79.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\n\n\n\n数组是一种用于存储和访问数据的结构。 你可以把数组想象成一个空间中的网格，每个“格子”中存储一个数据元素。\n\n一个 ndarray 拥有以下属性： - ndim: 数组的维度数量。 - shape: 每个维度上的元素个数（即数组的形状）。 - size: 数组中所有元素的总数。 - dtype: 数组中所有元素的数据类型。\n\n#@title Example Creating Arrays\n\n\n# NumPy 创建不同类型的数组（arrays）的基本方法\n\n# 从列表创建数组\nmy_list = [1, 2, 3]\nmy_arr = np.array(my_list)\nmy_arr\n\n# 使用 np.arange() 创建范围数组\nmy_arr2 = np.arange(4)                    # similar to python list: my_list = list(range(4))\nmy_arr2                                   # 生成 [0, 1, 2, 3] 的数组（不包括上限 4），类似于 range() 函数\n\n#固定值数组：全为 0\nzeros = np.zeros((2, 5, 5))\nzeros                                     # 创建2组 5x5的数组，所有值均为0，默认数据类型是 float64\n\n#固定值数组：全为 1（指定形状 & 类型）\nones = np.ones((3, 3, 3), dtype=np.int16) # dtype=np.int16 指定数据类型为 16 位整数（可以省略）\nones                                      # 创建一个 3x3x3 的三维数组，所有元素都是 1\n\n#固定值数组：自定义值\narr_custom = np.full((2), 7.645)          # 创建一个 长度为 2 的数组，所有元素都是 7.645\narr_custom\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])\n\n\n\n\n\n\n\n要从数组中选择某个元素，使用方括号并通过索引指定所需元素的位置（从 0 开始计数）\n一维数组的索引方式与 Python 列表类似:\narray[index]\n\n\n\n对于多维数组，要获取某个元素，需要为数组的每一维提供一个索引。\n对于二维数组（matrix），使用一对索引 (row, column) 来访问：\narray[row, column]\n\n\n\n对于维度更高的数组，可以将“列索引”视为最后一维，“行索引”为倒数第二维。\narray[:, :, row, column] # 示例：用于一个 4D 数组\n\n#@title Example Array Indexing\n\narr_2d = np.arange(27).reshape((9,3))  #创建一个数字从0到27，9行三列的矩阵\narr_2d\n\narr_2d[0, 0] # get first row first column value\narr_2d[0, -1] # last column of first row\narr_2d[1, ] # all of second row\n\narr_3d = np.arange(27).reshape((3,3,3))\narr_3d\n\narr_3d[1] # get second element of first dimension (matrix)\narr_3d[1, 1] # get second matrix, second row\narr_3d[:, -1, -1] # get second matrix, second row, third column value\n\n\n\n\n\n要获取多个元素或子数组，我们仍然使用方括号（square brakets）和切片语法：\narray[start:stop:step]\n\nstart 起始索引（默认值为 0）\ntop 终止索引（默认是该维度的长度，不包含该索引）\nstep 步长（默认是 1）\n\n如果将 step 设置为负值，start 和 stop 的默认值会被交换，这是一种反转数组的常见技巧。\n\nNumPy 的切片操作会创建数组的一个视图（view），而不是副本（copy）。这意味着对切片后的数组进行更改，会影响原数组中的对应部分。\n\n\n\n\n\n\n\n\n\n\n操作\n含义\n示例\n说明/备注\n\n\n\n\narray[i, j]\n访问第 i 行第 j 列的元素\na[1, 2]\n用于二维或更高维数组索引\n\n\n::2\n步长为 2，隔一个取一个元素\na[::2]\n用于切片，返回偶数索引的元素\n\n\n[::-1]\n倒序\na[::-1]\n快速反转数组顺序\n\n\n.copy()\n显式复制数组\nb = a.copy()\n防止修改视图影响原数组\n\n\n.reshape()\n改变数组形状，不改变数据内容\na.reshape(2, 3)\n新形状必须与元素数量匹配\n\n\n\n\n#@title Example Slicing Arrays\n\narr_2d = np.arange(27).reshape((3, 9))     # 创建一个从 0 到 26 的一维数组，重塑成 3 行 9 列的二维数组\narr_2d\n\narr_2d[1, :4] # 第 2 行（索引为 1），前 4 个元素\narr_2d[1, ::2] # 第 2 行，每隔一个取一个（步长为 2）\narr_2d[::2, ::2] # 每隔一行 & 每隔一列取一个，起点都是第 0 行/列\n\narr_2d[::-1, ] # 所有行反转（上下颠倒）\n\n# view vs copy\nsecond_row = arr_2d[1]  # 这并不是复制了第 2 行，而是创建了 一个对原数组的“视图”（view）\nsecond_row\n\nsecond_row[:2] = 999 # 注意！把视图中的前两个值改成了 999，原数组也会被修改！\nsecond_row\narr_2d\n\n#⚠️注意⚠️ NumPy 默认用“视图”而不是“复制”，这可以节省内存，但如果你要避免影响原数据，还是需使用 .copy()\n\n\n# 创建三维数组\narr_3d = np.arange(27).reshape((3,3,3)) #创建一个形状为 (3,3,3) 的三维数组（3 个 3x3 的小矩阵）\narr_3d\n\narr_3d[1, 1, ::2] # 取第 2 个“矩阵”（索引为 1）它的第 2 行（索引为 1），这一行中，每隔一列取一个元素（步长为 2）\n\narray([12, 14])\n\n\n\n#@title Exercises Array Manipulation\n\n#### Your Code Here ####\n\n# 1: 所有矩阵、所有行、第3列（索引2）\nthird_col = arr_3d[:, :, 2]\n\n# 2: 第3个矩阵（索引2）、第2行（索引1）、所有列\nsecond_row_third_matrix = arr_3d[2, 1, :]\n\n# 3: 第1个矩阵（索引0）、最后一行（索引2）、中间列（索引1）\nmid_val = arr_3d[0, 2, 1]\n\n#输出结果\nprint(\"1️⃣ Third column of all matrices:\\n\", third_col)\nprint(\"2️⃣ Second row of third matrix:\\n\", second_row_third_matrix)\nprint(\"3️⃣ First matrix, last row, middle column:\\n\", mid_val)\n\n1️⃣ Third column of all matrices:\n [[ 2  5  8]\n [11 14 17]\n [20 23 26]]\n2️⃣ Second row of third matrix:\n [21 22 23]\n3️⃣ First matrix, last row, middle column:\n 7\n\n\n\n\n\n“高级索引”（也称为“花式索引” fancy indexing）指的是使用数组对另一个数组进行索引的所有情况。高级索引总是返回一个副本（copy），而不是原始数据的视图（view）。\n高级索引使你能够快速地访问和修改数组中某些值的子集，特别适用于按条件或特定索引位置提取多个元素。\n\n#@title Example Fancy Indexing\narr_2d = np.arange(15).reshape(5,3)\narr_2d\n\nidx = [1,2]\n\narr_2d[idx] # select indexed rows\narr_2d[:, idx] # select indexed columns\n\nrows = [0, 1]\ncols = [1, 2]\n\narr_2d[rows, cols]\n\narray([1, 5])\n\n\nNumPy 中的布尔索引操作符（Boolean Indexing Operators）\n\n\n\n\n\n\n\n\n\n操作符\n操作类型\n示例\n描述说明\n\n\n\n\n&\n按位与（AND）\n(arr &gt; 5) & (arr &lt; 10)\n元素级的逻辑与操作\n\n\n\\|\n按位或（OR）\n(arr &lt; 3) \\| (arr &gt; 8)\n元素级的逻辑或操作\n\n\n~\n按位非（NOT）\n~(arr &gt; 5)\n反转布尔掩码（mask）\n\n\n!=\n不等于\narr != 0\n筛选不等于某值的元素\n\n\n==\n等于\narr == 10\n筛选等于某值的元素\n\n\n&gt;\n大于\narr &gt; 5\n筛选大于某值的元素\n\n\n&lt;\n小于\narr &lt; 5\n筛选小于某值的元素\n\n\n&gt;=\n大于或等于\narr &gt;= 5\n筛选大于或等于某值的元素\n\n\n&lt;=\n小于或等于\narr &lt;= 5\n筛选小于或等于某值的元素\n\n\n\n\n#@title Example Fancy Idx (Boolean)\nmask = arr_2d &gt; 7\nmask\n# arr_2d[arr_2d &gt; 7]\n\narray([[False, False, False, False, False, False, False, False,  True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n\n\n\n\n数组非常强大，因为它们允许你不使用 for 循环就能进行批量操作。\n💡 通用函数（ufunc）\n通用函数（universal functions）是 NumPy 中的快速函数，可对数组中的数据逐元素执行操作。\n\n向量化（Vectorisation）是 NumPy 中的一种计算技巧，能在不显式写循环的情况下，对整个数组同时应用运算。\n\n\n\n\n\n\n\n\n\n\n函数名\n操作类型\n示例\n描述说明\n\n\n\n\nnp.add()\n加法\narr1 + arr2\n对应元素相加\n\n\nnp.subtract()\n减法\narr1 - arr2\n对应元素相减\n\n\nnp.multiply()\n乘法\narr1 * arr2\n对应元素相乘\n\n\nnp.divide()\n除法\narr1 / arr2\n对应元素相除\n\n\nnp.power()\n幂运算\narr**2\n每个元素的平方（或次方）\n\n\nnp.sqrt()\n开方\nnp.sqrt(arr)\n每个元素开平方\n\n\nnp.exp()\n指数\nnp.exp(arr)\ne 的每个元素次幂\n\n\nnp.abs()\n绝对值\nnp.abs(arr)\n每个元素取绝对值\n\n\n\n \n汇总统计（Summary Statistics）：\n\n\n\n\n\n\n\n\n\n函数名\n操作类型\n示例\n描述说明\n\n\n\n\nnp.mean()\n平均值\narr.mean()\n计算算术平均值\n\n\nnp.median()\n中位数\nnp.median(arr)\n排序后取中间的值\n\n\nnp.std()\n标准差\narr.std()\n数据的离散程度（波动）\n\n\nnp.var()\n方差\narr.var()\n每个元素与平均值的偏差平方的平均\n\n\nnp.sum()\n总和\narr.sum()\n所有元素相加\n\n\nnp.min()\n最小值\narr.min()\n数组中最小的元素\n\n\nnp.max()\n最大值\narr.max()\n数组中最大的元素\n\n\n\n\n📚 想要了解更多？ - NumPy ufuncs basics - NumPy ufuncs\n\n\n#@title Example Operations on Arrays\n\n#1. 创建数组 & 简单加法\narr = np.arange(10)\narr + 5\n\n#2. 反转数组并做对应元素相乘\narr2 = arr[::-1]\narr2\n\narr * arr2 # 将相应元素相乘\n\n#3. 统计方法：平均值 & 标准差\narr.mean() # 均值\n\narr.std() # 标准差（std）：默认是总体标准差（denominator = n）\n# arr.std(ddof=1) # 带贝塞尔修正的std\n\narr = arr.reshape(5,2)\narr\n\n# arr.sum() # 所有元素求和\n# arr.sum(axis = 1) # 沿着第1轴（列）求和，即对“每一行”求和\narr.sum(axis = 0) # 沿着第0轴（行）求和，即对“每一列”求和\n\narray([20, 25])\n\n\n\n\n\nnp_matrix_aggregation_row.png\n\n\nImg source: NumPy basics\n\n\n\n假设你有两个形状不同的数组，但你想对它们执行某种操作（例如相加）。通常你需要将它们“变为相同形状”才能进行逐元素运算。\n\n广播（Broadcasting） 是 NumPy 用来扩展或复制较小数组形状的机制，从而使它们在逐元素操作（element-wise operations）时形状相容。\n\n✅ 两个维度在以下情况下被认为是兼容的： 1. 两个维度相等，或 2. 其中一个维度为 1\n🔄 广播的工作方式：从右向左比较数组的形状维度 （即从最后一个维度开始，逐步向前比较）\n📢 广播的步骤详解：\n\n对齐维度（Align Dims）: 如果两个数组的维度数量不同，NumPy 会在较小数组的形状前补上 1，直到两者维度数一致。\n\na.shape = (3, 4, 5)\nb.shape = (4, 5) → prepend 1 to make (1, 4, 5)\n\n\n检查兼容性（Check Compatibility）: 对于每一个对应维度，满足以下任一条件即可广播： 1. 两个维度值相等 2. 其中一个维度为 1（会自动“扩展”）\n\na.shape = (3, 4, 5)\nb.shape = (1, 4, 5) broadcast to (3, 4, 5)\n\n完成操作（Element-wise Operation）：如果所有维度都满足条件，两个数组就被广播为相同形状，可以进行逐元素操作（如加法、乘法等）。\n\n参考: - NumPy Broadcasting Explained (mCoding) - NumPy Website on Broadcasting\n\na = np.array([1,2,3])\nb = 2\nprint(a.shape)\n\n(3,)\n\n\n\na * b\n\narray([2, 4, 6])\n\n\n\n\n\nbroadcasting_1.png\n\n\nImg source: NumPy broadcasting\n\na = np.array([[0, 0, 0],\n            [10, 10, 10],\n            [20, 20, 20],\n            [30, 30, 30]])\n\nb = np.array([1, 2, 3])\n\nprint(a.shape, b.shape)\n\n(4, 3) (3,)\n\n\n\na + b # shapes: (4, 3) + (1, 3)\n\narray([[ 1,  2,  3],\n       [11, 12, 13],\n       [21, 22, 23],\n       [31, 32, 33]])\n\n\n\n\n\nbroadcasting_2.png\n\n\nImg source: NumPy broadcasting\n\na = np.array([[0, 0, 0],\n            [10, 10, 10],\n            [20, 20, 20],\n            [30, 30, 30]])\n\nb = np.array([1, 2, 3, 4]) # added one more element!\n\nprint(a.shape, b.shape)\n\n(4, 3) (4,)\n\n\n\na + b # shapes: (4, 3) + (1, 4) will give error bc 3 and 4 not match\n# a + b.reshape(-1, 1)\n\n\n\n\nbroadcasting_3.png\n\n\nImg source: NumPy broadcasting\n\na = np.array([[0], [10], [20], [30]])\n\nb = np.array([1, 2, 3])\n\nprint(a.shape, b.shape)\n\n(4, 1) (3,)\n\n\n\n\n\nbroadcasting_4.png\n\n\nImg source: NumPy broadcasting\n\n#@title Exercises NumPy\n\narr_2d = np.arange(27).reshape(3, -1)\nprint(arr_2d)\n\n# 任务 1：对每列进行中心化（减去列的均值）\ncol_means = arr_2d.mean(axis=0)                    # 沿 axis=0（即列）计算均值\ncentered_by_column = arr_2d - col_means            # 对每一列的每个元素减去该列的均值\nprint(\"\\n任务1：\\n\", centered_by_column)\n\n\n# 任务 2：将每一行归一化（总和为 1）\nrow_sums = arr_2d.sum(axis=1, keepdims=True)       # 计算每一行的总和，保持列结构（用于广播）\nnormalized_by_row = arr_2d / row_sums              # 每行除以它自己的和，实现归一化\nprint(\"\\n任务2：\\n\", normalized_by_row)\n\n\n# 任务 3：将每个元素替换为它与数组整体均值的距离\noverall_mean = arr_2d.mean()                       # 计算整个数组的均值\ndistance_from_mean = np.abs(arr_2d - overall_mean) # 使用 np.abs 计算每个元素与整体均值的绝对距离\nprint(\"\\n任务3：\\n\", distance_from_mean)\n\n[[ 0  1  2  3  4  5  6  7  8]\n [ 9 10 11 12 13 14 15 16 17]\n [18 19 20 21 22 23 24 25 26]]\n\n任务1：\n [[-9. -9. -9. -9. -9. -9. -9. -9. -9.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 9.  9.  9.  9.  9.  9.  9.  9.  9.]]\n\n任务2：\n [[0.         0.02777778 0.05555556 0.08333333 0.11111111 0.13888889\n  0.16666667 0.19444444 0.22222222]\n [0.07692308 0.08547009 0.09401709 0.1025641  0.11111111 0.11965812\n  0.12820513 0.13675214 0.14529915]\n [0.09090909 0.0959596  0.1010101  0.10606061 0.11111111 0.11616162\n  0.12121212 0.12626263 0.13131313]]\n\n任务3：\n [[13. 12. 11. 10.  9.  8.  7.  6.  5.]\n [ 4.  3.  2.  1.  0.  1.  2.  3.  4.]\n [ 5.  6.  7.  8.  9. 10. 11. 12. 13.]]"
  },
  {
    "objectID": "index.html#课程介绍",
    "href": "index.html#课程介绍",
    "title": "量化文本研究与语言模型",
    "section": "课程介绍",
    "text": "课程介绍\n在社会科学以及广义的人文学科中，一些有趣且重要的概念主要（有时甚至是唯一）通过文字形式来呈现的。这是因为社会生活在很大程度上是通过语言来展开的：法律以书面形式制定，演讲以口语表达，历史事件被记录在案，信件和通信被交换等等。长期以来，社会科学家在分析这些社会过程所产生的文本时所投入的时间相对较少，主要有两个原因。首先，针对社会文本展开研究时，研究者通常只能获取数量有限的文献资料，因此定性研究在此类分析中既具吸引力又具可行性。其次，当大规模文献资料可得时，分析这些文本往往成本高昂且技术复杂。 然而，近年来，社会科学研究中可获取的基于文本的数据量迅猛增长，这主要得益于海量文本资源的在线公开。此外，文本数字化程度的不断提高，也促使学者发展出一系列新的方法，用以分析并（我们希望）从中提取有意义的信息。这两大变化——即数字化文本数据的日益丰富以及方法论的快速发展——共同推动了“定量文本分析”（quantitative text analysis）或“文本即数据”（text-as-data）研究领域的蓬勃发展。 本课程试图引导学生从一个新的视角——“文本即数据”——来重新思考文本与社会之间的关系。我们的问题是：如何在不背离文本复杂性与多义性的前提下，使用计算方法从中识别模式、提取结构并追问其背后的意义构造过程。课程将结合政治学、经济学与公共政策中的具体案例，教授学生如何使用“文本即数据”方法对大规模文本进行收集、预处理与分析，同时辅以对方法论假设、技术中立性与意义可测性的哲学反思。学生将被鼓励批判性地思考：我们在使用工具分析文本时，是否在某种程度上简化或扁平化了语言的复杂性？这种简化是必要的操作还是值得警惕的技术偏见？"
  },
  {
    "objectID": "index.html#课程形式",
    "href": "index.html#课程形式",
    "title": "量化文本研究与语言模型",
    "section": "课程形式",
    "text": "课程形式\n课程将分为讲座（Lecture）和研讨会（Seminar）两个部分。 1. 讲座💬 所有主要的课程内容都将在讲座中进行。每一个主题的讲座时间为120分钟（上下各50分钟，并提供20分钟的休息时间），我们建议学生参加每一次讲座。 2. 研讨会💻 研讨会在讲座结束后进行（中间间隔30分钟休息时间），时长为50分钟。这是一个实践性很强的模块，主要的目标是让学生能够使用真实数据来联系我们在讲座中介绍的相关概念和方法。在每个主题中，我们会要求学生完胜一个相应的问题集（问题和解答将会在课程网站上给出），其中涉及用 R 和 Python 编程语言编写代码（详情课程网站）并解释结果。 研讨会的目的是让学生有充足的时间就问题集以及与R 和 Python 编程语言相关的特定问题提问。在研讨会的时间内，学生可以向老师提问；与其他同学讨论问题集；观看老师的简短现场演示。"
  },
  {
    "objectID": "index.html#学习成果",
    "href": "index.html#学习成果",
    "title": "量化文本研究与语言模型",
    "section": "学习成果",
    "text": "学习成果\n在课程结束时，学生应能够： - 理解用于分析大规模文本语料的基础模型与方法； - 对现有的“文本即数据”（Text-as-Data）研究进行批判性评估； - 掌握并能够应用与解释多种用于文本数据的定量分析方法； - 掌握基本的文本数据的可视化技术，能够将分析结果以图形方式有效呈现； - 掌握Python与R语言中与文本分析和语言模型相关的基本编程技能，能够使用这些工具进行文本处理、建模与可视化。"
  },
  {
    "objectID": "index.html#讲师和助教介绍",
    "href": "index.html#讲师和助教介绍",
    "title": "量化文本研究与语言模型",
    "section": "讲师和助教介绍",
    "text": "讲师和助教介绍\n讲师 董寒旭，伦敦大学学院（UCL）政治科学-计算社会科学博士研究生，伦敦大学克拉拉·克莱学者；阿兰·图灵人工智能与数据科学学会博士生研究员（2025～2026）。\n助教 曹智诚，复旦大学哲学学院博士研究生。 王济东，复旦大学哲学学院博士研究生。"
  },
  {
    "objectID": "Seminar/Seminar7.html",
    "href": "Seminar/Seminar7.html",
    "title": "研讨会7: 语言模型C：神经网络、迁移学习与Transformer架构",
    "section": "",
    "text": "研讨会7: 语言模型C：神经网络、迁移学习与Transformer架构\n本次研讨课将向你介绍 transformer 模型，这些模型通过 HuggingFace 的 transformers 库在 Python 中实现。目前，这是使用 transformer 模型最直接的方法——在 R 中并没有很好的替代方案。其关键原因是：HuggingFace 作为一个模型库及其对应的分词器平台，是专为 Python 集成设计的。HuggingFace 提供了一个模型集散中心及配套的函数库，使得获取预训练模型、对其进行微调，以及将模型分享给其他研究者进一步修改变得相对容易。\n所有这些意味着：我们需要在 Python 中进行操作。但不用担心，我们不要求你编写任何原始的 Python 代码。所有用于推理（inference）和模型微调所需的代码都已为你准备好。我们也不要求你在评估中理解这些代码——这些材料只是作为一个初步入门，并为你将来在研究中想使用 transformer 模型时提供一个可回溯的资源。\n使用 Python 的一个关键难点是环境设置。与 R 和 RStudio 不同，Python 没有统一推荐的界面。相反，有许多可选工具，如 Jupyter Notebook、Spyder、VS Code、PyCharm 等，各自适用于不同的工作流。而在本地设置这些环境可能较为繁琐，尤其是由于 Python 中更容易发生包依赖冲突的问题。为了避免这些问题，我们将使用 Colab。\nColab 提供了一个虚拟的、临时的 Python 运行环境，无需任何设置。要在 Colab 中打开研讨会材料，只需点击下方的链接即可：\n研讨会材料"
  },
  {
    "objectID": "Seminar/Seminar4.html",
    "href": "Seminar/Seminar4.html",
    "title": "Seminar4",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n\n Seminar data \n\n\n美国国务院多年来一直定期编写有关世界各地人权做法的报告。在2018年发表的一篇论文中，Benjamin Baozzi 和 Daniel Berliner 对这些报告进行了分析，以确定一系列主题，并描述这些主题如何随时间和空间而变化。\n在今天的研讨会上，我们将分析这个报告（1977-2012 年），通过应用结构主题模型（STMs）来确定整个语料库和每份报告中受关注和审查的基本主题。我们还将评估不同主题在语料库中的流行程度与各国与美国关系的协变量之间的关系。\n\n在本次研讨中所使用的美国国务院人权报告数据集，纯粹是出于其文本结构与主题建模分析的适用性考虑。我无意讨论或评判任何国家或政府的人权状况，也不认为该数据集所反映的内容具有客观全面的代表性。此外，我不认同美国人权报告中所表达的许多观点和立场，尤其是在其可能带有政治偏见和选择性描述的内容方面。此次分析仅作为文本建模方法的教学案例。\n\n\n\n\n在开始研讨会时，先下载/加载以下 R 包：\nlibrary(stm)\nlibrary(tidyverse)\nlibrary(quanteda)\n# 如果无法加载这些库，请先尝试安装。例如：\n# install.packages(\"stm\")  \n\n\n\n下表介绍了该数据集中的一些变量：\n\n\n\n\n\n\n\n变量名\n描述说明\n\n\n\n\ncname\n报告所涉及国家的名称\n\n\nyear\n报告年份\n\n\nreport\n报告文本（文本已完成词干化处理，并去除停用词）\n\n\nalliance\n该国是否与美国有正式军事同盟（1 表示是，0 表示否）\n\n\np_polity2\n国家对应的 polity 民主指数得分\n\n\nlogus_aid_econ\n美国对该国提供的经济援助的对数值（log 形式）\n\n\noecd\n是否为 OECD 成员国（哑变量）\n\n\ncivil_war\n是否存在内战（哑变量）\n\n\n\n一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：\nhuman_rights &lt;- read_csv(\"human_rights_reports.csv\")\n使用 tidyverse 包中的 glimpse() 函数快速查看数据中的变量：\nglimpse(human_rights)\nRows: 4,067\nColumns: 16\n$ cname             &lt;chr&gt; \"Albania\", \"Albania\", \"Albania\", \"Albania\", \"Albania…\n$ year              &lt;dbl&gt; 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988…\n$ cowcode           &lt;dbl&gt; 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 33…\n$ logwdi_gdpc       &lt;dbl&gt; 7.524573, 7.560410, 7.568337, 7.558117, 7.524482, 7.…\n$ p_polity2         &lt;dbl&gt; -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, 1, 1, 5, 5, …\n$ alliance          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ logus_aid_econ    &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000…\n$ civilwar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ oecd              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ logtrade_with_US  &lt;dbl&gt; 3.010621, 2.502255, 3.131137, 2.263844, 2.627563, 2.…\n$ latentmean_Fariss &lt;dbl&gt; -0.915279270, -1.060029900, -1.053791400, -1.0242505…\n$ gd_ptsa           &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 4…\n$ years_to_election &lt;dbl&gt; 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3…\n$ rep_pres          &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0…\n$ pres_chambers     &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0…\n$ report            &lt;chr&gt; \"albania isol balkan nation peopl govern communist r…              \n\n\n\n我们将首先实现结构主题模型的空模型。该模型等同于相关主题模型（Correlated Topic Model）–是我们在讲座中介绍过的 LDA 模型的近亲，不过在该模型中，允许语料库中的主题彼此相关（LDA 假设主题互不相关）。\n你可以使用 stm 软件包中的 stm() 函数来拟合模型。你需要为该函数指定几个不同的参数：\n\n\n\n\n\n\n\n参数名\n描述说明\n\n\n\n\ndocuments\n拟用于拟合 STM 模型的文档特征矩阵（DFM）\n\n\nK\n你希望估计的主题数量\n\n\nprevalence\n一个不包含响应变量的公式，用于指定你希望用于建模文档中主题分布的协变量\n\n\ncontent\n一个不包含响应变量的公式，用于指定你希望用于建模每个主题内容的协变量\n\n\nseed\n一个种子数，用于使结果具有可重复性\n\n\n\n\n从 human_rights 数据中创建一个语料库。然后创建一个 DFM，并做出一些特征选择决策（主题模型可能需要很长时间才能估算出来，因此我建议先对 DFM 进行修剪，使其暂时保持在合理的小范围内）\n\n\n\nReveal Code\n\nhuman_rights_corpus &lt;- human_rights %&gt;%\n  corpus(text_field = \"report\")\n\nhuman_rights_dfm &lt;- human_rights_corpus %&gt;%\n                        tokens() %&gt;%\n                        dfm()\n\nhuman_rights_dfm &lt;- human_rights_dfm %&gt;%\n  dfm_trim(min_docfreq = .1,\n           max_docfreq = .9,\n           docfreq_type = \"prop\")\n\n\n使用 stm 包中的 stm() 函数拟合一个主题模型。你需要选择一个合适的主题数量（K），并注意：此问题不应使用任何协变量；由于 STM 模型运行时间较长（通常需一两分钟），你应保存模型结果，以避免重复运行模型代码。\n\n\n\nReveal Code\n\nstm_out &lt;- stm(documents = human_rights_dfm,\n               K = 15,\n               seed = 12345)\nsave(stm_out, file = \"stm_out.Rdata\")\n\n\n使用 plot() 函数评估每个主题在该语料库中的常见程度。最常见的主题是什么？最不常见的是什么？\n\n\n\nReveal Code\n\nplot(stm_out)\n\n\n\n使用 labelTopics() 函数提取每个主题中最具代表性的词语。对这些主题“标签”进行一些解释。（注意，stm 包为加权词语在估计的主题模型中提供了各种不同的度量方法。对我们来说最相关的两个是 Highest Prob 和 FREX。Highest Prob 简单地报告在每个主题中具有最高概率的词（即直接从参数推断得出）。FREX 是一种结合了频率和排他性的加权方法（当一个词在某一主题中常见但在其他主题中不常见时，其权重会上升））\n\n\n是否存在一个关于性暴力的主题？是否存在一个关于选举操控的主题？\n使用 cloud() 函数为两个最有趣的主题创建词云图。\n\n\n\nReveal Code\n\nlabelTopics(stm_out)\nTopic 1 Top Words:\n     Highest Prob: israel, west, bank, arab, territori, militari, occupi \n     FREX: israel, west, bank, territori, arab, occupi, east \n     Lift: israel, west, strip, arab, occupi, bank, territori \n     Score: israel, arab, west, territori, jewish, bank, east \nTopic 2 Top Words:\n     Highest Prob: presid, code, ministri, minimum, enforc, minist, opposit \n     FREX: code, franc, french, interior, minimum, radio, gendarmeri \n     Lift: franc, gendarmeri, french, leagu, le, apprenticeship, slaveri \n     Score: franc, gendarmeri, code, presid, french, ministri, disabl \nTopic 3 Top Words:\n     Highest Prob: civilian, war, militari, regim, attack, special, execut \n     FREX: war, regim, iraq, southern, revolutionari, insurg, north \n     Lift: iraq, revolutionari, regim, war, casualti, government-control, summarili \n     Score: iraq, insurg, war, regim, revolutionari, civilian, militia \nTopic 4 Top Words:\n     Highest Prob: traffick, victim, child, sexual, violenc, ministri, ngos \n     FREX: roma, sexual, traffick, societ, corrupt, exploit, victim \n     Lift: roma, bisexu, transgend, chat, lesbian, reproduct, gay \n     Score: roma, traffick, ngos, internet, sexual, child, ombudsman \nTopic 5 Top Words:\n     Highest Prob: militari, indigen, judg, ministri, crime, end, presid \n     FREX: indigen, guerrilla, de, kidnap, paramilitari, congress, prosecutor \n     Lift: guerrilla, jose, carlo, inter-american, san, homicid, el \n     Score: guerrilla, indigen, jose, inter-american, carlo, ombudsman, el \nTopic 6 Top Words:\n     Highest Prob: guarante, militari, amnesti, recent, rate, will, peopl \n     FREX: guarante, tion, ment, communist, now, growth, current \n     Lift: vital, ment, tion, non-government, inter, guarante, invas \n     Score: vital, tion, ment, communist, guarante, indian, now \nTopic 7 Top Words:\n     Highest Prob: provinc, sentenc, chines, detain, activist, china, provinci \n     FREX: provinc, chines, china, provinci, dissid, activist, enterpris \n     Lift: china, chines, provinc, dissid, anniversari, provinci, crackdown \n     Score: china, chines, provinc, dissid, provinci, internet, communist \nTopic 8 Top Words:\n     Highest Prob: see, end, soldier, child, journalist, militari, civilian \n     FREX: soldier, rebel, idp, fgm, girl, arm, unlik \n     Lift: rebel, fgm, idp, loot, soldier, unlik, rob \n     Score: rebel, idp, fgm, soldier, see, ngos, ethnic \nTopic 9 Top Words:\n     Highest Prob: south, african, black, end, parliament, africa, white \n     FREX: black, african, south, white, africa, farm, magistr \n     Lift: white, black, africa, african, color, south, customari \n     Score: white, african, south, africa, black, parliament, magistr \nTopic 10 Top Words:\n     Highest Prob: islam, ministri, see, muslim, sentenc, council, sharia \n     FREX: islam, sharia, non-muslim, king, muslim, bahai, christian \n     Lift: bahai, non-muslim, sunni, sharia, moham, ali, islam \n     Score: bahai, islam, sharia, sunni, non-muslim, king, arab \nTopic 11 Top Words:\n     Highest Prob: opposit, presid, militari, detain, minist, leader, detaine \n     FREX: opposit, decre, coup, martial, ralli, ban, emerg \n     Lift: martial, coup, campus, opposit, sedit, decre, lift \n     Score: martial, opposit, coup, presid, decre, militari, presidenti \nTopic 12 Top Words:\n     Highest Prob: refuge, ethnic, tradit, presid, power, peopl, can \n     FREX: king, loan, tradit, role, agricultur, known, citizenship \n     Lift: loan, consensus, king, nonpolit, expatri, monarchi, modern \n     Score: loan, king, ethnic, royal, tradit, dissid, refuge \nTopic 13 Top Words:\n     Highest Prob: district, violenc, child, see, death, end, muslim \n     FREX: district, milit, tribal, custodi, bond, injur, villag \n     Lift: milit, cast, tribal, ordin, epz, tribe, brothel \n     Score: milit, ngos, tribal, insurg, traffick, muslim, child \nTopic 14 Top Words:\n     Highest Prob: feder, asylum, legisl, immigr, parliament, equal, minor \n     FREX: immigr, asylum, feder, applic, equal, racial, european \n     Lift: kingdom, racist, racism, alien, german, treati, immigr \n     Score: kingdom, parliament, feder, immigr, disabl, asylum, seeker \nTopic 15 Top Words:\n     Highest Prob: prosecutor, ethnic, region, presid, media, ministri, parliament \n     FREX: prosecutor, russian, orthodox, registr, regist, soviet, region \n     Lift: russian, russia, orthodox, soviet, jehovah, procur, psychiatr \n     Score: russian, orthodox, soviet, russia, parliament, ethnic, prosecutor \ncloud(stm_out, 4) \n\ncloud(stm_out, 11) \n\n\n\n访问已估计的 STM 对象中的文档级主题比例（使用 stm_out$theta）。这个矩阵有多少行？有多少列？行和列分别表示什么？\n\n\n\nReveal Code\n\ndim(stm_out$theta)\n[1] 4067   15   \n\n这个矩阵有 4067 行, 15 列。行表示文档，列表示主题。矩阵中每个单元格的值表示对应文档 \\(d\\) 被分配给对应主题 \\(k\\) 的比例。\n\n\n例如，我们来看看矩阵的第一行：\n\nstm_out$theta[1,]\n [1] 0.0039514637 0.0042124561 0.1752190939 0.0002901283 0.0046482334\n [6] 0.6530333191 0.0840681810 0.0003160989 0.0012657616 0.0048301620\n[11] 0.0110138327 0.0317336337 0.0016632066 0.0149865858 0.0087678432\n\n我们可以看到，我们收集的第一份文档主要是关于主题 6 的，因为 65% 的文档分配给了该主题。\n\n\n\n选择一个主题，并将其与 human_rights 数据中的 year 变量作图。你可以从图中看出什么？\n\n\n\nReveal Code\n\n# 将感兴趣的主题分配到数据中\n# 我选择了第 4 个主题，你可能选择了其他主题。\nhuman_rights$sexual_violence_topic &lt;- stm_out$theta[,4]\n\n# 绘制主题随年份的变化图\nhuman_rights %&gt;%\n  ggplot(aes(x = year, y = sexual_violence_topic)) +\n  geom_point(alpha = .2) + \n  theme_bw()\n\n\n随着时间的推移，这一主题在不同国家的报告中变得更加突出。\n\n\n\n\n\n\nSTM 的一个关键创新在于它允许我们将任意协变量纳入文本模型中，使我们能够评估主题分布如何随着文档元数据而变化。在这个问题中，请重新拟合一个 STM 模型，并在 prevalence 参数中加入一个协变量。你可以选择任何你认为可能与估计出的主题存在有趣关系的变量。同样的，请记得保存你的模型输出，这样你就不需要多次重新输出估计模型。\n\n\n\nReveal Code\n\nstm_out_prevalence &lt;- stm(documents = human_rights_dfm,\n                          prevalence = ~alliance,\n                          K = 15,\n                          seed = 12345)\nsave(stm_out_prevalence, file = \"stm_out_prevalence.Rdata\")\n\n\n我们希望能够记录下该模型中估计出的主题，以便后续在绘图函数中使用。请根据每个主题中 “frex” 得分最高的词语，创建一个主题标签向量(vector of topic labels)。\n\n\n\nReveal Code\n\n# 提取每个主题中 frex 得分最高的词构成的矩阵\ntopic_labels_matrix &lt;- labelTopics(stm_out_prevalence, n = 7)$frex\n\n# 将每个主题的关键词合并成一个标签字符串\ntopic_labels &lt;- apply(topic_labels_matrix, 1, paste0, collapse = \"_\")\ntopic_labels\n [1] \"israel_west_bank_territori_arab_occupi_east\"                 \n [2] \"code_franc_french_minimum_interior_radio_extrajudici\"        \n [3] \"war_regim_iraq_southern_revolutionari_insurg_north\"          \n [4] \"roma_sexual_traffick_societ_corrupt_exploit_victim\"          \n [5] \"indigen_guerrilla_de_kidnap_paramilitari_congress_prosecutor\"\n [6] \"guarante_tion_ment_communist_now_growth_current\"             \n [7] \"provinc_chines_china_provinci_dissid_activist_enterpris\"     \n [8] \"soldier_rebel_idp_fgm_girl_arm_unlik\"                        \n [9] \"black_african_south_white_africa_farm_magistr\"               \n[10] \"islam_sharia_non-muslim_king_muslim_bahai_christian\"         \n[11] \"opposit_decre_coup_martial_ralli_ban_newspap\"                \n[12] \"king_loan_role_tradit_agricultur_known_citizenship\"          \n[13] \"district_milit_tribal_bond_custodi_injur_villag\"             \n[14] \"immigr_asylum_feder_applic_equal_racial_european\"            \n[15] \"prosecutor_russian_orthodox_registr_regist_soviet_region\"  \n\n请注意，这里的主题与我们在不使用协变量的 STM 中所提取的主题略有不同。这是因为我们在此处估计的是一个略有不同的模型，导致对词的分布也有所不同。这是主题模型的一个核心弱点：模型结果在一定程度上对模型设定敏感，包括主题数量、初始化方式、协变量的引入等因素都可能影响最终的主题结构。\n\n\n\n使用 estimateEffect() 函数估计数据集中一个协变量在主题使用方面的差异。该函数需要三个主要参数：\n\n\n\n\n\n\n\n\n参数\n描述\n\n\n\n\nformula\n回归所用的公式。格式应为 c(1,2,3) ~ covariate_name，左侧的数字表示你希望估计效应的主题编号。\n\n\nstmobj\nstm() 函数生成的模型输出。\n\n\nmetadata\n一个包含协变量的数据框（data.frame）。你可以使用 docvars(my_dfm) 来提取你用于建模的 dfm 中的协变量。\n\n\n\n\n\nReveal Code\n\n# 估计与美国结盟对所有主题的影响\nprevalence_effects &lt;- estimateEffect(\n  formula = c(1:15) ~ alliance,         # 对第 1 到第 15 个主题建模\n  stmobj = stm_out_prevalence,          # 使用带协变量的 STM 模型\n  metadata = docvars(human_rights_dfm)  # 提供文档元数据（用于读取协变量）\n)\n\n\n使用 summary() 函数提取估计的回归系数。对于哪些主题，你发现与所选协变量存在显著关系的证据？\n\n\n\nReveal Code\n\nsummary(prevalence_effects)\nCall:\nestimateEffect(formula = c(1:15) ~ alliance, stmobj = stm_out_prevalence, \n    metadata = docvars(human_rights_dfm))\n\n\nTopic 1:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.018014   0.001535  11.739  &lt; 2e-16 ***\nalliance    -0.010863   0.002554  -4.253 2.16e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 2:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.088464   0.003031  29.182   &lt;2e-16 ***\nalliance    -0.011958   0.005530  -2.162   0.0307 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 3:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.062277   0.002563  24.303  &lt; 2e-16 ***\nalliance    -0.029942   0.004457  -6.719 2.09e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 4:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.113109   0.004461  25.356  &lt; 2e-16 ***\nalliance    0.034314   0.007992   4.293  1.8e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 5:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.012421   0.002691   4.616 4.03e-06 ***\nalliance    0.185554   0.005944  31.218  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 6:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.081515   0.003665  22.243  &lt; 2e-16 ***\nalliance    0.044115   0.006689   6.596 4.78e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 7:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.041081   0.002387  17.210   &lt;2e-16 ***\nalliance    -0.008466   0.003995  -2.119   0.0341 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 8:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.078359   0.003186   24.59   &lt;2e-16 ***\nalliance    -0.059236   0.005231  -11.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 9:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.045399   0.002326  19.516  &lt; 2e-16 ***\nalliance    -0.023675   0.003738  -6.334 2.65e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 10:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.076367   0.003208   23.80   &lt;2e-16 ***\nalliance    -0.056660   0.005144  -11.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 11:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.077145   0.002698  28.595   &lt;2e-16 ***\nalliance    -0.007542   0.004602  -1.639    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 12:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.103965   0.002956   35.17   &lt;2e-16 ***\nalliance    -0.073486   0.004845  -15.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 13:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.049324   0.002583  19.096  &lt; 2e-16 ***\nalliance    -0.016406   0.004569  -3.591 0.000334 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 14:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.071055   0.003512   20.23   &lt;2e-16 ***\nalliance    0.077002   0.006412   12.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 15:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.081439   0.003383  24.070  &lt; 2e-16 ***\nalliance    -0.042845   0.006304  -6.796 1.23e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n大部分都是！\n\n\n\n使用 plot.estimateEffect() 函数绘制你刚才估计出的一些更有趣的差异。该函数可以接受多个不同的参数。你可以参阅该函数的帮助文件（?plot.estimateEffect）\n\n\n\nReveal Code\n\nplot.estimateEffect(prevalence_effects,\n     topics = 4,\n     covariate = \"alliance\",\n     method = \"pointestimate\",\n     main = topic_labels[4])\n\nplot.estimateEffect(prevalence_effects,\n     topics = 14,\n     covariate = \"alliance\",\n     method = \"pointestimate\",\n     main = topic_labels[14])"
  },
  {
    "objectID": "Seminar/Seminar4.html#报告的主题模式",
    "href": "Seminar/Seminar4.html#报告的主题模式",
    "title": "Seminar4",
    "section": "",
    "text": "美国国务院多年来一直定期编写有关世界各地人权做法的报告。在2018年发表的一篇论文中，Benjamin Baozzi 和 Daniel Berliner 对这些报告进行了分析，以确定一系列主题，并描述这些主题如何随时间和空间而变化。\n在今天的研讨会上，我们将分析这个报告（1977-2012 年），通过应用结构主题模型（STMs）来确定整个语料库和每份报告中受关注和审查的基本主题。我们还将评估不同主题在语料库中的流行程度与各国与美国关系的协变量之间的关系。\n\n在本次研讨中所使用的美国国务院人权报告数据集，纯粹是出于其文本结构与主题建模分析的适用性考虑。我无意讨论或评判任何国家或政府的人权状况，也不认为该数据集所反映的内容具有客观全面的代表性。此外，我不认同美国人权报告中所表达的许多观点和立场，尤其是在其可能带有政治偏见和选择性描述的内容方面。此次分析仅作为文本建模方法的教学案例。"
  },
  {
    "objectID": "Seminar/Seminar4.html#packages",
    "href": "Seminar/Seminar4.html#packages",
    "title": "Seminar4",
    "section": "",
    "text": "在开始研讨会时，先下载/加载以下 R 包：\nlibrary(stm)\nlibrary(tidyverse)\nlibrary(quanteda)\n# 如果无法加载这些库，请先尝试安装。例如：\n# install.packages(\"stm\")"
  },
  {
    "objectID": "Seminar/Seminar4.html#数据集",
    "href": "Seminar/Seminar4.html#数据集",
    "title": "Seminar4",
    "section": "",
    "text": "下表介绍了该数据集中的一些变量：\n\n\n\n\n\n\n\n变量名\n描述说明\n\n\n\n\ncname\n报告所涉及国家的名称\n\n\nyear\n报告年份\n\n\nreport\n报告文本（文本已完成词干化处理，并去除停用词）\n\n\nalliance\n该国是否与美国有正式军事同盟（1 表示是，0 表示否）\n\n\np_polity2\n国家对应的 polity 民主指数得分\n\n\nlogus_aid_econ\n美国对该国提供的经济援助的对数值（log 形式）\n\n\noecd\n是否为 OECD 成员国（哑变量）\n\n\ncivil_war\n是否存在内战（哑变量）\n\n\n\n一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：\nhuman_rights &lt;- read_csv(\"human_rights_reports.csv\")\n使用 tidyverse 包中的 glimpse() 函数快速查看数据中的变量：\nglimpse(human_rights)\nRows: 4,067\nColumns: 16\n$ cname             &lt;chr&gt; \"Albania\", \"Albania\", \"Albania\", \"Albania\", \"Albania…\n$ year              &lt;dbl&gt; 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988…\n$ cowcode           &lt;dbl&gt; 339, 339, 339, 339, 339, 339, 339, 339, 339, 339, 33…\n$ logwdi_gdpc       &lt;dbl&gt; 7.524573, 7.560410, 7.568337, 7.558117, 7.524482, 7.…\n$ p_polity2         &lt;dbl&gt; -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, 1, 1, 5, 5, …\n$ alliance          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ logus_aid_econ    &lt;dbl&gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000…\n$ civilwar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ oecd              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ logtrade_with_US  &lt;dbl&gt; 3.010621, 2.502255, 3.131137, 2.263844, 2.627563, 2.…\n$ latentmean_Fariss &lt;dbl&gt; -0.915279270, -1.060029900, -1.053791400, -1.0242505…\n$ gd_ptsa           &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 4…\n$ years_to_election &lt;dbl&gt; 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3, 2, 1, 0, 3…\n$ rep_pres          &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0…\n$ pres_chambers     &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0…\n$ report            &lt;chr&gt; \"albania isol balkan nation peopl govern communist r…"
  },
  {
    "objectID": "Seminar/Seminar4.html#无协变量的-stm",
    "href": "Seminar/Seminar4.html#无协变量的-stm",
    "title": "Seminar4",
    "section": "",
    "text": "我们将首先实现结构主题模型的空模型。该模型等同于相关主题模型（Correlated Topic Model）–是我们在讲座中介绍过的 LDA 模型的近亲，不过在该模型中，允许语料库中的主题彼此相关（LDA 假设主题互不相关）。\n你可以使用 stm 软件包中的 stm() 函数来拟合模型。你需要为该函数指定几个不同的参数：\n\n\n\n\n\n\n\n参数名\n描述说明\n\n\n\n\ndocuments\n拟用于拟合 STM 模型的文档特征矩阵（DFM）\n\n\nK\n你希望估计的主题数量\n\n\nprevalence\n一个不包含响应变量的公式，用于指定你希望用于建模文档中主题分布的协变量\n\n\ncontent\n一个不包含响应变量的公式，用于指定你希望用于建模每个主题内容的协变量\n\n\nseed\n一个种子数，用于使结果具有可重复性\n\n\n\n\n从 human_rights 数据中创建一个语料库。然后创建一个 DFM，并做出一些特征选择决策（主题模型可能需要很长时间才能估算出来，因此我建议先对 DFM 进行修剪，使其暂时保持在合理的小范围内）\n\n\n\nReveal Code\n\nhuman_rights_corpus &lt;- human_rights %&gt;%\n  corpus(text_field = \"report\")\n\nhuman_rights_dfm &lt;- human_rights_corpus %&gt;%\n                        tokens() %&gt;%\n                        dfm()\n\nhuman_rights_dfm &lt;- human_rights_dfm %&gt;%\n  dfm_trim(min_docfreq = .1,\n           max_docfreq = .9,\n           docfreq_type = \"prop\")\n\n\n使用 stm 包中的 stm() 函数拟合一个主题模型。你需要选择一个合适的主题数量（K），并注意：此问题不应使用任何协变量；由于 STM 模型运行时间较长（通常需一两分钟），你应保存模型结果，以避免重复运行模型代码。\n\n\n\nReveal Code\n\nstm_out &lt;- stm(documents = human_rights_dfm,\n               K = 15,\n               seed = 12345)\nsave(stm_out, file = \"stm_out.Rdata\")\n\n\n使用 plot() 函数评估每个主题在该语料库中的常见程度。最常见的主题是什么？最不常见的是什么？\n\n\n\nReveal Code\n\nplot(stm_out)\n\n\n\n使用 labelTopics() 函数提取每个主题中最具代表性的词语。对这些主题“标签”进行一些解释。（注意，stm 包为加权词语在估计的主题模型中提供了各种不同的度量方法。对我们来说最相关的两个是 Highest Prob 和 FREX。Highest Prob 简单地报告在每个主题中具有最高概率的词（即直接从参数推断得出）。FREX 是一种结合了频率和排他性的加权方法（当一个词在某一主题中常见但在其他主题中不常见时，其权重会上升））\n\n\n是否存在一个关于性暴力的主题？是否存在一个关于选举操控的主题？\n使用 cloud() 函数为两个最有趣的主题创建词云图。\n\n\n\nReveal Code\n\nlabelTopics(stm_out)\nTopic 1 Top Words:\n     Highest Prob: israel, west, bank, arab, territori, militari, occupi \n     FREX: israel, west, bank, territori, arab, occupi, east \n     Lift: israel, west, strip, arab, occupi, bank, territori \n     Score: israel, arab, west, territori, jewish, bank, east \nTopic 2 Top Words:\n     Highest Prob: presid, code, ministri, minimum, enforc, minist, opposit \n     FREX: code, franc, french, interior, minimum, radio, gendarmeri \n     Lift: franc, gendarmeri, french, leagu, le, apprenticeship, slaveri \n     Score: franc, gendarmeri, code, presid, french, ministri, disabl \nTopic 3 Top Words:\n     Highest Prob: civilian, war, militari, regim, attack, special, execut \n     FREX: war, regim, iraq, southern, revolutionari, insurg, north \n     Lift: iraq, revolutionari, regim, war, casualti, government-control, summarili \n     Score: iraq, insurg, war, regim, revolutionari, civilian, militia \nTopic 4 Top Words:\n     Highest Prob: traffick, victim, child, sexual, violenc, ministri, ngos \n     FREX: roma, sexual, traffick, societ, corrupt, exploit, victim \n     Lift: roma, bisexu, transgend, chat, lesbian, reproduct, gay \n     Score: roma, traffick, ngos, internet, sexual, child, ombudsman \nTopic 5 Top Words:\n     Highest Prob: militari, indigen, judg, ministri, crime, end, presid \n     FREX: indigen, guerrilla, de, kidnap, paramilitari, congress, prosecutor \n     Lift: guerrilla, jose, carlo, inter-american, san, homicid, el \n     Score: guerrilla, indigen, jose, inter-american, carlo, ombudsman, el \nTopic 6 Top Words:\n     Highest Prob: guarante, militari, amnesti, recent, rate, will, peopl \n     FREX: guarante, tion, ment, communist, now, growth, current \n     Lift: vital, ment, tion, non-government, inter, guarante, invas \n     Score: vital, tion, ment, communist, guarante, indian, now \nTopic 7 Top Words:\n     Highest Prob: provinc, sentenc, chines, detain, activist, china, provinci \n     FREX: provinc, chines, china, provinci, dissid, activist, enterpris \n     Lift: china, chines, provinc, dissid, anniversari, provinci, crackdown \n     Score: china, chines, provinc, dissid, provinci, internet, communist \nTopic 8 Top Words:\n     Highest Prob: see, end, soldier, child, journalist, militari, civilian \n     FREX: soldier, rebel, idp, fgm, girl, arm, unlik \n     Lift: rebel, fgm, idp, loot, soldier, unlik, rob \n     Score: rebel, idp, fgm, soldier, see, ngos, ethnic \nTopic 9 Top Words:\n     Highest Prob: south, african, black, end, parliament, africa, white \n     FREX: black, african, south, white, africa, farm, magistr \n     Lift: white, black, africa, african, color, south, customari \n     Score: white, african, south, africa, black, parliament, magistr \nTopic 10 Top Words:\n     Highest Prob: islam, ministri, see, muslim, sentenc, council, sharia \n     FREX: islam, sharia, non-muslim, king, muslim, bahai, christian \n     Lift: bahai, non-muslim, sunni, sharia, moham, ali, islam \n     Score: bahai, islam, sharia, sunni, non-muslim, king, arab \nTopic 11 Top Words:\n     Highest Prob: opposit, presid, militari, detain, minist, leader, detaine \n     FREX: opposit, decre, coup, martial, ralli, ban, emerg \n     Lift: martial, coup, campus, opposit, sedit, decre, lift \n     Score: martial, opposit, coup, presid, decre, militari, presidenti \nTopic 12 Top Words:\n     Highest Prob: refuge, ethnic, tradit, presid, power, peopl, can \n     FREX: king, loan, tradit, role, agricultur, known, citizenship \n     Lift: loan, consensus, king, nonpolit, expatri, monarchi, modern \n     Score: loan, king, ethnic, royal, tradit, dissid, refuge \nTopic 13 Top Words:\n     Highest Prob: district, violenc, child, see, death, end, muslim \n     FREX: district, milit, tribal, custodi, bond, injur, villag \n     Lift: milit, cast, tribal, ordin, epz, tribe, brothel \n     Score: milit, ngos, tribal, insurg, traffick, muslim, child \nTopic 14 Top Words:\n     Highest Prob: feder, asylum, legisl, immigr, parliament, equal, minor \n     FREX: immigr, asylum, feder, applic, equal, racial, european \n     Lift: kingdom, racist, racism, alien, german, treati, immigr \n     Score: kingdom, parliament, feder, immigr, disabl, asylum, seeker \nTopic 15 Top Words:\n     Highest Prob: prosecutor, ethnic, region, presid, media, ministri, parliament \n     FREX: prosecutor, russian, orthodox, registr, regist, soviet, region \n     Lift: russian, russia, orthodox, soviet, jehovah, procur, psychiatr \n     Score: russian, orthodox, soviet, russia, parliament, ethnic, prosecutor \ncloud(stm_out, 4) \n\ncloud(stm_out, 11) \n\n\n\n访问已估计的 STM 对象中的文档级主题比例（使用 stm_out$theta）。这个矩阵有多少行？有多少列？行和列分别表示什么？\n\n\n\nReveal Code\n\ndim(stm_out$theta)\n[1] 4067   15   \n\n这个矩阵有 4067 行, 15 列。行表示文档，列表示主题。矩阵中每个单元格的值表示对应文档 \\(d\\) 被分配给对应主题 \\(k\\) 的比例。\n\n\n例如，我们来看看矩阵的第一行：\n\nstm_out$theta[1,]\n [1] 0.0039514637 0.0042124561 0.1752190939 0.0002901283 0.0046482334\n [6] 0.6530333191 0.0840681810 0.0003160989 0.0012657616 0.0048301620\n[11] 0.0110138327 0.0317336337 0.0016632066 0.0149865858 0.0087678432\n\n我们可以看到，我们收集的第一份文档主要是关于主题 6 的，因为 65% 的文档分配给了该主题。\n\n\n\n选择一个主题，并将其与 human_rights 数据中的 year 变量作图。你可以从图中看出什么？\n\n\n\nReveal Code\n\n# 将感兴趣的主题分配到数据中\n# 我选择了第 4 个主题，你可能选择了其他主题。\nhuman_rights$sexual_violence_topic &lt;- stm_out$theta[,4]\n\n# 绘制主题随年份的变化图\nhuman_rights %&gt;%\n  ggplot(aes(x = year, y = sexual_violence_topic)) +\n  geom_point(alpha = .2) + \n  theme_bw()\n\n\n随着时间的推移，这一主题在不同国家的报告中变得更加突出。"
  },
  {
    "objectID": "Seminar/Seminar4.html#有协变量的-stm",
    "href": "Seminar/Seminar4.html#有协变量的-stm",
    "title": "Seminar4",
    "section": "",
    "text": "STM 的一个关键创新在于它允许我们将任意协变量纳入文本模型中，使我们能够评估主题分布如何随着文档元数据而变化。在这个问题中，请重新拟合一个 STM 模型，并在 prevalence 参数中加入一个协变量。你可以选择任何你认为可能与估计出的主题存在有趣关系的变量。同样的，请记得保存你的模型输出，这样你就不需要多次重新输出估计模型。\n\n\n\nReveal Code\n\nstm_out_prevalence &lt;- stm(documents = human_rights_dfm,\n                          prevalence = ~alliance,\n                          K = 15,\n                          seed = 12345)\nsave(stm_out_prevalence, file = \"stm_out_prevalence.Rdata\")\n\n\n我们希望能够记录下该模型中估计出的主题，以便后续在绘图函数中使用。请根据每个主题中 “frex” 得分最高的词语，创建一个主题标签向量(vector of topic labels)。\n\n\n\nReveal Code\n\n# 提取每个主题中 frex 得分最高的词构成的矩阵\ntopic_labels_matrix &lt;- labelTopics(stm_out_prevalence, n = 7)$frex\n\n# 将每个主题的关键词合并成一个标签字符串\ntopic_labels &lt;- apply(topic_labels_matrix, 1, paste0, collapse = \"_\")\ntopic_labels\n [1] \"israel_west_bank_territori_arab_occupi_east\"                 \n [2] \"code_franc_french_minimum_interior_radio_extrajudici\"        \n [3] \"war_regim_iraq_southern_revolutionari_insurg_north\"          \n [4] \"roma_sexual_traffick_societ_corrupt_exploit_victim\"          \n [5] \"indigen_guerrilla_de_kidnap_paramilitari_congress_prosecutor\"\n [6] \"guarante_tion_ment_communist_now_growth_current\"             \n [7] \"provinc_chines_china_provinci_dissid_activist_enterpris\"     \n [8] \"soldier_rebel_idp_fgm_girl_arm_unlik\"                        \n [9] \"black_african_south_white_africa_farm_magistr\"               \n[10] \"islam_sharia_non-muslim_king_muslim_bahai_christian\"         \n[11] \"opposit_decre_coup_martial_ralli_ban_newspap\"                \n[12] \"king_loan_role_tradit_agricultur_known_citizenship\"          \n[13] \"district_milit_tribal_bond_custodi_injur_villag\"             \n[14] \"immigr_asylum_feder_applic_equal_racial_european\"            \n[15] \"prosecutor_russian_orthodox_registr_regist_soviet_region\"  \n\n请注意，这里的主题与我们在不使用协变量的 STM 中所提取的主题略有不同。这是因为我们在此处估计的是一个略有不同的模型，导致对词的分布也有所不同。这是主题模型的一个核心弱点：模型结果在一定程度上对模型设定敏感，包括主题数量、初始化方式、协变量的引入等因素都可能影响最终的主题结构。\n\n\n\n使用 estimateEffect() 函数估计数据集中一个协变量在主题使用方面的差异。该函数需要三个主要参数：\n\n\n\n\n\n\n\n\n参数\n描述\n\n\n\n\nformula\n回归所用的公式。格式应为 c(1,2,3) ~ covariate_name，左侧的数字表示你希望估计效应的主题编号。\n\n\nstmobj\nstm() 函数生成的模型输出。\n\n\nmetadata\n一个包含协变量的数据框（data.frame）。你可以使用 docvars(my_dfm) 来提取你用于建模的 dfm 中的协变量。\n\n\n\n\n\nReveal Code\n\n# 估计与美国结盟对所有主题的影响\nprevalence_effects &lt;- estimateEffect(\n  formula = c(1:15) ~ alliance,         # 对第 1 到第 15 个主题建模\n  stmobj = stm_out_prevalence,          # 使用带协变量的 STM 模型\n  metadata = docvars(human_rights_dfm)  # 提供文档元数据（用于读取协变量）\n)\n\n\n使用 summary() 函数提取估计的回归系数。对于哪些主题，你发现与所选协变量存在显著关系的证据？\n\n\n\nReveal Code\n\nsummary(prevalence_effects)\nCall:\nestimateEffect(formula = c(1:15) ~ alliance, stmobj = stm_out_prevalence, \n    metadata = docvars(human_rights_dfm))\n\n\nTopic 1:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.018014   0.001535  11.739  &lt; 2e-16 ***\nalliance    -0.010863   0.002554  -4.253 2.16e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 2:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.088464   0.003031  29.182   &lt;2e-16 ***\nalliance    -0.011958   0.005530  -2.162   0.0307 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 3:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.062277   0.002563  24.303  &lt; 2e-16 ***\nalliance    -0.029942   0.004457  -6.719 2.09e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 4:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.113109   0.004461  25.356  &lt; 2e-16 ***\nalliance    0.034314   0.007992   4.293  1.8e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 5:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.012421   0.002691   4.616 4.03e-06 ***\nalliance    0.185554   0.005944  31.218  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 6:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.081515   0.003665  22.243  &lt; 2e-16 ***\nalliance    0.044115   0.006689   6.596 4.78e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 7:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.041081   0.002387  17.210   &lt;2e-16 ***\nalliance    -0.008466   0.003995  -2.119   0.0341 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 8:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.078359   0.003186   24.59   &lt;2e-16 ***\nalliance    -0.059236   0.005231  -11.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 9:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.045399   0.002326  19.516  &lt; 2e-16 ***\nalliance    -0.023675   0.003738  -6.334 2.65e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 10:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.076367   0.003208   23.80   &lt;2e-16 ***\nalliance    -0.056660   0.005144  -11.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 11:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.077145   0.002698  28.595   &lt;2e-16 ***\nalliance    -0.007542   0.004602  -1.639    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 12:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.103965   0.002956   35.17   &lt;2e-16 ***\nalliance    -0.073486   0.004845  -15.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 13:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.049324   0.002583  19.096  &lt; 2e-16 ***\nalliance    -0.016406   0.004569  -3.591 0.000334 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 14:\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.071055   0.003512   20.23   &lt;2e-16 ***\nalliance    0.077002   0.006412   12.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTopic 15:\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.081439   0.003383  24.070  &lt; 2e-16 ***\nalliance    -0.042845   0.006304  -6.796 1.23e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n大部分都是！\n\n\n\n使用 plot.estimateEffect() 函数绘制你刚才估计出的一些更有趣的差异。该函数可以接受多个不同的参数。你可以参阅该函数的帮助文件（?plot.estimateEffect）\n\n\n\nReveal Code\n\nplot.estimateEffect(prevalence_effects,\n     topics = 4,\n     covariate = \"alliance\",\n     method = \"pointestimate\",\n     main = topic_labels[4])\n\nplot.estimateEffect(prevalence_effects,\n     topics = 14,\n     covariate = \"alliance\",\n     method = \"pointestimate\",\n     main = topic_labels[14])"
  },
  {
    "objectID": "Seminar/Seminar1.html",
    "href": "Seminar/Seminar1.html",
    "title": "Seminar1",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n\n Seminar data 1 \n Seminar data 2 \n\n\n\n本次研讨课旨在引导学生实际操作 quanteda 套件及若干相关 R 语言程序包。课程重点包括：探索该套件的基本功能，将文本导入并转换为语料库（corpus）对象格式，学习如何将文本转化为文档-特征矩阵（document-feature matrices, DFM），在此基础上开展描述性分析，并尝试构建与验证字典工具（dictionaries）以支持文本分析研究。\n对于R语言，请参考我们day1讨论的 R 语言入门（链接施工中…）。\n\n\n\n“道德基础理论”（Moral Foundations Theory）是一种社会心理学理论，认为人们的道德判断基于五种彼此独立的道德价值。这五种基础（如下表所列）分别是：关怀/伤害（care/harm）、公正/欺骗（fairness/cheating）、忠诚/背叛（loyalty/betrayal）、权威/颠覆（authority/subversion）以及神圣/堕落（sanctity/degradation）。\n根据该理论，人们在面对不同的议题和情境时，会依据这些道德基础进行判断。此外，该理论指出，不同个体对于这五种价值的重视程度存在差异，而这种差异有助于解释人们在道德判断上的文化性和个体性差异。\n下表提供了这五项道德基础的简要概述：\n\n\n\n\n\n\n\n基础\n介绍\n\n\n\n\nCare\nConcern for caring behaviour, kindness, compassion\n\n\nFairness\nConcern for fairness, justice, trustworthiness\n\n\nAuthority\nConcern for obedience and deference to figures of authority (religious, state, etc)\n\n\nLoyalty\nConcern for loyalty, patriotism, self-sacrifice\n\n\nSanctity\nConcern for temperance, chastity, piety, cleanliness\n\n\n\n我们能否从文本中识别出道德基础的使用？在政治论辩中，道德框架与修辞策略发挥着重要作用。而根深蒂固的道德分歧常被认为是政治极化，尤其是在网络环境中加剧的重要根源。在我们探讨诸如“不同政治立场的群体在道德语言使用上是否存在显著差异”（案例）或“道德论证是否有助于缓解政治极化”（案例）等关键研究问题之前，前提是我们必须能够在大规模文本中对道德语言的使用进行有效测量。\n因此，在本次研讨课中，我们将采用一种简化的字典分析方法，以衡量一组在线文本中所体现的道德语言使用程度。该方法将依据“道德基础理论”中所界定的不同类型道德价值，评估文本内容与相应道德框架之间的契合程度。\n\n\n\n在本节研讨会中，我们会使用两个数据集：\n1. Moral Foundations Dictionary – mft_dictionary.csv\n\n该数据集收录了一系列被认为能指示文本中不同道德关切的词汇列表。该字典最初由 Jesse Graham 与 Jonathan Haidt 开发，并在其相关论文中有更为详尽的介绍。\n字典共划分为五类道德关切维度：权威（authority）、忠诚（loyalty）、神圣（sanctity）、公正（fairness）与关怀（care），每一类均对应一组特定的代表性词汇，用于识别该类道德语义在文本中的体现。\n\n\nMoral Foundations Reddit Corpus – mft_texts.csv\n\n\n本文件包含 17,886 条英文 Reddit 评论，这些评论由 11 个不同的子版块（subreddits）中精选而来。评论内容涵盖多种不同主题。此外，该数据集还包含由专业标注员手工完成的注释，标注依据为“道德基础理论”中定义的各类道德关切维度。\n\n在下载并妥善保存这些数据文件后，我们可以通过以下R语言指令将其载入工作环境：\nmft_dictionary_words &lt;- read_csv(\"mft_dictionary.csv\")\nmft_texts &lt;- read_csv(\"mft_texts.csv\")\n\n\n\n在开始seminar时，我们需要安装并加载以下R语言程序包。\n请运行以下代码行以完成这些程序包的安装。请注意，程序包仅需在本机安装一次；安装完成后，就可以删除这些安装指令：\ninstall.packages(\"tidyverse\") # Set of packages helpful for data manipulation\ninstall.packages(\"quanteda\") # Main quanteda package\ninstall.packages(\"quanteda.textplots\") # Package with helpful plotting functions\ninstall.packages(\"quanteda.textstats\") # Package with helpful statistical functions\ninstall.packages(\"remotes\") # Package for installing other packages\nremotes::install_github(\"kbenoit/quanteda.dictionaries\")\n在完成上述程序包的安装后，我们还需要加载这些程序包，以便在R Studio中使用其所包含的各类函数。\n每当我们希望使用这些程序包的函数时，都需要运行以下代码行来加载它们（此操作在每次新的 R 会话中都需重复执行）：\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\nlibrary(quanteda.dictionaries)\n\n\n\n\n\n用R编程往往是一个令人沮丧的经历。幸运的是，当我们遇到困难时，有很多方法可以寻求帮助。\n\n官方网站 http://quanteda.io 提供了详尽的文档与使用说明。\n你可以通过输入 ?function_name 的形式，查阅任意函数的帮助文档。\n此外，还可以使用 example() 函数来运行某一函数的示例代码，以观察其具体的用法与效果。\n\n例如，若要查阅 corpus() 函数的帮助文档，可使用以下代码：\n?corpus\n\n\n\n在 quanteda 中，corpus 对象是我们开展一切文本分析工作的基础。因此，在将文本数据加载至 R 环境后，首要步骤是使用 corpus() 函数将其转换为语料库格式。\n\n创建语料库最简单的方式，是直接使用 R 全局环境中已有的一组文本对象。在本例中，我们已加载 mft_texts.csv 文件，并将其存储为 mft_texts 对象。我们可以先查看这个对象的内容，了解其结构。请使用 head() 函数作用于 mft_texts 对象，并记录其输出结果。思考：哪个变量字段包含了 Reddit 评论的正文文本？\n\nhead(mft_texts)\n\n\nClick to show output\n\n# A tibble: 6 × 9\n  ...1 text              subreddit  care authority loyalty sanctity fairness non_mor\n                                         \n1 1     Alternati…       worldnews  FALSE FALSE     FALSE   FALSE    FALSE    TRUE\n2 2     Dr. Rober…       politics   FALSE TRUE      FALSE   FALSE    TRUE     TRUE\n3 3     If you pr…       worldnews  FALSE FALSE     FALSE   FALSE    FALSE    TRUE\n4 4     Ben Judah…       geopolit… FALSE TRUE      FALSE   FALSE    TRUE     FALSE\n5 5     Ergo, he…        neoliber… FALSE FALSE     TRUE    FALSE    FALSE    TRUE\n6 6     He looks …        nostalgia FALSE FALSE     FALSE   FALSE    FALSE    TRUE\n\n\n\n\n输出结果显示，该对象是一个 “tibble” —— 这是一种特殊类型的 data.frame 数据结构。我们可以查看该数据框前六行的内容。其中，名为 text 的列包含了 Reddit 评论的正文文本。\n\n\n请使用 corpus() 函数对这组文本数据进行处理，以创建一个新的语料库对象。corpus() 函数的第一个参数应为 mft_texts 对象；同时，还需设置 text_field 为 \"text\"，以便 quanteda 知道我们感兴趣的文本内容存储在哪一个变量列中。\n\nmft_corpus &lt;- corpus(mft_texts, text_field = \"text\")\n\n3. 在成功构建语料库对象之后，请使用 summary() 函数对该语料库进行简要描述，以查看其基本信息和结构摘要。思考：在这些 subreddit 名称中，你觉得哪个最有趣、最具创意或最令人发笑？\nsummary(mft_corpus)\n\n\nClick to show output\n\nCorpus consisting of 17886 documents, showing 100 documents:\n\n    Text Types Tokens Sentences ...1           subreddit  care authority\n   text1    67    101         7    1           worldnews FALSE     FALSE\n   text2    59     72         2    2            politics FALSE      TRUE\n   text3    81    177         3    3           worldnews FALSE     FALSE\n   text4    65     82         5    4         geopolitics FALSE      TRUE\n   text5    22     22         2    5          neoliberal FALSE     FALSE\n   text6    21     28         2    6           nostalgia FALSE     FALSE\n   text7    22     24         3    7           worldnews  TRUE     FALSE\n   text8    41     44         3    8 relationship_advice  TRUE     FALSE\n   text9    20     21         1    9       AmItheAsshole FALSE      TRUE\n  text10    12     12         2   10            antiwork FALSE     FALSE\n  text11    35     43         3   11              europe FALSE     FALSE\n  text12    35     40         2   12          confession FALSE     FALSE\n  text13    38     43         2   13 relationship_advice  TRUE     FALSE\n  text14    46     62         2   14           worldnews  TRUE     FALSE\n  text15    57     88         4   15            antiwork FALSE     FALSE\n  text16    16     17         1   16           worldnews FALSE     FALSE\n  text17    21     22         3   17       AmItheAsshole  TRUE     FALSE\n  text18    20     21         3   18          neoliberal FALSE      TRUE\n  text19    22     24         1   19            politics FALSE      TRUE\n  text20    19     21         3   20          confession FALSE     FALSE\n  text21    48     58         1   21            antiwork  TRUE     FALSE\n  text22    78    115         4   22            politics FALSE     FALSE\n  text23    12     13         1   23        Conservative FALSE     FALSE\n  text24    38     66         1   24            antiwork FALSE     FALSE\n  text25    18     26         2   25          confession FALSE     FALSE\n  text26    22     32         1   26            politics FALSE     FALSE\n  text27    70    103         7   27       AmItheAsshole  TRUE     FALSE\n  text28    66     92         5   28           worldnews FALSE     FALSE\n  text29    43     57         3   29          confession FALSE     FALSE\n  text30    35     42         3   30       AmItheAsshole FALSE     FALSE\n  text31    44     52         3   31           worldnews  TRUE     FALSE\n  text32    31     39         4   32            antiwork FALSE     FALSE\n  text33    20     24         2   33            antiwork  TRUE     FALSE\n  text34    33     39         2   34            antiwork FALSE      TRUE\n  text35    19     24         1   35          confession FALSE     FALSE\n  text36    11     14         1   36        Conservative FALSE     FALSE\n  text37    21     24         2   37           worldnews FALSE     FALSE\n  text38    73    104         6   38 relationship_advice  TRUE      TRUE\n  text39    32     41         3   39            antiwork FALSE     FALSE\n  text40    73     95         2   40          neoliberal FALSE     FALSE\n  text41    27     29         2   41        Conservative FALSE     FALSE\n  text42    11     13         1   42            politics FALSE     FALSE\n  text43    37     48         2   43              europe FALSE     FALSE\n  text44    66    101         6   44           worldnews FALSE     FALSE\n  text45    11     13         1   45            antiwork FALSE     FALSE\n  text46    50     66         1   46            politics FALSE     FALSE\n  text47    41     49         3   47              europe FALSE      TRUE\n  text48    13     16         1   48            politics FALSE      TRUE\n  text49    41     52         2   49        Conservative FALSE     FALSE\n  text50    39     49         3   50              europe FALSE     FALSE\n  text51    14     15         1   51 relationship_advice  TRUE     FALSE\n  text52    73    110         3   52           nostalgia FALSE     FALSE\n  text53    34     39         3   53          confession FALSE     FALSE\n  text54    21     22         1   54        Conservative FALSE     FALSE\n  text55    30     33         2   55              europe FALSE      TRUE\n  text56    20     21         2   56            politics FALSE     FALSE\n  text57    13     15         1   57              europe  TRUE     FALSE\n  text58    30     42         5   58           worldnews  TRUE      TRUE\n  text59    34     40         2   59          neoliberal FALSE     FALSE\n  text60    30     38         4   60          confession FALSE     FALSE\n  text61    40     44         3   61           nostalgia FALSE     FALSE\n  text62    20     25         1   62        Conservative FALSE     FALSE\n  text63    33     38         2   63           worldnews  TRUE     FALSE\n  text64    21     31         4   64        Conservative FALSE     FALSE\n  text65    28     35         1   65          neoliberal  TRUE      TRUE\n  text66    13     14         2   66            antiwork FALSE     FALSE\n  text67    11     12         1   67            antiwork  TRUE     FALSE\n  text68    14     17         2   68          neoliberal FALSE     FALSE\n  text69    18     19         1   69            politics FALSE     FALSE\n  text70    15     20         1   70          neoliberal FALSE     FALSE\n  text71    40     50         1   71           worldnews FALSE     FALSE\n  text72    35     45         1   72           nostalgia FALSE     FALSE\n  text73    32     43         2   73       AmItheAsshole  TRUE     FALSE\n  text74    32     36         3   74              europe FALSE      TRUE\n  text75    38     54         5   75 relationship_advice  TRUE     FALSE\n  text76    15     18         1   76        Conservative FALSE      TRUE\n  text77    24     25         1   77 relationship_advice  TRUE     FALSE\n  text78    20     23         1   78        Conservative FALSE     FALSE\n  text79    22     24         3   79            antiwork FALSE     FALSE\n  text80    17     21         1   80           worldnews FALSE     FALSE\n  text81    36     47         4   81 relationship_advice  TRUE     FALSE\n  text82    49     68         1   82            antiwork FALSE     FALSE\n  text83    67     82         2   83          confession FALSE     FALSE\n  text84    26     32         1   84           worldnews FALSE     FALSE\n  text85    25     35         2   85            antiwork FALSE     FALSE\n  text86    14     16         1   86            antiwork  TRUE     FALSE\n  text87    27     28         1   87       AmItheAsshole  TRUE      TRUE\n  text88    18     20         1   88       AmItheAsshole FALSE     FALSE\n  text89    33     42         3   89           worldnews FALSE      TRUE\n  text90    16     20         1   90          neoliberal FALSE     FALSE\n  text91    42     55         3   91            antiwork FALSE     FALSE\n  text92    26     30         3   92       AmItheAsshole  TRUE     FALSE\n  text93    34     47         2   93        Conservative FALSE     FALSE\n  text94    22     28         3   94 relationship_advice  TRUE     FALSE\n  text95    53     71         5   95          neoliberal FALSE     FALSE\n  text96    38     55         3   96        Conservative FALSE     FALSE\n  text97    19     21         2   97           worldnews FALSE      TRUE\n  text98    32     34         2   98              europe FALSE     FALSE\n  text99    13     15         1   99           worldnews FALSE     FALSE\n text100    19     30         2  100           worldnews FALSE     FALSE\n loyalty sanctity fairness non_moral\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE    FALSE     FALSE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE     FALSE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE     TRUE     TRUE     FALSE\n    TRUE    FALSE    FALSE     FALSE\n   FALSE     TRUE     TRUE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE    FALSE      TRUE\n    TRUE     TRUE     TRUE     FALSE\n   FALSE    FALSE     TRUE      TRUE\n    TRUE    FALSE    FALSE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n\n\n\n4. 请注意，尽管我们在构建语料库时指定了 text_field = \"text\"，但我们并没有移除与文本相关的元数据。为了访问这些其他变量，我们可以将 docvars() 函数应用于我们上面创建的语料库对象:\nhead(docvars(mft_corpus))\n...1   subreddit  care authority loyalty sanctity fairness non_moral\n1    1   worldnews FALSE     FALSE   FALSE    FALSE    FALSE      TRUE\n2    2    politics FALSE      TRUE   FALSE    FALSE    FALSE      TRUE\n3    3   worldnews FALSE     FALSE   FALSE    FALSE    FALSE      TRUE\n4    4 geopolitics FALSE      TRUE   FALSE    FALSE     TRUE     FALSE\n5    5  neoliberal FALSE     FALSE    TRUE    FALSE    FALSE      TRUE\n6    6   nostalgia FALSE     FALSE   FALSE    FALSE    FALSE      TRUE\n\n\n\n\n为了统计词频，我们首先需要通过一个称为“分词（tokenization）”的过程，将文本拆分为词语（或更长的短语）。请查阅 quanteda 中关于 tokens() 函数的文档。\n\n对我们之前创建的语料库对象使用 tokens 命令，并查看其结果:\n\nmft_tokens &lt;- tokens(mft_corpus)\nhead(mft_tokens)\n\n\nClick to show output\n\nTokens consisting of 6 documents and 8 docvars.\ntext1 :\n [1] \"Alternative\" \"Fact\"        \":\"           \"They\"        \"argued\"     \n [6] \"a\"           \"crowd\"       \"movement\"    \"around\"      \"Ms\"         \n[11] \"Le\"          \"Pen\"        \n[ ... and 89 more ]\n\ntext2 :\n [1] \"Dr\"            \".\"             \"Robert\"        \"Jay\"          \n [5] \"Lifton\"        \",\"             \"distinguished\" \"professor\"    \n [9] \"emeritus\"      \"at\"            \"John\"          \"Jay\"          \n[ ... and 60 more ]\n\ntext3 :\n [1] \"If\"      \"you\"     \"prefer\"  \"not\"     \"to\"      \"click\"   \"on\"     \n [8] \"Daily\"   \"Mail\"    \"sources\" \",\"       \"then\"   \n[ ... and 165 more ]\n\ntext4 :\n [1] \"Ben\"      \"Judah\"    \"details\"  \"Emmanuel\" \"Macron's\" \"nascent\" \n [7] \"foreign\"  \"policy\"   \"doctrine\" \".\"        \"Noting\"   \"both\"    \n[ ... and 70 more ]\n\ntext5 :\n [1] \"Ergo\"     \",\"        \"he\"       \"supports\" \"Macron\"   \"but\"     \n [7] \"doesn't\"  \"want\"     \"to\"       \"say\"      \"it\"       \"out\"     \n[ ... and 10 more ]\n\ntext6 :\n [1] \"He\"      \"looks\"   \"exactly\" \"the\"     \"same\"    \"in\"      \"Richie\" \n [8] \"Rich\"    \"as\"      \"he\"      \"does\"    \"as\"     \n[ ... and 16 more ]\n\n\n\n接下来，请尝试使用 tokens() 函数的一些参数，例如 remove_punct 和 remove_numbers:\n\nmft_tokens &lt;- tokens(mft_corpus, remove_punct = TRUE, remove_numbers = TRUE)\nhead(mft_tokens)\n\n\nClick to show output\n\nTokens consisting of 6 documents and 8 docvars.\ntext1 :\n [1] \"Alternative\" \"Fact\"        \"They\"        \"argued\"      \"a\"          \n [6] \"crowd\"       \"movement\"    \"around\"      \"Ms\"          \"Le\"         \n[11] \"Pen\"         \"had\"        \n[ ... and 72 more ]\n\ntext2 :\n [1] \"Dr\"            \"Robert\"        \"Jay\"           \"Lifton\"       \n [5] \"distinguished\" \"professor\"     \"emeritus\"      \"at\"           \n [9] \"John\"          \"Jay\"           \"College\"       \"and\"          \n[ ... and 56 more ]\n\ntext3 :\n [1] \"If\"      \"you\"     \"prefer\"  \"not\"     \"to\"      \"click\"   \"on\"     \n [8] \"Daily\"   \"Mail\"    \"sources\" \"then\"    \"here\"   \n[ ... and 131 more ]\n\ntext4 :\n [1] \"Ben\"      \"Judah\"    \"details\"  \"Emmanuel\" \"Macron's\" \"nascent\" \n [7] \"foreign\"  \"policy\"   \"doctrine\" \"Noting\"   \"both\"     \"that\"    \n[ ... and 65 more ]\n\ntext5 :\n [1] \"Ergo\"     \"he\"       \"supports\" \"Macron\"   \"but\"      \"doesn't\" \n [7] \"want\"     \"to\"       \"say\"      \"it\"       \"out\"      \"loud\"    \n[ ... and 8 more ]\n\ntext6 :\n [1] \"He\"      \"looks\"   \"exactly\" \"the\"     \"same\"    \"in\"      \"Richie\" \n [8] \"Rich\"    \"as\"      \"he\"      \"does\"    \"as\"     \n[ ... and 12 more ]\n\n\n\n\n\n\n文档-特征矩阵（document-feature matrices）是将文本表示为量化数据的标准方式。幸运的是，在 quanteda 中将 tokens 对象转换为 dfm 非常简单。\n\n请使用 dfm 函数对你之前创建的分词（tokenized）对象构建一个文档-特征矩阵。我们可以使用 ?dfm 阅读该函数的帮助文档，以了解可用的参数选项。一旦创建完成 dfm，请使用 topfeatures() 函数来查看该矩阵中出现频率最高的前 20 个特征词。你观察到的都是哪一类词语？\n\nmft_dfm &lt;- dfm(mft_tokens)\nmft_dfm\n\n\nClick to show output\n\nDocument-feature matrix of: 17,886 documents, 30,419 features (99.91% sparse) and 8 docvars.\n       features\ndocs    alternative fact they argued a crowd movement around ms le\n  text1           2    2    1      1 5     2        2      1  1  2\n  text2           0    0    0      0 1     0        0      0  0  0\n  text3           1    0    1      0 3     0        0      0  0  0\n  text4           0    0    0      0 2     0        0      0  0  0\n  text5           0    0    0      0 1     0        0      0  0  0\n  text6           0    0    0      0 0     0        0      0  0  0\n[ reached max_ndoc ... 17,880 more documents, reached max_nfeat ... 30,409 more features ]\n\n\n\ntopfeatures(mft_dfm, 20)\n\n\nClick to show output\n\nthe    to   and     a    of    is  that     i   you    in   for    it   not \n22026 17337 14025 12792 11252 10653  8757  8681  8518  6972  6380  6190  5358 \n this    be   are  they   but    le  with \n 4601  4549  4494  4149  4109  3861  3635 \n\n\n\n\n基本都是停用词（stop words）\n\n\n尝试使用不同的 dfm_* 函数，例如 dfm_wordstem()、dfm_remove() 和 dfm_trim()。这些函数可以在文档-特征矩阵构建完成后，用于缩减其规模。将它们依次应用于你在上一个问题中创建的 dfm 对象，并观察特征数量的变化情况。\n\ndim(mft_dfm)\n[1] 17886 30419\ndim(dfm_wordstem(mft_dfm))\n[1] 17886 21523\ndim(dfm_remove(mft_dfm, pattern = c(\"of\", \"the\", \"and\")))\n[1] 17886 30416\ndim(dfm_trim(mft_dfm, min_termfreq = 5, min_docfreq = 0.01, termfreq_type = \"count\", docfreq_type = \"prop\"))\n[1] 17886   380\n\n请使用 dfm_remove() 函数从该数据中移除英文停用词。你可以通过以下命令获取英文停用词列表：\n\nstopwords(\"english\")\n#移除英文停用词\nmft_dfm_nostops &lt;- dfm_remove(mft_dfm, pattern = stopwords(\"english\"))\nmft_dfm_nostops\n\n\nClick to show output\n\nDocument-feature matrix of: 17,886 documents, 30,247 features (99.94% sparse) and 8 docvars.\n       features\ndocs    alternative fact argued crowd movement around ms le pen become\n  text1           2    2      1     2        2      1  1  2   2      1\n  text2           0    0      0     0        0      0  0  0   0      0\n  text3           1    0      0     0        0      0  0  0   0      0\n  text4           0    0      0     0        0      0  0  0   0      0\n  text5           0    0      0     0        0      0  0  0   0      0\n  text6           0    0      0     0        0      0  0  0   0      0\n[ reached max_ndoc ... 17,880 more documents, reached max_nfeat ... 30,237 more features ]\n\n\n\n\n\n在讲座中，我们学习了 %&gt;% 这个“管道”操作符，它可以将多个函数连接起来，使得一个函数的输出可以直接作为下一个函数的输入。我们可以使用这种管道语法来简化代码，并使其更易阅读。\n例如，我们可以使用管道操作符，将先前分别进行的语料库构建与分词步骤合并为一行代码：\nmft_tokens &lt;- mft_texts %&gt;% # Take the original data object\n  corpus(text_field = \"text\") %&gt;% # ...convert to a corpus\n  tokens(remove_punct = TRUE) #...and then tokenize\n\nmft_tokens[1]\n\n\nClick to show output\n\nTokens consisting of 1 document and 8 docvars.\ntext1 :\n [1] \"Alternative\" \"Fact\"        \"They\"        \"argued\"      \"a\"          \n [6] \"crowd\"       \"movement\"    \"around\"      \"Ms\"          \"Le\"         \n[11] \"Pen\"         \"had\"        \n[ ... and 72 more ]\n\n\n\n使用 %&gt;% 管道操作符编写的 R 语言代码，依次完成以下任务：a) 创建语料库; b) 对文本进行分词; c) 构建文档-特征矩阵（dfm）; d) 移除停用词; e) 输出出现频率最高的特征词:\n\nmft_texts %&gt;%                           # 获取原始数据对象\n  corpus(text_field = \"text\") %&gt;%       # 转换为语料库\n  tokens(remove_punct = TRUE) %&gt;%       # 分词化\n  dfm() %&gt;%                             #构建文档-特征矩阵（dfm）\n  dfm_remove(pattern = stopwords(\"english\")) %&gt;% # 移除停用词\n  topfeatures()                         # 输出出现频率最高的特征词\n\n\nClick to show output\n\nle    pen macron people   like   just    can  think    get  trump \n  3861   3517   3307   3124   2967   2584   1812   1684   1460   1367 \n\n\n\n\n\n\n使用 ntoken() 函数来统计语料库中每条文本的词元数量。将该函数的输出赋值给一个新对象，以便后续使用:\n\nmft_n_words &lt;- ntoken(mft_corpus)\n\n使用 hist() 函数绘制直方图，以展示语料库中各文档长度（以词元数计）的分布情况。同时，使用 summary() 函数报告集中趋势的相关统计量:\n\nhist(mft_n_words)\n\n\nClick to show figure\n\n\n\nsummary(mft_n_words)\n Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   7.00   20.00   31.00   39.09   53.00  177.00 \n\nReddit 语料库中评论的中位长度为 31 个词。绘制的图形表明，该语料库中的文档长度分布呈正偏态（positively skewed），也就是说，大多数评论较短，分布的主体集中在左侧，而右尾较长，表明存在少量篇幅较长的评论。\n\n\n\n词典（dictionary）是具名列表（named list），由“键（key）”与一组对应条目组成，这些条目定义了该键的等价类。在 quanteda 中，词典可通过 dictionary() 函数创建。该函数的输入是一个列表（list），其中包含若干具名的字符向量（named character vectors）。\n例如，假设我们希望构建一个简单的词典，用于衡量与两门课程相关的词语：量化文本分析（quantitative text analysis）与因果推断（causal inference）。我们可以首先为每个概念创建一组相关词汇的向量，并将其存储在一个列表中：\nteaching_dictionary_list &lt;- list(qta = c(\"quantitative\", \"text\", \"analysis\", \"document\", \"feature\", \"matrix\"),\n                                 causal = c(\"potential\", \"outcomes\", \"framework\", \"counterfactual\"))\n然后我们将该向量传递给 dictionary() 函数：\nteaching_dictionary &lt;- dictionary(teaching_dictionary_list)\nteaching_dictionary\nDictionary object with 2 key entries.\n- [qta]:\n  - quantitative, text, analysis, document, feature, matrix\n- [causal]:\n  - potential, outcomes, framework, counterfactual\n我们当然可以扩展类别的数量，并为每个类别添加更多词语。\n在开始编码练习之前，请先查看 mft_dictionary.csv 文件。你认为每个道德基础所关联的词语是否合理？是否有某些词语看起来不太恰当？请记住，构建词典在很大程度上依赖主观判断，所选词汇的范围与内容会对你所进行的任何分析结果产生显著影响！\n\n从 mft_dictionary_words 数据中的词汇创建一个 quanteda 词典对象。你的词典应包含两个类别——一个对应 “care”(关心)，另一个对应 “sanctity”（神圣）：\n\nmft_dictionary_list &lt;- list(\n  care = mft_dictionary_words$word[mft_dictionary_words$foundation == \"care\"],\n  sanctity = mft_dictionary_words$word[mft_dictionary_words$foundation == \"sanctity\"]\n  )\n\nmft_dictionary &lt;- dictionary(mft_dictionary_list)\nmft_dictionary\nDictionary object with 2 key entries.\n- [care]:\n  - alleviate, alleviated, alleviates, alleviating, alleviation, altruism, altruist, beneficence, beneficiary, benefit, benefits, benefitted, benefitting, benevolence, benevolent, care, cared, caregiver, cares, caring [ ... and 444 more ]\n- [sanctity]:\n  - abstinance, abstinence, allah, almighty, angel, apostle, apostles, atone, atoned, atonement, atones, atoning, beatification, beatify, beatifying, bible, bibles, biblical, bless, blessed [ ... and 640 more ]\n\n使用你在上一个问题中创建的词典，并将其应用于你在本次作业前面创建的文档-特征矩阵。为此，你需要使用 dfm_lookup() 函数。(如果需要帮助，可以阅该函数的帮助文档?dfm_lookup）\n\nmft_dfm_dictionary &lt;- dfm_lookup(mft_dfm, mft_dictionary)\nmft_dfm_dictionary\n\n\nClick to show output\n\nDocument-feature matrix of: 17,886 documents, 2 features (78.13% sparse) and 8 docvars.\n       features\ndocs    care sanctity\n  text1    0        1\n  text2    1        0\n  text3    1        0\n  text4    1        0\n  text5    0        1\n  text6    0        0\n[ reached max_ndoc ... 17,880 more documents ]\n\n\n\n你刚刚创建的词典-文档矩阵（dictionary-dfm）记录了每条文本中与每个道德基础类别相关的词汇数量。然而，正如我们之前所见，并非所有文本的词数都相同。请创建该词典-文档矩阵的一个新版本，其中记录的是每条文本中与各道德基础类别相关的词汇占总词数的比例。\n\nmft_dfm_dictionary_proportions &lt;- mft_dfm_dictionary/mft_n_words\n\n将每个道德基础类别的词典得分存储为新的变量，添加到原始的 mft_texts 数据框中。你需要使用 as.numeric 函数，将 dfm 中的每一列强制转换为适合存储在 data.frame 中的格式:\n\nmft_texts$care_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,1])\nmft_texts$sanctity_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,2])\n\n请注意，这里的代码只是将应用词典后的输出结果赋值给 mft_texts 数据框。这只是为了使后续的分析步骤更为简便。\n\n\n\n\n现在我们已经构建好了词典指标，接下来将进行一些基本的效度检验。我们将从直接检查在各个道德基础类别中被词典赋予高分的文本开始。\n为此，我们需要根据词典分析所赋予的分值对文本进行排序。例如，对于 “care” 基础，可以使用如下代码：\nmft_texts$text[order(mft_texts$care_dictionary, decreasing = TRUE)][1:5]\n\n\n方括号运算符（[]）允许我们对 mft_texts$text 变量进行子集提取;\norder() 函数根据 mft_texts$care_dictionary 变量的数值对观测进行排序，参数 decreasing = TRUE 表示按从大到小的顺序排列。\n\n\n\n在 care 和 sanctity 两个道德基础类别中，词典得分最高的前 5 条文本分别是哪些？\n\nmft_texts$text[order(mft_texts$care_dictionary, decreasing = TRUE)][1:5]\n\n\nClick to show output\n\n[1] \"i doubt she'd want \\\"help\\\" from a childhood bully 🙄\"\n[2] \"We are talking about threats to human safety, not threats to property.\"\n[3] \"What guarantees adoptive parents would have been loving? That the child wouldn’t suffer\" \n[4] \"The dress design didn’t have anything to do with child sexual assault victims. https://www.papermag.com/alexander-mcqueen-dancing-girls-dress-2645945769.html\"\n[5] \"This right here. Threatening violence of any kind is not ok, sexual rape violence SO not ok.\"                                                                 \n\n\nmft_texts$text[order(mft_texts$sanctity_dictionary, decreasing = TRUE)][1:5]\n\n\nClick to show output\n\n[1] \"hot fucking damn macron this last bit was inspiring as hell\"   \n[2] \"Fuck you. Seriously, fuck you. Get your shit together you fucking junkie.\"\n[3] \"Hell yeah! That’s some hardcore nostalgia right there god DAMN.\"          \n[4] \"This is so fucking sad. I hate this god damned country\"         \n[5] \"Holy fuck. This makes OP TA forever. How fucking awful.\"                                                                         \n\n\n\n阅读与 care 和 sanctity 两个道德基础最强相关的文本。你认为这些词典是否准确捕捉了 “关怀” 与 “神圣” 这两个概念？\n\n\n在一些情况下，这些文本与相应的道德基础类别的关联似乎是合理的。例如，许多与 “care” 相关的文本确实涉及对他人伤害的描述。\n但在其他情况下，词典似乎捕捉到的内容并不准确。例如，所有与 “sanctity” 相关的高分文本几乎都是因为包含脏话。虽然咒骂行为可能在某种程度上与对神圣议题的道德敏感性相关，但它本身并不构成对“神圣”概念的实质性表达，因此这提示我们：sanctity 词典可能仍需改进。\n\n\nmft_texts 对象中包含了一系列变量，记录了每条文本由人工标注所归类的道德基础类别。请使用 table() 函数创建一个混淆矩阵 (confusion matrix)，以比较人工标注结果与词典分析生成的得分之间的一致性:\n\ncare_confusion &lt;- table( \n  dictionary = mft_texts$care_dictionary &gt; 0,\n  human_coding = mft_texts$care)\ncare_confusion\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 11248  2553\n     TRUE   1898  2187\nsanctity_confusion &lt;- table(\n  dictionary = mft_texts$sanctity_dictionary &gt; 0,\n  human_coding = mft_texts$sanctity)\nsanctity_confusion\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 13268   878\n     TRUE   2871   869\n\n对于每个道德基础类别，分类器的准确率是多少？（如果你忘记了如何计算，请查看Lecture讲义）\n\n# Care\n(2187 + 11248)/(2187 + 11248 + 2553 + 1898)\n[1] 0.7511461\n# Sanctity\n(869 + 13268)/(869 + 13268 + 2871 + 878)\n[1] 0.7903947\n\n对于每个道德基础类别，分类器的灵敏度（sensitivity）是多少？\n\n# Care\n(2187)/(2187 + 2553)\n[1] 0.4613924\n# Sanctity\n869/(869 + 878)\n[1] 0.4974242\n\n对于每个道德基础类别，分类器的特异度（specificity）是多少？\n\n# Care\n(11248)/(11248 + 1898)\n[1] 0.8556215\n# Sanctity\n13268/(13268 + 2871)\n[1] 0.8221079\n\n根据这些数值，我们的词典在分类任务中的表现如何？\n\n\n虽然整体的准确率（accuracy）看起来相对较高，但从灵敏度（sensitivity）得分来看，词典在识别真正的 “care” 和 “sanctity” 编码文本方面的表现其实并不理想。\n具体而言，词典对于这两个类别都未能识别出 50% 以上的真正阳性样本（true positives）。换句话说，虽然词典在多数情况下可能避免了错误分类，但却漏掉了大量真正具有“关怀”或“神圣”特征的文本。\n这种情况有些类似于一个安检系统，它在避免误报（不把无害物误判为危险品）方面做得不错，但却频繁漏检真正的危险品。这对研究者来说是一个警示：词典虽然提供了一种高效的量化方法，但在覆盖复杂语义和细微道德表达方面，可能存在着结构性不足。\n\n\n除了以上这种直接手算的方式，我们也可以使用 caret 程序包非常简便地计算这些统计量:\n\n# install.packages(\"caret\") # 只需要在第一次运行这行代码安装!\nlibrary(caret)\n\nconfusionMatrix(care_confusion, positive = \"TRUE\")\n\n\nClick to show output\n\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 11248  2553\n     TRUE   1898  2187\n                                          \n               Accuracy : 0.7511          \n                 95% CI : (0.7447, 0.7575)\n    No Information Rate : 0.735           \n    P-Value [Acc &gt; NIR] : 4.343e-07       \n                                          \n                  Kappa : 0.3317          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.4614          \n            Specificity : 0.8556          \n         Pos Pred Value : 0.5354          \n         Neg Pred Value : 0.8150          \n             Prevalence : 0.2650          \n         Detection Rate : 0.1223          \n   Detection Prevalence : 0.2284          \n      Balanced Accuracy : 0.6585          \n                                          \n       'Positive' Class : TRUE          \n\n\nconfusionMatrix(sanctity_confusion, positive = \"TRUE\")\n\n\nClick to show output\n\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 13268   878\n     TRUE   2871   869\n                                          \n               Accuracy : 0.7904          \n                 95% CI : (0.7844, 0.7963)\n    No Information Rate : 0.9023          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2118          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.49742         \n            Specificity : 0.82211         \n         Pos Pred Value : 0.23235         \n         Neg Pred Value : 0.93793         \n             Prevalence : 0.09767         \n         Detection Rate : 0.04859         \n   Detection Prevalence : 0.20910         \n      Balanced Accuracy : 0.65977         \n                                          \n       'Positive' Class : TRUE            \n                                                                                                                 \n\n\n\n\n\n在完成上述验证（尽管词典得分与人工编码的相关性相对较弱）之后，我们现在可以继续进行一个简单的应用。\n\n请计算数据中 11 个 subreddit 各自的 care 和 sanctity 词典得分的平均值：\n\ndictionary_means_by_subreddit &lt;- mft_texts %&gt;%\n  group_by(subreddit) %&gt;%\n  summarise(care_dictionary = mean(care_dictionary),\n            sanctity_dictionary = mean(sanctity_dictionary)) \n\ndictionary_means_by_subreddit\n\n\nClick to show output\n\n# A tibble: 11 × 3\n   subreddit           care_dictionary sanctity_dictionary\n                                           \n 1 AmItheAsshole               0.0118              0.00966\n 2 Conservative                0.00946             0.0101 \n 3 antiwork                    0.0106              0.0120 \n 4 confession                  0.0117              0.0127 \n 5 europe                      0.00488             0.00453\n 6 geopolitics                 0.00394             0.00137\n 7 neoliberal                  0.00430             0.00662\n 8 nostalgia                   0.00767             0.00959\n 9 politics                    0.00923             0.0108 \n10 relationship_advice         0.0127              0.0110 \n11 worldnews                   0.00651             0.00616\n                                                                  \n\n\n\ngroup_by() 是一个特殊函数，它允许我们根据指定变量的分组对数据进行操作（在这里，我们按 subreddit 分组，因为我们希望了解每个 subreddit 的词典得分均值）\nsummarise() 是一个函数，可用于对数据计算各种类型的汇总统计量。\n\n2.对你刚刚计算出的 subreddit 平均值进行解释，或许将这些信息以可视化图形形式展示出来更直观：\ndictionary_means_by_subreddit %&gt;%\n  # 将数据转换成\"long format\"以便于绘制图表\n  pivot_longer(-subreddit) %&gt;%\n  # 使用 ggplot\n  ggplot(aes(x = name, y = subreddit, fill = value)) + \n  # 使用 geom_tile 绘制 heatmap\n  geom_tile() + \n  # 修改颜色让它更美观\n  scale_fill_gradient(low = \"white\", high = \"purple\") + \n  # 去除坐标轴\n  xlab(\"\") + \n  ylab(\"\") \n\n\nClick to show figure\n\n\n\n\n“geopolitics” 子版块似乎几乎不包含与 sanctity（神圣） 相关的语言；而 “confession” 子版块则大量使用了与 sanctity 相关的词汇。\n与 care（关怀） 相关的语言在 “relationship_advice” 和 “AmItheAsshole” 子版块中则较为常见。\n\n\ncare 和 sanctity 的词典得分之间的相关性是多少？这两个道德基础是否彼此密切相关？\n\ncor(mft_texts$care_dictionary,mft_texts$sanctity_dictionary)\n[1] 0.03435137\n\n不，基于 care 与 sanctity 的语言之间的相关性非常低。\n\n\n\n\n\n（这一部分在正式授课中并不会直接提供代码，而会在相应seminar结束后的第二天更新代码和解决方案）\n\n请模仿seminar中的分析过程，将其应用到其余三个道德基础类别上：fairness、loyalty 和 authority。然后，绘制一张图表，展示每个 subreddit 在每个道德基础类别上的平均词典得分。\n\nmft_dictionary_list &lt;- list(\n  care = mft_dictionary_words$word[mft_dictionary_words$foundation == \"care\"],\n  sanctity = mft_dictionary_words$word[mft_dictionary_words$foundation == \"sanctity\"],\n  authority = mft_dictionary_words$word[mft_dictionary_words$foundation == \"authority\"],\n  fairness = mft_dictionary_words$word[mft_dictionary_words$foundation == \"fairness\"],\n  loyalty = mft_dictionary_words$word[mft_dictionary_words$foundation == \"loyalty\"]\n  )\n\nmft_dictionary &lt;- dictionary(mft_dictionary_list)\n\nmft_dfm_dictionary &lt;- dfm_lookup(mft_dfm, mft_dictionary)\n\nmft_dfm_dictionary_proportions &lt;- mft_dfm_dictionary/mft_n_words\n\nmft_texts$care_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,1])\nmft_texts$sanctity_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,2])\nmft_texts$authority_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,3])\nmft_texts$fairness_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,4])\nmft_texts$loyalty_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,5])\n\ndictionary_means_by_subreddit &lt;- mft_texts %&gt;%\n  group_by(subreddit) %&gt;%\n  summarise(care_dictionary = mean(care_dictionary),\n            sanctity_dictionary = mean(sanctity_dictionary),\n            authority_dictionary = mean(authority_dictionary),\n            fairness_dictionary = mean(fairness_dictionary),\n            loyalty_dictionary = mean(loyalty_dictionary)) \n\n\ndictionary_means_by_subreddit %&gt;%\n  # Transform the data to \"long\" format for plotting\n  pivot_longer(-subreddit) %&gt;%\n  # Use ggplot\n  ggplot(aes(x = name, y = subreddit, fill = value)) + \n  # geom_tile creates a heatmap\n  geom_tile() + \n  # change the colours to make them prettier\n  scale_fill_gradient(low = \"white\", high = \"purple\") + \n  # remove the axis labels\n  xlab(\"\") + \n  ylab(\"\") \n\n\nClick to show figure"
  },
  {
    "objectID": "Seminar/Seminar1.html#研讨会介绍",
    "href": "Seminar/Seminar1.html#研讨会介绍",
    "title": "Seminar1",
    "section": "",
    "text": "本次研讨课旨在引导学生实际操作 quanteda 套件及若干相关 R 语言程序包。课程重点包括：探索该套件的基本功能，将文本导入并转换为语料库（corpus）对象格式，学习如何将文本转化为文档-特征矩阵（document-feature matrices, DFM），在此基础上开展描述性分析，并尝试构建与验证字典工具（dictionaries）以支持文本分析研究。\n对于R语言，请参考我们day1讨论的 R 语言入门（链接施工中…）。"
  },
  {
    "objectID": "Seminar/Seminar1.html#道德情感的测量",
    "href": "Seminar/Seminar1.html#道德情感的测量",
    "title": "Seminar1",
    "section": "",
    "text": "“道德基础理论”（Moral Foundations Theory）是一种社会心理学理论，认为人们的道德判断基于五种彼此独立的道德价值。这五种基础（如下表所列）分别是：关怀/伤害（care/harm）、公正/欺骗（fairness/cheating）、忠诚/背叛（loyalty/betrayal）、权威/颠覆（authority/subversion）以及神圣/堕落（sanctity/degradation）。\n根据该理论，人们在面对不同的议题和情境时，会依据这些道德基础进行判断。此外，该理论指出，不同个体对于这五种价值的重视程度存在差异，而这种差异有助于解释人们在道德判断上的文化性和个体性差异。\n下表提供了这五项道德基础的简要概述：\n\n\n\n\n\n\n\n基础\n介绍\n\n\n\n\nCare\nConcern for caring behaviour, kindness, compassion\n\n\nFairness\nConcern for fairness, justice, trustworthiness\n\n\nAuthority\nConcern for obedience and deference to figures of authority (religious, state, etc)\n\n\nLoyalty\nConcern for loyalty, patriotism, self-sacrifice\n\n\nSanctity\nConcern for temperance, chastity, piety, cleanliness\n\n\n\n我们能否从文本中识别出道德基础的使用？在政治论辩中，道德框架与修辞策略发挥着重要作用。而根深蒂固的道德分歧常被认为是政治极化，尤其是在网络环境中加剧的重要根源。在我们探讨诸如“不同政治立场的群体在道德语言使用上是否存在显著差异”（案例）或“道德论证是否有助于缓解政治极化”（案例）等关键研究问题之前，前提是我们必须能够在大规模文本中对道德语言的使用进行有效测量。\n因此，在本次研讨课中，我们将采用一种简化的字典分析方法，以衡量一组在线文本中所体现的道德语言使用程度。该方法将依据“道德基础理论”中所界定的不同类型道德价值，评估文本内容与相应道德框架之间的契合程度。"
  },
  {
    "objectID": "Seminar/Seminar1.html#数据集",
    "href": "Seminar/Seminar1.html#数据集",
    "title": "Seminar1",
    "section": "",
    "text": "在本节研讨会中，我们会使用两个数据集：\n1. Moral Foundations Dictionary – mft_dictionary.csv\n\n该数据集收录了一系列被认为能指示文本中不同道德关切的词汇列表。该字典最初由 Jesse Graham 与 Jonathan Haidt 开发，并在其相关论文中有更为详尽的介绍。\n字典共划分为五类道德关切维度：权威（authority）、忠诚（loyalty）、神圣（sanctity）、公正（fairness）与关怀（care），每一类均对应一组特定的代表性词汇，用于识别该类道德语义在文本中的体现。\n\n\nMoral Foundations Reddit Corpus – mft_texts.csv\n\n\n本文件包含 17,886 条英文 Reddit 评论，这些评论由 11 个不同的子版块（subreddits）中精选而来。评论内容涵盖多种不同主题。此外，该数据集还包含由专业标注员手工完成的注释，标注依据为“道德基础理论”中定义的各类道德关切维度。\n\n在下载并妥善保存这些数据文件后，我们可以通过以下R语言指令将其载入工作环境：\nmft_dictionary_words &lt;- read_csv(\"mft_dictionary.csv\")\nmft_texts &lt;- read_csv(\"mft_texts.csv\")"
  },
  {
    "objectID": "Seminar/Seminar1.html#packages",
    "href": "Seminar/Seminar1.html#packages",
    "title": "Seminar1",
    "section": "",
    "text": "在开始seminar时，我们需要安装并加载以下R语言程序包。\n请运行以下代码行以完成这些程序包的安装。请注意，程序包仅需在本机安装一次；安装完成后，就可以删除这些安装指令：\ninstall.packages(\"tidyverse\") # Set of packages helpful for data manipulation\ninstall.packages(\"quanteda\") # Main quanteda package\ninstall.packages(\"quanteda.textplots\") # Package with helpful plotting functions\ninstall.packages(\"quanteda.textstats\") # Package with helpful statistical functions\ninstall.packages(\"remotes\") # Package for installing other packages\nremotes::install_github(\"kbenoit/quanteda.dictionaries\")\n在完成上述程序包的安装后，我们还需要加载这些程序包，以便在R Studio中使用其所包含的各类函数。\n每当我们希望使用这些程序包的函数时，都需要运行以下代码行来加载它们（此操作在每次新的 R 会话中都需重复执行）：\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\nlibrary(quanteda.dictionaries)"
  },
  {
    "objectID": "Seminar/Seminar1.html#建立一个corpus",
    "href": "Seminar/Seminar1.html#建立一个corpus",
    "title": "Seminar1",
    "section": "",
    "text": "用R编程往往是一个令人沮丧的经历。幸运的是，当我们遇到困难时，有很多方法可以寻求帮助。\n\n官方网站 http://quanteda.io 提供了详尽的文档与使用说明。\n你可以通过输入 ?function_name 的形式，查阅任意函数的帮助文档。\n此外，还可以使用 example() 函数来运行某一函数的示例代码，以观察其具体的用法与效果。\n\n例如，若要查阅 corpus() 函数的帮助文档，可使用以下代码：\n?corpus\n\n\n\n在 quanteda 中，corpus 对象是我们开展一切文本分析工作的基础。因此，在将文本数据加载至 R 环境后，首要步骤是使用 corpus() 函数将其转换为语料库格式。\n\n创建语料库最简单的方式，是直接使用 R 全局环境中已有的一组文本对象。在本例中，我们已加载 mft_texts.csv 文件，并将其存储为 mft_texts 对象。我们可以先查看这个对象的内容，了解其结构。请使用 head() 函数作用于 mft_texts 对象，并记录其输出结果。思考：哪个变量字段包含了 Reddit 评论的正文文本？\n\nhead(mft_texts)\n\n\nClick to show output\n\n# A tibble: 6 × 9\n  ...1 text              subreddit  care authority loyalty sanctity fairness non_mor\n                                         \n1 1     Alternati…       worldnews  FALSE FALSE     FALSE   FALSE    FALSE    TRUE\n2 2     Dr. Rober…       politics   FALSE TRUE      FALSE   FALSE    TRUE     TRUE\n3 3     If you pr…       worldnews  FALSE FALSE     FALSE   FALSE    FALSE    TRUE\n4 4     Ben Judah…       geopolit… FALSE TRUE      FALSE   FALSE    TRUE     FALSE\n5 5     Ergo, he…        neoliber… FALSE FALSE     TRUE    FALSE    FALSE    TRUE\n6 6     He looks …        nostalgia FALSE FALSE     FALSE   FALSE    FALSE    TRUE\n\n\n\n\n输出结果显示，该对象是一个 “tibble” —— 这是一种特殊类型的 data.frame 数据结构。我们可以查看该数据框前六行的内容。其中，名为 text 的列包含了 Reddit 评论的正文文本。\n\n\n请使用 corpus() 函数对这组文本数据进行处理，以创建一个新的语料库对象。corpus() 函数的第一个参数应为 mft_texts 对象；同时，还需设置 text_field 为 \"text\"，以便 quanteda 知道我们感兴趣的文本内容存储在哪一个变量列中。\n\nmft_corpus &lt;- corpus(mft_texts, text_field = \"text\")\n\n3. 在成功构建语料库对象之后，请使用 summary() 函数对该语料库进行简要描述，以查看其基本信息和结构摘要。思考：在这些 subreddit 名称中，你觉得哪个最有趣、最具创意或最令人发笑？\nsummary(mft_corpus)\n\n\nClick to show output\n\nCorpus consisting of 17886 documents, showing 100 documents:\n\n    Text Types Tokens Sentences ...1           subreddit  care authority\n   text1    67    101         7    1           worldnews FALSE     FALSE\n   text2    59     72         2    2            politics FALSE      TRUE\n   text3    81    177         3    3           worldnews FALSE     FALSE\n   text4    65     82         5    4         geopolitics FALSE      TRUE\n   text5    22     22         2    5          neoliberal FALSE     FALSE\n   text6    21     28         2    6           nostalgia FALSE     FALSE\n   text7    22     24         3    7           worldnews  TRUE     FALSE\n   text8    41     44         3    8 relationship_advice  TRUE     FALSE\n   text9    20     21         1    9       AmItheAsshole FALSE      TRUE\n  text10    12     12         2   10            antiwork FALSE     FALSE\n  text11    35     43         3   11              europe FALSE     FALSE\n  text12    35     40         2   12          confession FALSE     FALSE\n  text13    38     43         2   13 relationship_advice  TRUE     FALSE\n  text14    46     62         2   14           worldnews  TRUE     FALSE\n  text15    57     88         4   15            antiwork FALSE     FALSE\n  text16    16     17         1   16           worldnews FALSE     FALSE\n  text17    21     22         3   17       AmItheAsshole  TRUE     FALSE\n  text18    20     21         3   18          neoliberal FALSE      TRUE\n  text19    22     24         1   19            politics FALSE      TRUE\n  text20    19     21         3   20          confession FALSE     FALSE\n  text21    48     58         1   21            antiwork  TRUE     FALSE\n  text22    78    115         4   22            politics FALSE     FALSE\n  text23    12     13         1   23        Conservative FALSE     FALSE\n  text24    38     66         1   24            antiwork FALSE     FALSE\n  text25    18     26         2   25          confession FALSE     FALSE\n  text26    22     32         1   26            politics FALSE     FALSE\n  text27    70    103         7   27       AmItheAsshole  TRUE     FALSE\n  text28    66     92         5   28           worldnews FALSE     FALSE\n  text29    43     57         3   29          confession FALSE     FALSE\n  text30    35     42         3   30       AmItheAsshole FALSE     FALSE\n  text31    44     52         3   31           worldnews  TRUE     FALSE\n  text32    31     39         4   32            antiwork FALSE     FALSE\n  text33    20     24         2   33            antiwork  TRUE     FALSE\n  text34    33     39         2   34            antiwork FALSE      TRUE\n  text35    19     24         1   35          confession FALSE     FALSE\n  text36    11     14         1   36        Conservative FALSE     FALSE\n  text37    21     24         2   37           worldnews FALSE     FALSE\n  text38    73    104         6   38 relationship_advice  TRUE      TRUE\n  text39    32     41         3   39            antiwork FALSE     FALSE\n  text40    73     95         2   40          neoliberal FALSE     FALSE\n  text41    27     29         2   41        Conservative FALSE     FALSE\n  text42    11     13         1   42            politics FALSE     FALSE\n  text43    37     48         2   43              europe FALSE     FALSE\n  text44    66    101         6   44           worldnews FALSE     FALSE\n  text45    11     13         1   45            antiwork FALSE     FALSE\n  text46    50     66         1   46            politics FALSE     FALSE\n  text47    41     49         3   47              europe FALSE      TRUE\n  text48    13     16         1   48            politics FALSE      TRUE\n  text49    41     52         2   49        Conservative FALSE     FALSE\n  text50    39     49         3   50              europe FALSE     FALSE\n  text51    14     15         1   51 relationship_advice  TRUE     FALSE\n  text52    73    110         3   52           nostalgia FALSE     FALSE\n  text53    34     39         3   53          confession FALSE     FALSE\n  text54    21     22         1   54        Conservative FALSE     FALSE\n  text55    30     33         2   55              europe FALSE      TRUE\n  text56    20     21         2   56            politics FALSE     FALSE\n  text57    13     15         1   57              europe  TRUE     FALSE\n  text58    30     42         5   58           worldnews  TRUE      TRUE\n  text59    34     40         2   59          neoliberal FALSE     FALSE\n  text60    30     38         4   60          confession FALSE     FALSE\n  text61    40     44         3   61           nostalgia FALSE     FALSE\n  text62    20     25         1   62        Conservative FALSE     FALSE\n  text63    33     38         2   63           worldnews  TRUE     FALSE\n  text64    21     31         4   64        Conservative FALSE     FALSE\n  text65    28     35         1   65          neoliberal  TRUE      TRUE\n  text66    13     14         2   66            antiwork FALSE     FALSE\n  text67    11     12         1   67            antiwork  TRUE     FALSE\n  text68    14     17         2   68          neoliberal FALSE     FALSE\n  text69    18     19         1   69            politics FALSE     FALSE\n  text70    15     20         1   70          neoliberal FALSE     FALSE\n  text71    40     50         1   71           worldnews FALSE     FALSE\n  text72    35     45         1   72           nostalgia FALSE     FALSE\n  text73    32     43         2   73       AmItheAsshole  TRUE     FALSE\n  text74    32     36         3   74              europe FALSE      TRUE\n  text75    38     54         5   75 relationship_advice  TRUE     FALSE\n  text76    15     18         1   76        Conservative FALSE      TRUE\n  text77    24     25         1   77 relationship_advice  TRUE     FALSE\n  text78    20     23         1   78        Conservative FALSE     FALSE\n  text79    22     24         3   79            antiwork FALSE     FALSE\n  text80    17     21         1   80           worldnews FALSE     FALSE\n  text81    36     47         4   81 relationship_advice  TRUE     FALSE\n  text82    49     68         1   82            antiwork FALSE     FALSE\n  text83    67     82         2   83          confession FALSE     FALSE\n  text84    26     32         1   84           worldnews FALSE     FALSE\n  text85    25     35         2   85            antiwork FALSE     FALSE\n  text86    14     16         1   86            antiwork  TRUE     FALSE\n  text87    27     28         1   87       AmItheAsshole  TRUE      TRUE\n  text88    18     20         1   88       AmItheAsshole FALSE     FALSE\n  text89    33     42         3   89           worldnews FALSE      TRUE\n  text90    16     20         1   90          neoliberal FALSE     FALSE\n  text91    42     55         3   91            antiwork FALSE     FALSE\n  text92    26     30         3   92       AmItheAsshole  TRUE     FALSE\n  text93    34     47         2   93        Conservative FALSE     FALSE\n  text94    22     28         3   94 relationship_advice  TRUE     FALSE\n  text95    53     71         5   95          neoliberal FALSE     FALSE\n  text96    38     55         3   96        Conservative FALSE     FALSE\n  text97    19     21         2   97           worldnews FALSE      TRUE\n  text98    32     34         2   98              europe FALSE     FALSE\n  text99    13     15         1   99           worldnews FALSE     FALSE\n text100    19     30         2  100           worldnews FALSE     FALSE\n loyalty sanctity fairness non_moral\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE    FALSE     FALSE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE     FALSE\n   FALSE    FALSE     TRUE     FALSE\n   FALSE     TRUE     TRUE     FALSE\n    TRUE    FALSE    FALSE     FALSE\n   FALSE     TRUE     TRUE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE     TRUE    FALSE      TRUE\n    TRUE     TRUE     TRUE     FALSE\n   FALSE    FALSE     TRUE      TRUE\n    TRUE    FALSE    FALSE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n   FALSE    FALSE     TRUE      TRUE\n   FALSE    FALSE    FALSE     FALSE\n   FALSE    FALSE    FALSE      TRUE\n    TRUE    FALSE    FALSE      TRUE\n   FALSE    FALSE    FALSE      TRUE\n\n\n\n4. 请注意，尽管我们在构建语料库时指定了 text_field = \"text\"，但我们并没有移除与文本相关的元数据。为了访问这些其他变量，我们可以将 docvars() 函数应用于我们上面创建的语料库对象:\nhead(docvars(mft_corpus))\n...1   subreddit  care authority loyalty sanctity fairness non_moral\n1    1   worldnews FALSE     FALSE   FALSE    FALSE    FALSE      TRUE\n2    2    politics FALSE      TRUE   FALSE    FALSE    FALSE      TRUE\n3    3   worldnews FALSE     FALSE   FALSE    FALSE    FALSE      TRUE\n4    4 geopolitics FALSE      TRUE   FALSE    FALSE     TRUE     FALSE\n5    5  neoliberal FALSE     FALSE    TRUE    FALSE    FALSE      TRUE\n6    6   nostalgia FALSE     FALSE   FALSE    FALSE    FALSE      TRUE"
  },
  {
    "objectID": "Seminar/Seminar1.html#tokenizing-texts",
    "href": "Seminar/Seminar1.html#tokenizing-texts",
    "title": "Seminar1",
    "section": "",
    "text": "为了统计词频，我们首先需要通过一个称为“分词（tokenization）”的过程，将文本拆分为词语（或更长的短语）。请查阅 quanteda 中关于 tokens() 函数的文档。\n\n对我们之前创建的语料库对象使用 tokens 命令，并查看其结果:\n\nmft_tokens &lt;- tokens(mft_corpus)\nhead(mft_tokens)\n\n\nClick to show output\n\nTokens consisting of 6 documents and 8 docvars.\ntext1 :\n [1] \"Alternative\" \"Fact\"        \":\"           \"They\"        \"argued\"     \n [6] \"a\"           \"crowd\"       \"movement\"    \"around\"      \"Ms\"         \n[11] \"Le\"          \"Pen\"        \n[ ... and 89 more ]\n\ntext2 :\n [1] \"Dr\"            \".\"             \"Robert\"        \"Jay\"          \n [5] \"Lifton\"        \",\"             \"distinguished\" \"professor\"    \n [9] \"emeritus\"      \"at\"            \"John\"          \"Jay\"          \n[ ... and 60 more ]\n\ntext3 :\n [1] \"If\"      \"you\"     \"prefer\"  \"not\"     \"to\"      \"click\"   \"on\"     \n [8] \"Daily\"   \"Mail\"    \"sources\" \",\"       \"then\"   \n[ ... and 165 more ]\n\ntext4 :\n [1] \"Ben\"      \"Judah\"    \"details\"  \"Emmanuel\" \"Macron's\" \"nascent\" \n [7] \"foreign\"  \"policy\"   \"doctrine\" \".\"        \"Noting\"   \"both\"    \n[ ... and 70 more ]\n\ntext5 :\n [1] \"Ergo\"     \",\"        \"he\"       \"supports\" \"Macron\"   \"but\"     \n [7] \"doesn't\"  \"want\"     \"to\"       \"say\"      \"it\"       \"out\"     \n[ ... and 10 more ]\n\ntext6 :\n [1] \"He\"      \"looks\"   \"exactly\" \"the\"     \"same\"    \"in\"      \"Richie\" \n [8] \"Rich\"    \"as\"      \"he\"      \"does\"    \"as\"     \n[ ... and 16 more ]\n\n\n\n接下来，请尝试使用 tokens() 函数的一些参数，例如 remove_punct 和 remove_numbers:\n\nmft_tokens &lt;- tokens(mft_corpus, remove_punct = TRUE, remove_numbers = TRUE)\nhead(mft_tokens)\n\n\nClick to show output\n\nTokens consisting of 6 documents and 8 docvars.\ntext1 :\n [1] \"Alternative\" \"Fact\"        \"They\"        \"argued\"      \"a\"          \n [6] \"crowd\"       \"movement\"    \"around\"      \"Ms\"          \"Le\"         \n[11] \"Pen\"         \"had\"        \n[ ... and 72 more ]\n\ntext2 :\n [1] \"Dr\"            \"Robert\"        \"Jay\"           \"Lifton\"       \n [5] \"distinguished\" \"professor\"     \"emeritus\"      \"at\"           \n [9] \"John\"          \"Jay\"           \"College\"       \"and\"          \n[ ... and 56 more ]\n\ntext3 :\n [1] \"If\"      \"you\"     \"prefer\"  \"not\"     \"to\"      \"click\"   \"on\"     \n [8] \"Daily\"   \"Mail\"    \"sources\" \"then\"    \"here\"   \n[ ... and 131 more ]\n\ntext4 :\n [1] \"Ben\"      \"Judah\"    \"details\"  \"Emmanuel\" \"Macron's\" \"nascent\" \n [7] \"foreign\"  \"policy\"   \"doctrine\" \"Noting\"   \"both\"     \"that\"    \n[ ... and 65 more ]\n\ntext5 :\n [1] \"Ergo\"     \"he\"       \"supports\" \"Macron\"   \"but\"      \"doesn't\" \n [7] \"want\"     \"to\"       \"say\"      \"it\"       \"out\"      \"loud\"    \n[ ... and 8 more ]\n\ntext6 :\n [1] \"He\"      \"looks\"   \"exactly\" \"the\"     \"same\"    \"in\"      \"Richie\" \n [8] \"Rich\"    \"as\"      \"he\"      \"does\"    \"as\"     \n[ ... and 12 more ]"
  },
  {
    "objectID": "Seminar/Seminar1.html#建立一个dfm",
    "href": "Seminar/Seminar1.html#建立一个dfm",
    "title": "Seminar1",
    "section": "",
    "text": "文档-特征矩阵（document-feature matrices）是将文本表示为量化数据的标准方式。幸运的是，在 quanteda 中将 tokens 对象转换为 dfm 非常简单。\n\n请使用 dfm 函数对你之前创建的分词（tokenized）对象构建一个文档-特征矩阵。我们可以使用 ?dfm 阅读该函数的帮助文档，以了解可用的参数选项。一旦创建完成 dfm，请使用 topfeatures() 函数来查看该矩阵中出现频率最高的前 20 个特征词。你观察到的都是哪一类词语？\n\nmft_dfm &lt;- dfm(mft_tokens)\nmft_dfm\n\n\nClick to show output\n\nDocument-feature matrix of: 17,886 documents, 30,419 features (99.91% sparse) and 8 docvars.\n       features\ndocs    alternative fact they argued a crowd movement around ms le\n  text1           2    2    1      1 5     2        2      1  1  2\n  text2           0    0    0      0 1     0        0      0  0  0\n  text3           1    0    1      0 3     0        0      0  0  0\n  text4           0    0    0      0 2     0        0      0  0  0\n  text5           0    0    0      0 1     0        0      0  0  0\n  text6           0    0    0      0 0     0        0      0  0  0\n[ reached max_ndoc ... 17,880 more documents, reached max_nfeat ... 30,409 more features ]\n\n\n\ntopfeatures(mft_dfm, 20)\n\n\nClick to show output\n\nthe    to   and     a    of    is  that     i   you    in   for    it   not \n22026 17337 14025 12792 11252 10653  8757  8681  8518  6972  6380  6190  5358 \n this    be   are  they   but    le  with \n 4601  4549  4494  4149  4109  3861  3635 \n\n\n\n\n基本都是停用词（stop words）\n\n\n尝试使用不同的 dfm_* 函数，例如 dfm_wordstem()、dfm_remove() 和 dfm_trim()。这些函数可以在文档-特征矩阵构建完成后，用于缩减其规模。将它们依次应用于你在上一个问题中创建的 dfm 对象，并观察特征数量的变化情况。\n\ndim(mft_dfm)\n[1] 17886 30419\ndim(dfm_wordstem(mft_dfm))\n[1] 17886 21523\ndim(dfm_remove(mft_dfm, pattern = c(\"of\", \"the\", \"and\")))\n[1] 17886 30416\ndim(dfm_trim(mft_dfm, min_termfreq = 5, min_docfreq = 0.01, termfreq_type = \"count\", docfreq_type = \"prop\"))\n[1] 17886   380\n\n请使用 dfm_remove() 函数从该数据中移除英文停用词。你可以通过以下命令获取英文停用词列表：\n\nstopwords(\"english\")\n#移除英文停用词\nmft_dfm_nostops &lt;- dfm_remove(mft_dfm, pattern = stopwords(\"english\"))\nmft_dfm_nostops\n\n\nClick to show output\n\nDocument-feature matrix of: 17,886 documents, 30,247 features (99.94% sparse) and 8 docvars.\n       features\ndocs    alternative fact argued crowd movement around ms le pen become\n  text1           2    2      1     2        2      1  1  2   2      1\n  text2           0    0      0     0        0      0  0  0   0      0\n  text3           1    0      0     0        0      0  0  0   0      0\n  text4           0    0      0     0        0      0  0  0   0      0\n  text5           0    0      0     0        0      0  0  0   0      0\n  text6           0    0      0     0        0      0  0  0   0      0\n[ reached max_ndoc ... 17,880 more documents, reached max_nfeat ... 30,237 more features ]"
  },
  {
    "objectID": "Seminar/Seminar1.html#pipes",
    "href": "Seminar/Seminar1.html#pipes",
    "title": "Seminar1",
    "section": "",
    "text": "在讲座中，我们学习了 %&gt;% 这个“管道”操作符，它可以将多个函数连接起来，使得一个函数的输出可以直接作为下一个函数的输入。我们可以使用这种管道语法来简化代码，并使其更易阅读。\n例如，我们可以使用管道操作符，将先前分别进行的语料库构建与分词步骤合并为一行代码：\nmft_tokens &lt;- mft_texts %&gt;% # Take the original data object\n  corpus(text_field = \"text\") %&gt;% # ...convert to a corpus\n  tokens(remove_punct = TRUE) #...and then tokenize\n\nmft_tokens[1]\n\n\nClick to show output\n\nTokens consisting of 1 document and 8 docvars.\ntext1 :\n [1] \"Alternative\" \"Fact\"        \"They\"        \"argued\"      \"a\"          \n [6] \"crowd\"       \"movement\"    \"around\"      \"Ms\"          \"Le\"         \n[11] \"Pen\"         \"had\"        \n[ ... and 72 more ]\n\n\n\n使用 %&gt;% 管道操作符编写的 R 语言代码，依次完成以下任务：a) 创建语料库; b) 对文本进行分词; c) 构建文档-特征矩阵（dfm）; d) 移除停用词; e) 输出出现频率最高的特征词:\n\nmft_texts %&gt;%                           # 获取原始数据对象\n  corpus(text_field = \"text\") %&gt;%       # 转换为语料库\n  tokens(remove_punct = TRUE) %&gt;%       # 分词化\n  dfm() %&gt;%                             #构建文档-特征矩阵（dfm）\n  dfm_remove(pattern = stopwords(\"english\")) %&gt;% # 移除停用词\n  topfeatures()                         # 输出出现频率最高的特征词\n\n\nClick to show output\n\nle    pen macron people   like   just    can  think    get  trump \n  3861   3517   3307   3124   2967   2584   1812   1684   1460   1367"
  },
  {
    "objectID": "Seminar/Seminar1.html#描述性统计",
    "href": "Seminar/Seminar1.html#描述性统计",
    "title": "Seminar1",
    "section": "",
    "text": "使用 ntoken() 函数来统计语料库中每条文本的词元数量。将该函数的输出赋值给一个新对象，以便后续使用:\n\nmft_n_words &lt;- ntoken(mft_corpus)\n\n使用 hist() 函数绘制直方图，以展示语料库中各文档长度（以词元数计）的分布情况。同时，使用 summary() 函数报告集中趋势的相关统计量:\n\nhist(mft_n_words)\n\n\nClick to show figure\n\n\n\nsummary(mft_n_words)\n Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   7.00   20.00   31.00   39.09   53.00  177.00 \n\nReddit 语料库中评论的中位长度为 31 个词。绘制的图形表明，该语料库中的文档长度分布呈正偏态（positively skewed），也就是说，大多数评论较短，分布的主体集中在左侧，而右尾较长，表明存在少量篇幅较长的评论。\n\n\n\n词典（dictionary）是具名列表（named list），由“键（key）”与一组对应条目组成，这些条目定义了该键的等价类。在 quanteda 中，词典可通过 dictionary() 函数创建。该函数的输入是一个列表（list），其中包含若干具名的字符向量（named character vectors）。\n例如，假设我们希望构建一个简单的词典，用于衡量与两门课程相关的词语：量化文本分析（quantitative text analysis）与因果推断（causal inference）。我们可以首先为每个概念创建一组相关词汇的向量，并将其存储在一个列表中：\nteaching_dictionary_list &lt;- list(qta = c(\"quantitative\", \"text\", \"analysis\", \"document\", \"feature\", \"matrix\"),\n                                 causal = c(\"potential\", \"outcomes\", \"framework\", \"counterfactual\"))\n然后我们将该向量传递给 dictionary() 函数：\nteaching_dictionary &lt;- dictionary(teaching_dictionary_list)\nteaching_dictionary\nDictionary object with 2 key entries.\n- [qta]:\n  - quantitative, text, analysis, document, feature, matrix\n- [causal]:\n  - potential, outcomes, framework, counterfactual\n我们当然可以扩展类别的数量，并为每个类别添加更多词语。\n在开始编码练习之前，请先查看 mft_dictionary.csv 文件。你认为每个道德基础所关联的词语是否合理？是否有某些词语看起来不太恰当？请记住，构建词典在很大程度上依赖主观判断，所选词汇的范围与内容会对你所进行的任何分析结果产生显著影响！\n\n从 mft_dictionary_words 数据中的词汇创建一个 quanteda 词典对象。你的词典应包含两个类别——一个对应 “care”(关心)，另一个对应 “sanctity”（神圣）：\n\nmft_dictionary_list &lt;- list(\n  care = mft_dictionary_words$word[mft_dictionary_words$foundation == \"care\"],\n  sanctity = mft_dictionary_words$word[mft_dictionary_words$foundation == \"sanctity\"]\n  )\n\nmft_dictionary &lt;- dictionary(mft_dictionary_list)\nmft_dictionary\nDictionary object with 2 key entries.\n- [care]:\n  - alleviate, alleviated, alleviates, alleviating, alleviation, altruism, altruist, beneficence, beneficiary, benefit, benefits, benefitted, benefitting, benevolence, benevolent, care, cared, caregiver, cares, caring [ ... and 444 more ]\n- [sanctity]:\n  - abstinance, abstinence, allah, almighty, angel, apostle, apostles, atone, atoned, atonement, atones, atoning, beatification, beatify, beatifying, bible, bibles, biblical, bless, blessed [ ... and 640 more ]\n\n使用你在上一个问题中创建的词典，并将其应用于你在本次作业前面创建的文档-特征矩阵。为此，你需要使用 dfm_lookup() 函数。(如果需要帮助，可以阅该函数的帮助文档?dfm_lookup）\n\nmft_dfm_dictionary &lt;- dfm_lookup(mft_dfm, mft_dictionary)\nmft_dfm_dictionary\n\n\nClick to show output\n\nDocument-feature matrix of: 17,886 documents, 2 features (78.13% sparse) and 8 docvars.\n       features\ndocs    care sanctity\n  text1    0        1\n  text2    1        0\n  text3    1        0\n  text4    1        0\n  text5    0        1\n  text6    0        0\n[ reached max_ndoc ... 17,880 more documents ]\n\n\n\n你刚刚创建的词典-文档矩阵（dictionary-dfm）记录了每条文本中与每个道德基础类别相关的词汇数量。然而，正如我们之前所见，并非所有文本的词数都相同。请创建该词典-文档矩阵的一个新版本，其中记录的是每条文本中与各道德基础类别相关的词汇占总词数的比例。\n\nmft_dfm_dictionary_proportions &lt;- mft_dfm_dictionary/mft_n_words\n\n将每个道德基础类别的词典得分存储为新的变量，添加到原始的 mft_texts 数据框中。你需要使用 as.numeric 函数，将 dfm 中的每一列强制转换为适合存储在 data.frame 中的格式:\n\nmft_texts$care_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,1])\nmft_texts$sanctity_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,2])\n\n请注意，这里的代码只是将应用词典后的输出结果赋值给 mft_texts 数据框。这只是为了使后续的分析步骤更为简便。\n\n\n\n\n现在我们已经构建好了词典指标，接下来将进行一些基本的效度检验。我们将从直接检查在各个道德基础类别中被词典赋予高分的文本开始。\n为此，我们需要根据词典分析所赋予的分值对文本进行排序。例如，对于 “care” 基础，可以使用如下代码：\nmft_texts$text[order(mft_texts$care_dictionary, decreasing = TRUE)][1:5]\n\n\n方括号运算符（[]）允许我们对 mft_texts$text 变量进行子集提取;\norder() 函数根据 mft_texts$care_dictionary 变量的数值对观测进行排序，参数 decreasing = TRUE 表示按从大到小的顺序排列。\n\n\n\n在 care 和 sanctity 两个道德基础类别中，词典得分最高的前 5 条文本分别是哪些？\n\nmft_texts$text[order(mft_texts$care_dictionary, decreasing = TRUE)][1:5]\n\n\nClick to show output\n\n[1] \"i doubt she'd want \\\"help\\\" from a childhood bully 🙄\"\n[2] \"We are talking about threats to human safety, not threats to property.\"\n[3] \"What guarantees adoptive parents would have been loving? That the child wouldn’t suffer\" \n[4] \"The dress design didn’t have anything to do with child sexual assault victims. https://www.papermag.com/alexander-mcqueen-dancing-girls-dress-2645945769.html\"\n[5] \"This right here. Threatening violence of any kind is not ok, sexual rape violence SO not ok.\"                                                                 \n\n\nmft_texts$text[order(mft_texts$sanctity_dictionary, decreasing = TRUE)][1:5]\n\n\nClick to show output\n\n[1] \"hot fucking damn macron this last bit was inspiring as hell\"   \n[2] \"Fuck you. Seriously, fuck you. Get your shit together you fucking junkie.\"\n[3] \"Hell yeah! That’s some hardcore nostalgia right there god DAMN.\"          \n[4] \"This is so fucking sad. I hate this god damned country\"         \n[5] \"Holy fuck. This makes OP TA forever. How fucking awful.\"                                                                         \n\n\n\n阅读与 care 和 sanctity 两个道德基础最强相关的文本。你认为这些词典是否准确捕捉了 “关怀” 与 “神圣” 这两个概念？\n\n\n在一些情况下，这些文本与相应的道德基础类别的关联似乎是合理的。例如，许多与 “care” 相关的文本确实涉及对他人伤害的描述。\n但在其他情况下，词典似乎捕捉到的内容并不准确。例如，所有与 “sanctity” 相关的高分文本几乎都是因为包含脏话。虽然咒骂行为可能在某种程度上与对神圣议题的道德敏感性相关，但它本身并不构成对“神圣”概念的实质性表达，因此这提示我们：sanctity 词典可能仍需改进。\n\n\nmft_texts 对象中包含了一系列变量，记录了每条文本由人工标注所归类的道德基础类别。请使用 table() 函数创建一个混淆矩阵 (confusion matrix)，以比较人工标注结果与词典分析生成的得分之间的一致性:\n\ncare_confusion &lt;- table( \n  dictionary = mft_texts$care_dictionary &gt; 0,\n  human_coding = mft_texts$care)\ncare_confusion\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 11248  2553\n     TRUE   1898  2187\nsanctity_confusion &lt;- table(\n  dictionary = mft_texts$sanctity_dictionary &gt; 0,\n  human_coding = mft_texts$sanctity)\nsanctity_confusion\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 13268   878\n     TRUE   2871   869\n\n对于每个道德基础类别，分类器的准确率是多少？（如果你忘记了如何计算，请查看Lecture讲义）\n\n# Care\n(2187 + 11248)/(2187 + 11248 + 2553 + 1898)\n[1] 0.7511461\n# Sanctity\n(869 + 13268)/(869 + 13268 + 2871 + 878)\n[1] 0.7903947\n\n对于每个道德基础类别，分类器的灵敏度（sensitivity）是多少？\n\n# Care\n(2187)/(2187 + 2553)\n[1] 0.4613924\n# Sanctity\n869/(869 + 878)\n[1] 0.4974242\n\n对于每个道德基础类别，分类器的特异度（specificity）是多少？\n\n# Care\n(11248)/(11248 + 1898)\n[1] 0.8556215\n# Sanctity\n13268/(13268 + 2871)\n[1] 0.8221079\n\n根据这些数值，我们的词典在分类任务中的表现如何？\n\n\n虽然整体的准确率（accuracy）看起来相对较高，但从灵敏度（sensitivity）得分来看，词典在识别真正的 “care” 和 “sanctity” 编码文本方面的表现其实并不理想。\n具体而言，词典对于这两个类别都未能识别出 50% 以上的真正阳性样本（true positives）。换句话说，虽然词典在多数情况下可能避免了错误分类，但却漏掉了大量真正具有“关怀”或“神圣”特征的文本。\n这种情况有些类似于一个安检系统，它在避免误报（不把无害物误判为危险品）方面做得不错，但却频繁漏检真正的危险品。这对研究者来说是一个警示：词典虽然提供了一种高效的量化方法，但在覆盖复杂语义和细微道德表达方面，可能存在着结构性不足。\n\n\n除了以上这种直接手算的方式，我们也可以使用 caret 程序包非常简便地计算这些统计量:\n\n# install.packages(\"caret\") # 只需要在第一次运行这行代码安装!\nlibrary(caret)\n\nconfusionMatrix(care_confusion, positive = \"TRUE\")\n\n\nClick to show output\n\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 11248  2553\n     TRUE   1898  2187\n                                          \n               Accuracy : 0.7511          \n                 95% CI : (0.7447, 0.7575)\n    No Information Rate : 0.735           \n    P-Value [Acc &gt; NIR] : 4.343e-07       \n                                          \n                  Kappa : 0.3317          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.4614          \n            Specificity : 0.8556          \n         Pos Pred Value : 0.5354          \n         Neg Pred Value : 0.8150          \n             Prevalence : 0.2650          \n         Detection Rate : 0.1223          \n   Detection Prevalence : 0.2284          \n      Balanced Accuracy : 0.6585          \n                                          \n       'Positive' Class : TRUE          \n\n\nconfusionMatrix(sanctity_confusion, positive = \"TRUE\")\n\n\nClick to show output\n\nConfusion Matrix and Statistics\n\n          human_coding\ndictionary FALSE  TRUE\n     FALSE 13268   878\n     TRUE   2871   869\n                                          \n               Accuracy : 0.7904          \n                 95% CI : (0.7844, 0.7963)\n    No Information Rate : 0.9023          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2118          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.49742         \n            Specificity : 0.82211         \n         Pos Pred Value : 0.23235         \n         Neg Pred Value : 0.93793         \n             Prevalence : 0.09767         \n         Detection Rate : 0.04859         \n   Detection Prevalence : 0.20910         \n      Balanced Accuracy : 0.65977         \n                                          \n       'Positive' Class : TRUE            \n                                                                                                                 \n\n\n\n\n\n在完成上述验证（尽管词典得分与人工编码的相关性相对较弱）之后，我们现在可以继续进行一个简单的应用。\n\n请计算数据中 11 个 subreddit 各自的 care 和 sanctity 词典得分的平均值：\n\ndictionary_means_by_subreddit &lt;- mft_texts %&gt;%\n  group_by(subreddit) %&gt;%\n  summarise(care_dictionary = mean(care_dictionary),\n            sanctity_dictionary = mean(sanctity_dictionary)) \n\ndictionary_means_by_subreddit\n\n\nClick to show output\n\n# A tibble: 11 × 3\n   subreddit           care_dictionary sanctity_dictionary\n                                           \n 1 AmItheAsshole               0.0118              0.00966\n 2 Conservative                0.00946             0.0101 \n 3 antiwork                    0.0106              0.0120 \n 4 confession                  0.0117              0.0127 \n 5 europe                      0.00488             0.00453\n 6 geopolitics                 0.00394             0.00137\n 7 neoliberal                  0.00430             0.00662\n 8 nostalgia                   0.00767             0.00959\n 9 politics                    0.00923             0.0108 \n10 relationship_advice         0.0127              0.0110 \n11 worldnews                   0.00651             0.00616\n                                                                  \n\n\n\ngroup_by() 是一个特殊函数，它允许我们根据指定变量的分组对数据进行操作（在这里，我们按 subreddit 分组，因为我们希望了解每个 subreddit 的词典得分均值）\nsummarise() 是一个函数，可用于对数据计算各种类型的汇总统计量。\n\n2.对你刚刚计算出的 subreddit 平均值进行解释，或许将这些信息以可视化图形形式展示出来更直观：\ndictionary_means_by_subreddit %&gt;%\n  # 将数据转换成\"long format\"以便于绘制图表\n  pivot_longer(-subreddit) %&gt;%\n  # 使用 ggplot\n  ggplot(aes(x = name, y = subreddit, fill = value)) + \n  # 使用 geom_tile 绘制 heatmap\n  geom_tile() + \n  # 修改颜色让它更美观\n  scale_fill_gradient(low = \"white\", high = \"purple\") + \n  # 去除坐标轴\n  xlab(\"\") + \n  ylab(\"\") \n\n\nClick to show figure\n\n\n\n\n“geopolitics” 子版块似乎几乎不包含与 sanctity（神圣） 相关的语言；而 “confession” 子版块则大量使用了与 sanctity 相关的词汇。\n与 care（关怀） 相关的语言在 “relationship_advice” 和 “AmItheAsshole” 子版块中则较为常见。\n\n\ncare 和 sanctity 的词典得分之间的相关性是多少？这两个道德基础是否彼此密切相关？\n\ncor(mft_texts$care_dictionary,mft_texts$sanctity_dictionary)\n[1] 0.03435137\n\n不，基于 care 与 sanctity 的语言之间的相关性非常低。"
  },
  {
    "objectID": "Seminar/Seminar1.html#课后思考",
    "href": "Seminar/Seminar1.html#课后思考",
    "title": "Seminar1",
    "section": "",
    "text": "（这一部分在正式授课中并不会直接提供代码，而会在相应seminar结束后的第二天更新代码和解决方案）\n\n请模仿seminar中的分析过程，将其应用到其余三个道德基础类别上：fairness、loyalty 和 authority。然后，绘制一张图表，展示每个 subreddit 在每个道德基础类别上的平均词典得分。\n\nmft_dictionary_list &lt;- list(\n  care = mft_dictionary_words$word[mft_dictionary_words$foundation == \"care\"],\n  sanctity = mft_dictionary_words$word[mft_dictionary_words$foundation == \"sanctity\"],\n  authority = mft_dictionary_words$word[mft_dictionary_words$foundation == \"authority\"],\n  fairness = mft_dictionary_words$word[mft_dictionary_words$foundation == \"fairness\"],\n  loyalty = mft_dictionary_words$word[mft_dictionary_words$foundation == \"loyalty\"]\n  )\n\nmft_dictionary &lt;- dictionary(mft_dictionary_list)\n\nmft_dfm_dictionary &lt;- dfm_lookup(mft_dfm, mft_dictionary)\n\nmft_dfm_dictionary_proportions &lt;- mft_dfm_dictionary/mft_n_words\n\nmft_texts$care_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,1])\nmft_texts$sanctity_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,2])\nmft_texts$authority_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,3])\nmft_texts$fairness_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,4])\nmft_texts$loyalty_dictionary &lt;- as.numeric(mft_dfm_dictionary_proportions[,5])\n\ndictionary_means_by_subreddit &lt;- mft_texts %&gt;%\n  group_by(subreddit) %&gt;%\n  summarise(care_dictionary = mean(care_dictionary),\n            sanctity_dictionary = mean(sanctity_dictionary),\n            authority_dictionary = mean(authority_dictionary),\n            fairness_dictionary = mean(fairness_dictionary),\n            loyalty_dictionary = mean(loyalty_dictionary)) \n\n\ndictionary_means_by_subreddit %&gt;%\n  # Transform the data to \"long\" format for plotting\n  pivot_longer(-subreddit) %&gt;%\n  # Use ggplot\n  ggplot(aes(x = name, y = subreddit, fill = value)) + \n  # geom_tile creates a heatmap\n  geom_tile() + \n  # change the colours to make them prettier\n  scale_fill_gradient(low = \"white\", high = \"purple\") + \n  # remove the axis labels\n  xlab(\"\") + \n  ylab(\"\") \n\n\nClick to show figure"
  },
  {
    "objectID": "Seminar/Seminar3.html",
    "href": "Seminar/Seminar3.html",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "讲座人：Hanxu hanxu.dong.21@ucl.ac.uk\n\n Seminar data \n\n\n政府提供的医疗服务让民众有多满意/不满意？关于这个问题的一个有趣的数据来源是文本数据，因为医疗服务的使用者通常可以对他们在接受政府提供的医疗服务时的体验进行评论和反馈。\n在本次研讨课中，我们将使用朴素贝叶斯模型（Naive Bayes）来预测英国患者对全科诊所（doctors’ surgeries）的评论是正面的还是负面的。为此，我们将使用一份患者对居住地附近英国国家医疗服务体系（NHS）设施的评论语料库。\n\n\n\n在开始研讨会时，先下载/加载以下 R 包：\nlibrary(tidyverse)\nlibrary(quanteda)\n\n# 如果无法加载 quanteda.textmodels 包，请运行以下代码\n# 仅适用于 Windows 用户，Mac 用户请查看下方注释说明\n# remotes::install_github(\"quanteda/quanteda.textmodels\")\n\nlibrary(quanteda.textmodels)\n\n# 如果无法加载 caret 包，请运行以下代码\n# install.packages(\"caret\")\n\nlibrary(caret)\n\n\n\n对于 Mac 用户，当前使用 install.packages() 或 remotes::install_github() 安装 quanteda.textmodels 包可能会遇到问题。因此，你需要使用以下（稍显繁琐的）替代方案：\n\n第一步：安装 gfortran 在你的计算机上安装 gfortran，你可以从以下链接找到与你系统匹配的版本：🔗 https://github.com/fxcoudert/gfortran-for-macOS/releases 如果提示你没有完整的编译工具集，请按照提示安装 Xcode 命令行工具。\n第二步：在 R 中运行以下代码：\n\ndir.create('~/.R')\nfile.create('~/.R/Makevars')\n\n第三步：定位 Makevars 文件。要定位相关文件，请通过 Finder 前往你的主目录（你可以打开 Finder 然后按下 ⇧⌘H）。如果你看不到 .R 文件夹，首先需要按下 ⇧⌘.（Shift + Command + 点号），以显示你 Mac 上的隐藏文件。找到 .R 文件夹并打开，在其中应该可以看到一个名为 Makevars 的文件。\n完成后，将以下内容复制粘贴到该文件中并保存：\n\nFC = /opt/homebrew/Cellar/gcc/14.2.0/bin/gfortran\nF77 = /opt/homebrew/Cellar/gcc/14.2.0/bin/gfortran\nFLIBS = -L/opt/homebrew/Cellar/gcc/14.2.0/lib/gcc/11\n\n第四步：现在重启 R。\n第五步：接着运行以下代码：\n\nremotes::install_github(\"quanteda/quanteda.textmodels\")\n\n\n\n我们今天的研讨会将使用以下数据集：\nNHS 患者评论 – nhs_reviews.Rdata 该文件包含了英国各地 2000 条关于 NHS 全科诊所的患者评论。数据中包含以下变量：\n\n\n\n\n\n\n\n变量名\n说明\n\n\n\n\nreview_title\n患者评论的标题\n\n\nreview_text\n患者评论的正文\n\n\nstar_rating\n患者给出的星级评分（满分为 5 星）\n\n\nreview_positive\n分类变量：若评分为 3 星及以上则为 \"Positive\"，否则为 \"Negative\"\n\n\nreview_date\n评论的日期\n\n\ngp_response\n分类变量：表示医生是否回复了该评论。若回复则为 \"Responded\"，否则为 \"Has not responded\"\n\n\n\n一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：\nload(\"nhs_reviews.Rdata\")\n与以往一样，你可以使用 tidyverse 软件包中的 glimpse() 函数快速查看数据中的变量：\nglimpse(nhs_reviews)\nRows: 2,000\nColumns: 6\n$ review_title    &lt;chr&gt; \"Great service\", \"Friendly helpful staff\", \"Excellent …\n$ review_text     &lt;chr&gt; \"Phoned up to book a appointment receptionist very hel…\n$ star_rating     &lt;dbl&gt; 4, 5, 5, 1, 1, 5, 5, 5, 5, 1, 3, 5, 2, 5, 2, 5, 1, 1, …\n$ review_positive &lt;fct&gt; Positive, Positive, Positive, Negative, Negative, Posi…\n$ review_date     &lt;date&gt; 2021-10-13, 2021-07-26, 2021-09-18, 2021-06-19, 2021-…\n$ gp_response     &lt;chr&gt; \"Has not responded\", \"Responded\", \"Has not responded\",…  \n\n\n\n本次研讨课需要你随机生成训练集和测试集。任何涉及随机数生成的过程，都会导致你每次运行代码时可能得到不同的结果。例如，重新运行代码时，被分配到测试集和训练集的观测值可能会发生变化。\n因此，为了使结果完全可重复，你需要首先使用 set.seed() 函数。该函数的参数是一个单独的整数值。一旦你设置了seed，R 每次运行时都会生成相同的随机数序列，从而保证结果一致:\nset.seed(12345)\n我在这里使用了 12345，但你可以选择任何你喜欢的数值。如果你选择了不同的数字，那么你的结果可能会与我的略有不同。不过，只要你在每次运行 R 脚本时执行这一行代码，你每次都会得到相同的结果！\n\n\n\n\n在 nhs_reviews 数据中创建一个新变量，用于指示每条观测属于训练集还是测试集。确保大约 80% 的观测被分配到训练集，20% 被分配到测试集，将该变量命名为 train：\n\n\n\nReveal Code\n\nnhs_reviews$train &lt;- sample(x = c(TRUE, FALSE), \n                            size = nrow(nhs_reviews), \n                            replace = TRUE, \n                            prob = c(.8, .2))\n\nsample() 函数会从提供给 x 参数的元素中随机抽样。在这里，我们是从一个只有两个元素的向量 TRUE 和 FALSE 中进行抽样。\n\n\n\nsize 参数指定从 x 中要随机选择的元素数量，这里我们要求抽样的数量与 nhs_reviews 数据中的观测值数量相同；\nreplace 参数指定是否放回抽样（这里我们要进行放回抽样）；\nprob 参数接受一个向量，表示我们希望以何种概率抽取 x 中的每个值。\n\n\n\n\n将 nhs_reviews 数据转换为语料库`corpus，然后转换为文档-特征矩阵DFM, 并进行一定的特征选择:\n\n\n\nReveal Code\n\n# 转换为语料库\nnhs_reviews_corpus &lt;- corpus(nhs_reviews, text_field = \"review_text\")\n\n# 转换为 DFM，并移除英文停用词\nnhs_reviews_dfm &lt;- nhs_reviews_corpus %&gt;% \n  tokens() %&gt;%\n  dfm() %&gt;%\n  dfm_remove(stopwords(\"en\"))\n\n请记住，特征选择没有唯一“正确”的方式。在这里，我只是使用了unigram（单词）表示，并移除了停用词。你可以根据实际任务做出不同的选择。\n\n\n\n使用 dfm_subset() 函数将你的 DFM 拆分为训练集 DFM 和 测试集 DFM。 使用 dim() 函数确认它们是否具有你预期的行数和列数:\n\n\n\nReveal Code\n\n# 子集为训练集观测\nnhs_reviews_dfm_train &lt;- dfm_subset(nhs_reviews_dfm, train)\n\n# 子集为测试集观测（使用逻辑非 !）\nnhs_reviews_dfm_test &lt;- dfm_subset(nhs_reviews_dfm, !train)\n\n这里我使用了逻辑运算符 !，它会反转 train 向量的 TRUE 和 FALSE 值。也就是说，train 中为 TRUE 的元素在加上 ! 后变为 FALSE，为 FALSE 的元素变为 TRUE。\n\ndim(nhs_reviews_dfm_train)\n[1] 1578 7862  \ndim(nhs_reviews_dfm_test)\n[1]  422 7862\n\n我们的 DFM 拆分结果中，训练集和测试集包含不同数量的文档（这是符合预期的，因为我们之前设定了 80% 用于训练，20% 用于测试），但它们包含相同数量的特征（这也是必须的，因为我们要使用这些特征来为测试集生成预测）。\n\n\n\n\n\n\n使用 dictionary() 函数构建一个简单的字典，其中包含一些你认为能够体现患者评论中积极情绪的词汇。将这个字典应用到你之前创建的训练集 DFM上，并创建一个二元变量：当文本中包含来自该字典的词时设为 Positive，否则设为 Negative:\n\n\n\nReveal Code\n\npositive_dictionary &lt;- dictionary(list(positive = c(\"great\", \"excellent\", \"fantastic\", \"thank\")))\n\n# 将字典应用于训练集 DFM\nnhs_dictionary_dfm_train &lt;- dfm_lookup(x = nhs_reviews_dfm_train,\n                                       dictionary = positive_dictionary)\n\n# 创建一个逻辑（TRUE/FALSE）向量，对每条训练文本进行标记\npredicted_positive &lt;- as.numeric(nhs_dictionary_dfm_train[,1]) &gt; 0\n\n# 将逻辑向量转换为字符向量，TRUE 赋值为 \"Positive\"，FALSE 赋值为 \"Negative\"\n# 并将结果赋值给训练集数据\nnhs_dictionary_dfm_train$predicted_positive_dictionary &lt;- ifelse(predicted_positive, \"Positive\", \"Negative\")\n\n\n使用 table() 函数创建一个混淆矩阵(confusion matrix)，用于比较通过字典度量预测的正面评论 (predicted_positive_dictionary) 与正面评论的真实编码（存储在 review_positive 变量中）。将 table() 函数的输出保存为一个新对象:\n\n\n\nReveal Code\n\nconfusion_dictionary &lt;- table(predicted_classification = nhs_dictionary_dfm_train$predicted_positive_dictionary,\n                              true_classification = nhs_dictionary_dfm_train$review_positive)\n\nconfusion_dictionary\n                        true_classification\npredicted_classification Negative Positive\n                Negative      542      569\n                Positive       36      421\n\n\n使用 caret 包中的 confusionMatrix() 函数来计算关于你的字典分类器的多种性能统计量。（你还应设置 positive 参数为 “Positive”，以告知 R 哪个结果水平对应于“正面”结果。）预测的准确率是多少？灵敏度是多少？特异性是多少？解释这些数值。它们告诉我们这个字典在分类正面患者评论方面的性能如何？\n\n\n\nReveal Code\n\nconfusion_dictionary_statistics &lt;- confusionMatrix(confusion_dictionary, \n                                                   positive = \"Positive\")\n\nconfusion_dictionary_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      542      569\n                Positive       36      421\n                                          \n               Accuracy : 0.6142          \n                 95% CI : (0.5895, 0.6383)\n    No Information Rate : 0.6314          \n    P-Value [Acc &gt; NIR] : 0.9247          \n                                          \n                  Kappa : 0.3045          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.4253          \n            Specificity : 0.9377          \n         Pos Pred Value : 0.9212          \n         Neg Pred Value : 0.4878          \n             Prevalence : 0.6314          \n         Detection Rate : 0.2685          \n   Detection Prevalence : 0.2915          \n      Balanced Accuracy : 0.6815          \n                                          \n       'Positive' Class : Positive     \n\n我们的字典预测的准确率为 61%。虽然特异性很高——达到 94%——但这是因为字典将绝大多数评论都归为非正面评论，因此正确地为真实的非正面评论分配了该标签。灵敏度则低得多，仅为 43%。也就是说，我们只正确地识别了不到一半的真实“正面”评论。\n\n\n这是可以理解的，因为我们上面使用的字典非常简短，因此很可能漏掉了许多能表示正面情绪的词汇。\n\n\n我们可以尝试通过修改字典词列表并重新测试来提升这些分数，但接下来我们将使用朴素贝叶斯分类模型，让模型学习哪些词与正面或负面患者评论最密切相关。\n\n\n\n\n\n\n使用你之前创建的训练集 DFM，通过 textmodel_nb() 函数来估计一个针对 review_positive 结果变量的朴素贝叶斯模型。\n\n该模型有三个主要参数：\n\n\n\n\n\n\n\n参数名\n说明\n\n\n\n\nx\n用于训练朴素贝叶斯模型的 DFM（文档-特征矩阵）\n\n\ny\n要预测的结果向量（例如，此处为 review_positive）\n\n\nprior\n对结果变量 y 的各类别的先验分布形式。通常设为 \"docfreq\"，即每个类别的先验概率将等于该类别在训练数据中的相对频率\n\n\n\n\n\nReveal Code\n\nnb_train &lt;- textmodel_nb(x = nhs_reviews_dfm_train, \n                         y = nhs_reviews_dfm_train$review_positive,\n                         prior = \"docfreq\")\n\n\n检查你所估计的朴素贝叶斯模型中的类别条件词概率（class-conditional word probabilities）。（此处使用 sort() 函数会很有帮助。）你可以使用 coef() 函数对估计得到的模型对象进行调用来访问这些概率。思考以下问题：在每个类别（正面 / 负面）下，哪些词的出现概率最高？请注意，如果你在研讨会一开始做了不同的特征选择决策，你得到的词列表可能会有所不同。特别是，如果你没有移除停用词（stopwords），你会发现两个类别下都对这些词赋予了高概率 —— 这也是合理的，因为这些词在所有患者评论中都很常见。\n\n\n\nReveal Code\n\nhead(sort(coef(nb_train)[,\"Positive\"], decreasing = T), 20)\n          .           ,       staff    practice     surgery appointment \n0.075829687 0.037050475 0.009988262 0.009241276 0.008921140 0.007619251 \n     always          gp     service           !        time     helpful \n0.006061253 0.006061253 0.005997225 0.005890513 0.005762459 0.005719774 \n      thank      doctor        care        well     doctors    friendly \n0.005463664 0.004652652 0.004545940 0.004481912 0.004183118 0.004076406 \n  reception         get \n0.003670900 0.003606872    \nhead(sort(coef(nb_train)[,\"Negative\"], decreasing = T), 20)\n           .            ,  appointment          get         call      surgery \n 0.067243057  0.032460640  0.012294357  0.011874226  0.009021758  0.008667964 \n           !         told        phone     practice         time           gp \n 0.008093048  0.007584468  0.007473908  0.006147178  0.006036618  0.005771272 \n      doctor          day         back          one          can         just \n 0.004908898  0.004555103  0.004333982  0.004245533  0.004134973  0.004134973 \nappointments            ? \n 0.004112860  0.004046524    \n\n这些词大体上是合理的。在正面和负面评论中，患者都可能使用与医疗相关的词汇，例如 “staff”（工作人员）、“practice”（诊所）、“surgery”（手术/诊所）、“appointment”（预约）、“doctor”（医生）等。但在 “Positive”（正面）类别中，语言模型还对许多具有正向情感的词赋予了更高的概率，如 “thank”（感谢）、“helpful”（有帮助的）、“friendly”（友好的）。\n\n\n相比之下，在 “Negative”（负面）类别中，模型对一些可能与不满意服务相关的词赋予了更高的概率，例如 “time”（时间）、“service”（服务）、“day”（日子），以及 “!”（感叹号）。\n\n\n\n估计来自朴素贝叶斯模型的正面评论预测概率。使用这些概率来检查 review_title 变量：哪些患者评论标题最有可能是正面评论？哪些最有可能是负面评论？这些看起来合理吗？\n\n\n\nReveal Code\n\nnhs_reviews_dfm_train$positive_nb_probability &lt;- predict(nb_train, type = \"probability\")[,2]\n\nnhs_reviews_dfm_train$review_title[order(nhs_reviews_dfm_train$positive_nb_probability, decreasing = T)[1:10]]\n [1] \"Good GP Practice\"                    \"Efficient and compassionate service\"\n [3] \"Fantastic practice\"                  \"Caring and professional service\"    \n [5] \"Nurse was amazing\"                   \"Fantastic Practice\"                 \n [7] \"Consistently good care\"              \"Exemplary Practice\"                 \n [9] \"Fantastic practice\"                  \"Fantastic Practice\"   \nnhs_reviews_dfm_train$review_title[order(nhs_reviews_dfm_train$positive_nb_probability, decreasing = F)[1:10]]\n [1] \"Shambles\"                                    \n [2] \"Unacceptable practice\"                       \n [3] \"Impossible to get a service\"                 \n [4] \"Unprofessional, Poor and Dangerous\"          \n [5] \"Impossible to get an appointment!!!\"         \n [6] \"Honestly, you’re better off not having a GP\" \n [7] \"Disappointed with appointments.\"             \n [8] \"Not so great practice!!\"                     \n [9] \"Very bad practice and arrogant receptionists\"\n[10] \"Extremely rude receptionist\"   \n\n是的！从这些列表可以非常清楚地看出，该模型在区分正面和负面评论方面表现得相当不错。但若要确切了解其表现如何，我们需要做的不仅仅是表面效度检验。\n\n\n\n使用你拟合好的朴素贝叶斯模型对训练数据中的每条观测进行类别预测（正面或负面）。然后使用 table() 函数创建一个混淆矩阵，用于比较预测出的正面评论与真实的正面评论编码（存储在 review_positive 变量中）。与字典分析一样，将 table() 函数的输出保存为一个新对象。\n\n\n\nReveal Code\n\n## 训练数据集准确率\nnhs_reviews_dfm_train$predicted_classification_nb &lt;- predict(nb_train, type = \"class\")\n\nconfusion_train &lt;- table(predicted_classification = nhs_reviews_dfm_train$predicted_classification_nb, \n                         true_classification = nhs_reviews_dfm_train$review_positive)\n\nconfusion_train\n                        true_classification\npredicted_classification Negative Positive\n                Negative      565       59\n                Positive       13      931  \n\n\n使用 confusionMatrix() 函数计算你的分类器的准确率（accuracy）、灵敏度（sensitivity） 和特异性（specificity）。将这些得分与你从字典分析中获得的结果进行比较。哪种方法表现更好？\n\n\n\nReveal Code\n\nconfusion_train_statistics &lt;- confusionMatrix(confusion_train, \n                                              positive = \"Positive\")\n\nconfusion_train_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      565       59\n                Positive       13      931\n                                          \n               Accuracy : 0.9541          \n                 95% CI : (0.9425, 0.9639)\n    No Information Rate : 0.6314          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.903           \n                                          \n Mcnemar's Test P-Value : 1.137e-07       \n                                          \n            Sensitivity : 0.9404          \n            Specificity : 0.9775          \n         Pos Pred Value : 0.9862          \n         Neg Pred Value : 0.9054          \n             Prevalence : 0.6314          \n         Detection Rate : 0.5938          \n   Detection Prevalence : 0.6020          \n      Balanced Accuracy : 0.9590          \n                                          \n       'Positive' Class : Positive   \n\n朴素贝叶斯模型显然优于字典分析。该分类器的灵敏度现在为 94%，准确率为 95%。使用朴素贝叶斯方法，我们在训练集中检测情感的表现要好得多。\n\n\n\n重复评估分类器的准确性，但现在要评估测试数据集。与训练数据集相比，分类器在这些数据上的表现如何？\n\n\n\nReveal Code\n\n## 测试数据集准确性\n\nnhs_reviews_dfm_test$predicted_classification_nb &lt;- predict(nb_train, \n                                                  newdata = nhs_reviews_dfm_test, \n                                                  type = \"class\")\n\nconfusion_test &lt;- table(predicted_classification = nhs_reviews_dfm_test$predicted_classification_nb, \n                        true_classification = nhs_reviews_dfm_test$review_positive)\n\nconfusion_test_statistics &lt;- confusionMatrix(confusion_test, \n                                             positive = \"Positive\")\n\nconfusion_test_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      170       35\n                Positive        6      207\n                                          \n               Accuracy : 0.9019          \n                 95% CI : (0.8693, 0.9287)\n    No Information Rate : 0.5789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8032          \n                                          \n Mcnemar's Test P-Value : 1.226e-05       \n                                          \n            Sensitivity : 0.8554          \n            Specificity : 0.9659          \n         Pos Pred Value : 0.9718          \n         Neg Pred Value : 0.8293          \n             Prevalence : 0.5789          \n         Detection Rate : 0.4952          \n   Detection Prevalence : 0.5096          \n      Balanced Accuracy : 0.9106          \n                                          \n       'Positive' Class : Positive \n\n测试数据集上的准确率为 90%，灵敏度为 86%，特异度为 97%。测试数据集上的表现比训练数据集上的表现要差一些，但仍明显优于字典分析。\n\n\n\n\n\n\n从 nhs_reviews 数据中创建两个新的 DFM，每次都做出不同的特征选择决策（例如：词频修剪（trimming）、使用 n-gram、是否移除停用词等）。使用这些 DFM 训练两个新的朴素贝叶斯模型，并将它们在测试集数据上的表现与你之前创建的模型进行比较。 创建一个表格，比较你所估算模型的结果，和你为每个模型所做的特征选择决策。哪个模型在准确性、灵敏度和特异性方面表现最好？\n\n\n\nReveal Code\n\n## 无标点的 DFM\n\n# 转换为 DFM（移除标点符号）\nnhs_reviews_dfm_nopunct &lt;- nhs_reviews_corpus %&gt;% \n  tokens(remove_punct = T) %&gt;%\n  dfm() \n\n# 提取训练集中的观测值\nnhs_reviews_dfm_nopunct_train &lt;- dfm_subset(nhs_reviews_dfm_nopunct, train)\n\n# 提取测试集中的观测值\nnhs_reviews_dfm_nopunct_test &lt;- dfm_subset(nhs_reviews_dfm_nopunct, !train)\n\n# 训练一个新的朴素贝叶斯模型\nnb_train_nopunct &lt;- textmodel_nb(x = nhs_reviews_dfm_nopunct_train, \n                         y = nhs_reviews_dfm_nopunct_train$review_positive,\n                         prior = \"docfreq\")\n\n## 测试集准确率\n\n# 使用模型对测试集进行预测\nnhs_reviews_dfm_nopunct_test$predicted_classification &lt;- predict(nb_train_nopunct, \n                                                  newdata = nhs_reviews_dfm_nopunct_test, \n                                                  type = \"class\")\n\n# 创建混淆矩阵：预测类别 vs 真实类别\nconfusion_test_nopunct &lt;- table(predicted_classification = nhs_reviews_dfm_nopunct_test$predicted_classification, \n                        true_classification = nhs_reviews_dfm_nopunct_test$review_positive)\n\n# 计算混淆矩阵的性能统计（准确率、灵敏度、特异性等）\nconfusion_test_nopunct_statistics &lt;- confusionMatrix(confusion_test_nopunct, positive = \"Positive\")\n\n# 输出统计结果\nconfusion_test_nopunct_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      169       34\n                Positive        7      208\n                                          \n               Accuracy : 0.9019          \n                 95% CI : (0.8693, 0.9287)\n    No Information Rate : 0.5789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8029          \n                                          \n Mcnemar's Test P-Value : 4.896e-05       \n                                          \n            Sensitivity : 0.8595          \n            Specificity : 0.9602          \n         Pos Pred Value : 0.9674          \n         Neg Pred Value : 0.8325          \n             Prevalence : 0.5789          \n         Detection Rate : 0.4976          \n   Detection Prevalence : 0.5144          \n      Balanced Accuracy : 0.9099          \n                                          \n       'Positive' Class : Positive  \n## N-gram 模型\n\n# 转换为 DFM（使用 1 到 3 元的 n-gram 特征）\nnhs_reviews_dfm_ngram &lt;- nhs_reviews_corpus %&gt;% \n  tokens() %&gt;%\n  tokens_ngrams(1:3) %&gt;%\n  dfm()\n\n# 提取训练集中的观测值\nnhs_reviews_dfm_ngram_train &lt;- dfm_subset(nhs_reviews_dfm_ngram, train)\n\n# 提取测试集中的观测值\nnhs_reviews_dfm_ngram_test &lt;- dfm_subset(nhs_reviews_dfm_ngram, !train)\n\n# 训练一个新的朴素贝叶斯模型\nnb_train_ngram &lt;- textmodel_nb(x = nhs_reviews_dfm_ngram_train, \n                         y = nhs_reviews_dfm_ngram_train$review_positive,\n                         prior = \"docfreq\")\n\n## 测试集准确率\n\n# 使用模型对测试集进行预测\nnhs_reviews_dfm_ngram_test$predicted_classification &lt;- predict(nb_train_ngram, \n                                                  newdata = nhs_reviews_dfm_ngram_test, \n                                                  type = \"class\")\n\n# 创建混淆矩阵：预测类别 vs 真实类别\nconfusion_test_ngram &lt;- table(predicted_classification = nhs_reviews_dfm_ngram_test$predicted_classification, \n                        true_classification = nhs_reviews_dfm_ngram_test$review_positive)\n\n# 计算混淆矩阵的性能统计（准确率、灵敏度、特异性等）\nconfusion_test_ngram_statistics &lt;- confusionMatrix(confusion_test_ngram, positive = \"Positive\")\n\n# 输出统计结果\nconfusion_test_ngram_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      169       35\n                Positive        7      207\n                                          \n               Accuracy : 0.8995          \n                 95% CI : (0.8666, 0.9266)\n    No Information Rate : 0.5789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7983          \n                                          \n Mcnemar's Test P-Value : 3.097e-05       \n                                          \n            Sensitivity : 0.8554          \n            Specificity : 0.9602          \n         Pos Pred Value : 0.9673          \n         Neg Pred Value : 0.8284          \n             Prevalence : 0.5789          \n         Detection Rate : 0.4952          \n   Detection Prevalence : 0.5120          \n      Balanced Accuracy : 0.9078          \n                                          \n       'Positive' Class : Positive       \naccuracy &lt;- c(confusion_test_statistics$overall[1],\n              confusion_test_nopunct_statistics$overall[1],\n              confusion_test_ngram_statistics$overall[1])\n\nsensitivity &lt;- c(confusion_test_statistics$byClass[1],\n                 confusion_test_nopunct_statistics$byClass[1],\n                 confusion_test_ngram_statistics$byClass[1])\n\nspecificity &lt;- c(confusion_test_statistics$byClass[2],\n                 confusion_test_nopunct_statistics$byClass[2],\n                 confusion_test_ngram_statistics$byClass[2])\n\nmodel_names &lt;- c(\"Unigram\", \"No punctuation\", \"N-gram\")\n\nresults &lt;- data.frame(model_names,\n                      accuracy,\n                      sensitivity,\n                      specificity)\n\nresults                                 \n     model_names  accuracy sensitivity specificity\n1        Unigram 0.9019139   0.8553719   0.9659091\n2 No punctuation 0.9019139   0.8595041   0.9602273\n3         N-gram 0.8995215   0.8553719   0.9602273  \n\n不同特征选择方法之间的差异很小。这主要是因为预测积极性是一项非常简单的任务！表示积极性的词语非常清晰，因此我们可以从这些数据中获得强烈的信号。需要注意的是，情况并非总是如此，尤其是在我们试图对一个更细微的概念或一组更精细的类别进行分类时。\n\n\n\n💪困难任务：在这个问题中，你应该实现 k 折交叉验证(k-fold cross-validation)，以评估你在上面估计的一个朴素贝叶斯模型的准确率、灵敏度和特异性。\n\n\n我在下面提供了一些起始代码，用于一个函数，该函数将接受一个逻辑值向量作为输入，该向量应命名为 held_out。该向量应对交叉验证中每一折的保留集观测为 TRUE，训练集观测为 FALSE。\n\n你的任务是填写代码中 “你的代码在这里！” 的部分。\n\n\nReveal Code\n\nget_performance_scores &lt;- function(held_out){\n  \n  # 为该折叠设置训练集和测试集\n  dfm_train &lt;- dfm_subset(nhs_reviews_dfm, !held_out)\n  dfm_test &lt;- dfm_subset(nhs_reviews_dfm, held_out)\n  \n  # 在非 held-out 部分训练模型\n  \n      # 你的代码在这里！\n  \n  # 对 held-out 部分进行预测\n  \n      # 你的代码在这里！\n  \n  # 创建混淆矩阵,计算准确性、特异性和灵敏度\n  \n      # 你的代码在这里！\n\n  # 将结果保存在 data.frame 中\n  \n  return(output)\n  \n}  \nget_performance_scores &lt;- function(held_out){\n  \n  # 为该折叠设置训练集和测试集\n  dfm_train &lt;- dfm_subset(nhs_reviews_dfm, !held_out)\n  dfm_test &lt;- dfm_subset(nhs_reviews_dfm, held_out)\n  \n    # 在非 held-out 部分训练模型\n  nb_train &lt;- textmodel_nb(x = dfm_train, \n                         y = dfm_train$review_positive,\n                         prior = \"docfreq\")\n  \n   # 对 held-out 部分进行预测\n  dfm_test$predicted_classification &lt;- predict(nb_train, \n                                             newdata = dfm_test, \n                                             type = \"class\")\n  \n  # 创建混淆矩阵,计算准确性、特异性和灵敏度\n  confusion_nb &lt;- table(predicted_classification = dfm_test$predicted_classification,\n                        true_classification = dfm_test$review_positive)\n  \n  confusion_nb_statistics &lt;- confusionMatrix(confusion_nb, positive = \"Positive\")\n  \n  accuracy &lt;- confusion_nb_statistics$overall[1]\n  sensitivity &lt;- confusion_nb_statistics$byClass[1]\n  specificity &lt;- confusion_nb_statistics$byClass[2]\n  \n  return(data.frame(accuracy, sensitivity, specificity))\n  \n}         \n\n\n完成这个函数之后，你需要创建一个向量，用来表示你将在交叉验证中测试数据所使用的各个折（K folds）。为此，你需要使用 sample() 函数生成一个向量，该向量的元素数量应与 nhs_reviews_dfm 对象中的行数相同。\n\n\n\nReveal Code\n\nK &lt;- 10\nfolds &lt;- sample(1:K, nrow(nhs_reviews_dfm), replace = T)\n\n\n最后，你需要将 get_performance_scores() 函数应用到每一个折（fold）上。为此，lapply() 函数可能会对你有所帮助。这个函数将一个向量作为输入，对该向量的每个值应用一个函数，并返回一个列表对象，其中包含你输入向量每个值所对应的函数运行结果。\n\n\n\nReveal Code\n\nall_folds &lt;- lapply(1:K, function(k) get_performance_scores(folds==k))\n\n\n输出你的结果！\n\n\n\nReveal Code\n\nall_folds &lt;- lapply(1:K, function(k) get_performance_scores(folds==k))\ncolMeans(bind_rows(all_folds))\n   accuracy sensitivity specificity \n  0.9122027   0.8881802   0.9504991"
  },
  {
    "objectID": "Seminar/Seminar3.html#对-nhs-评论进行分类",
    "href": "Seminar/Seminar3.html#对-nhs-评论进行分类",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "政府提供的医疗服务让民众有多满意/不满意？关于这个问题的一个有趣的数据来源是文本数据，因为医疗服务的使用者通常可以对他们在接受政府提供的医疗服务时的体验进行评论和反馈。\n在本次研讨课中，我们将使用朴素贝叶斯模型（Naive Bayes）来预测英国患者对全科诊所（doctors’ surgeries）的评论是正面的还是负面的。为此，我们将使用一份患者对居住地附近英国国家医疗服务体系（NHS）设施的评论语料库。"
  },
  {
    "objectID": "Seminar/Seminar3.html#packages",
    "href": "Seminar/Seminar3.html#packages",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "在开始研讨会时，先下载/加载以下 R 包：\nlibrary(tidyverse)\nlibrary(quanteda)\n\n# 如果无法加载 quanteda.textmodels 包，请运行以下代码\n# 仅适用于 Windows 用户，Mac 用户请查看下方注释说明\n# remotes::install_github(\"quanteda/quanteda.textmodels\")\n\nlibrary(quanteda.textmodels)\n\n# 如果无法加载 caret 包，请运行以下代码\n# install.packages(\"caret\")\n\nlibrary(caret)"
  },
  {
    "objectID": "Seminar/Seminar3.html#mac-用户加载-quanteda.textmodels-遇到问题的说明",
    "href": "Seminar/Seminar3.html#mac-用户加载-quanteda.textmodels-遇到问题的说明",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "对于 Mac 用户，当前使用 install.packages() 或 remotes::install_github() 安装 quanteda.textmodels 包可能会遇到问题。因此，你需要使用以下（稍显繁琐的）替代方案：\n\n第一步：安装 gfortran 在你的计算机上安装 gfortran，你可以从以下链接找到与你系统匹配的版本：🔗 https://github.com/fxcoudert/gfortran-for-macOS/releases 如果提示你没有完整的编译工具集，请按照提示安装 Xcode 命令行工具。\n第二步：在 R 中运行以下代码：\n\ndir.create('~/.R')\nfile.create('~/.R/Makevars')\n\n第三步：定位 Makevars 文件。要定位相关文件，请通过 Finder 前往你的主目录（你可以打开 Finder 然后按下 ⇧⌘H）。如果你看不到 .R 文件夹，首先需要按下 ⇧⌘.（Shift + Command + 点号），以显示你 Mac 上的隐藏文件。找到 .R 文件夹并打开，在其中应该可以看到一个名为 Makevars 的文件。\n完成后，将以下内容复制粘贴到该文件中并保存：\n\nFC = /opt/homebrew/Cellar/gcc/14.2.0/bin/gfortran\nF77 = /opt/homebrew/Cellar/gcc/14.2.0/bin/gfortran\nFLIBS = -L/opt/homebrew/Cellar/gcc/14.2.0/lib/gcc/11\n\n第四步：现在重启 R。\n第五步：接着运行以下代码：\n\nremotes::install_github(\"quanteda/quanteda.textmodels\")"
  },
  {
    "objectID": "Seminar/Seminar3.html#数据",
    "href": "Seminar/Seminar3.html#数据",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "我们今天的研讨会将使用以下数据集：\nNHS 患者评论 – nhs_reviews.Rdata 该文件包含了英国各地 2000 条关于 NHS 全科诊所的患者评论。数据中包含以下变量：\n\n\n\n\n\n\n\n变量名\n说明\n\n\n\n\nreview_title\n患者评论的标题\n\n\nreview_text\n患者评论的正文\n\n\nstar_rating\n患者给出的星级评分（满分为 5 星）\n\n\nreview_positive\n分类变量：若评分为 3 星及以上则为 \"Positive\"，否则为 \"Negative\"\n\n\nreview_date\n评论的日期\n\n\ngp_response\n分类变量：表示医生是否回复了该评论。若回复则为 \"Responded\"，否则为 \"Has not responded\"\n\n\n\n一旦你下载了这个文件并将其保存在一个合适的位置，你可以使用以下命令将其加载到 R 中：\nload(\"nhs_reviews.Rdata\")\n与以往一样，你可以使用 tidyverse 软件包中的 glimpse() 函数快速查看数据中的变量：\nglimpse(nhs_reviews)\nRows: 2,000\nColumns: 6\n$ review_title    &lt;chr&gt; \"Great service\", \"Friendly helpful staff\", \"Excellent …\n$ review_text     &lt;chr&gt; \"Phoned up to book a appointment receptionist very hel…\n$ star_rating     &lt;dbl&gt; 4, 5, 5, 1, 1, 5, 5, 5, 5, 1, 3, 5, 2, 5, 2, 5, 1, 1, …\n$ review_positive &lt;fct&gt; Positive, Positive, Positive, Negative, Negative, Posi…\n$ review_date     &lt;date&gt; 2021-10-13, 2021-07-26, 2021-09-18, 2021-06-19, 2021-…\n$ gp_response     &lt;chr&gt; \"Has not responded\", \"Responded\", \"Has not responded\",…"
  },
  {
    "objectID": "Seminar/Seminar3.html#随机数生成",
    "href": "Seminar/Seminar3.html#随机数生成",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "本次研讨课需要你随机生成训练集和测试集。任何涉及随机数生成的过程，都会导致你每次运行代码时可能得到不同的结果。例如，重新运行代码时，被分配到测试集和训练集的观测值可能会发生变化。\n因此，为了使结果完全可重复，你需要首先使用 set.seed() 函数。该函数的参数是一个单独的整数值。一旦你设置了seed，R 每次运行时都会生成相同的随机数序列，从而保证结果一致:\nset.seed(12345)\n我在这里使用了 12345，但你可以选择任何你喜欢的数值。如果你选择了不同的数字，那么你的结果可能会与我的略有不同。不过，只要你在每次运行 R 脚本时执行这一行代码，你每次都会得到相同的结果！"
  },
  {
    "objectID": "Seminar/Seminar3.html#训练集与测试集",
    "href": "Seminar/Seminar3.html#训练集与测试集",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "在 nhs_reviews 数据中创建一个新变量，用于指示每条观测属于训练集还是测试集。确保大约 80% 的观测被分配到训练集，20% 被分配到测试集，将该变量命名为 train：\n\n\n\nReveal Code\n\nnhs_reviews$train &lt;- sample(x = c(TRUE, FALSE), \n                            size = nrow(nhs_reviews), \n                            replace = TRUE, \n                            prob = c(.8, .2))\n\nsample() 函数会从提供给 x 参数的元素中随机抽样。在这里，我们是从一个只有两个元素的向量 TRUE 和 FALSE 中进行抽样。\n\n\n\nsize 参数指定从 x 中要随机选择的元素数量，这里我们要求抽样的数量与 nhs_reviews 数据中的观测值数量相同；\nreplace 参数指定是否放回抽样（这里我们要进行放回抽样）；\nprob 参数接受一个向量，表示我们希望以何种概率抽取 x 中的每个值。\n\n\n\n\n将 nhs_reviews 数据转换为语料库`corpus，然后转换为文档-特征矩阵DFM, 并进行一定的特征选择:\n\n\n\nReveal Code\n\n# 转换为语料库\nnhs_reviews_corpus &lt;- corpus(nhs_reviews, text_field = \"review_text\")\n\n# 转换为 DFM，并移除英文停用词\nnhs_reviews_dfm &lt;- nhs_reviews_corpus %&gt;% \n  tokens() %&gt;%\n  dfm() %&gt;%\n  dfm_remove(stopwords(\"en\"))\n\n请记住，特征选择没有唯一“正确”的方式。在这里，我只是使用了unigram（单词）表示，并移除了停用词。你可以根据实际任务做出不同的选择。\n\n\n\n使用 dfm_subset() 函数将你的 DFM 拆分为训练集 DFM 和 测试集 DFM。 使用 dim() 函数确认它们是否具有你预期的行数和列数:\n\n\n\nReveal Code\n\n# 子集为训练集观测\nnhs_reviews_dfm_train &lt;- dfm_subset(nhs_reviews_dfm, train)\n\n# 子集为测试集观测（使用逻辑非 !）\nnhs_reviews_dfm_test &lt;- dfm_subset(nhs_reviews_dfm, !train)\n\n这里我使用了逻辑运算符 !，它会反转 train 向量的 TRUE 和 FALSE 值。也就是说，train 中为 TRUE 的元素在加上 ! 后变为 FALSE，为 FALSE 的元素变为 TRUE。\n\ndim(nhs_reviews_dfm_train)\n[1] 1578 7862  \ndim(nhs_reviews_dfm_test)\n[1]  422 7862\n\n我们的 DFM 拆分结果中，训练集和测试集包含不同数量的文档（这是符合预期的，因为我们之前设定了 80% 用于训练，20% 用于测试），但它们包含相同数量的特征（这也是必须的，因为我们要使用这些特征来为测试集生成预测）。"
  },
  {
    "objectID": "Seminar/Seminar3.html#词典dictionaries",
    "href": "Seminar/Seminar3.html#词典dictionaries",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "使用 dictionary() 函数构建一个简单的字典，其中包含一些你认为能够体现患者评论中积极情绪的词汇。将这个字典应用到你之前创建的训练集 DFM上，并创建一个二元变量：当文本中包含来自该字典的词时设为 Positive，否则设为 Negative:\n\n\n\nReveal Code\n\npositive_dictionary &lt;- dictionary(list(positive = c(\"great\", \"excellent\", \"fantastic\", \"thank\")))\n\n# 将字典应用于训练集 DFM\nnhs_dictionary_dfm_train &lt;- dfm_lookup(x = nhs_reviews_dfm_train,\n                                       dictionary = positive_dictionary)\n\n# 创建一个逻辑（TRUE/FALSE）向量，对每条训练文本进行标记\npredicted_positive &lt;- as.numeric(nhs_dictionary_dfm_train[,1]) &gt; 0\n\n# 将逻辑向量转换为字符向量，TRUE 赋值为 \"Positive\"，FALSE 赋值为 \"Negative\"\n# 并将结果赋值给训练集数据\nnhs_dictionary_dfm_train$predicted_positive_dictionary &lt;- ifelse(predicted_positive, \"Positive\", \"Negative\")\n\n\n使用 table() 函数创建一个混淆矩阵(confusion matrix)，用于比较通过字典度量预测的正面评论 (predicted_positive_dictionary) 与正面评论的真实编码（存储在 review_positive 变量中）。将 table() 函数的输出保存为一个新对象:\n\n\n\nReveal Code\n\nconfusion_dictionary &lt;- table(predicted_classification = nhs_dictionary_dfm_train$predicted_positive_dictionary,\n                              true_classification = nhs_dictionary_dfm_train$review_positive)\n\nconfusion_dictionary\n                        true_classification\npredicted_classification Negative Positive\n                Negative      542      569\n                Positive       36      421\n\n\n使用 caret 包中的 confusionMatrix() 函数来计算关于你的字典分类器的多种性能统计量。（你还应设置 positive 参数为 “Positive”，以告知 R 哪个结果水平对应于“正面”结果。）预测的准确率是多少？灵敏度是多少？特异性是多少？解释这些数值。它们告诉我们这个字典在分类正面患者评论方面的性能如何？\n\n\n\nReveal Code\n\nconfusion_dictionary_statistics &lt;- confusionMatrix(confusion_dictionary, \n                                                   positive = \"Positive\")\n\nconfusion_dictionary_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      542      569\n                Positive       36      421\n                                          \n               Accuracy : 0.6142          \n                 95% CI : (0.5895, 0.6383)\n    No Information Rate : 0.6314          \n    P-Value [Acc &gt; NIR] : 0.9247          \n                                          \n                  Kappa : 0.3045          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.4253          \n            Specificity : 0.9377          \n         Pos Pred Value : 0.9212          \n         Neg Pred Value : 0.4878          \n             Prevalence : 0.6314          \n         Detection Rate : 0.2685          \n   Detection Prevalence : 0.2915          \n      Balanced Accuracy : 0.6815          \n                                          \n       'Positive' Class : Positive     \n\n我们的字典预测的准确率为 61%。虽然特异性很高——达到 94%——但这是因为字典将绝大多数评论都归为非正面评论，因此正确地为真实的非正面评论分配了该标签。灵敏度则低得多，仅为 43%。也就是说，我们只正确地识别了不到一半的真实“正面”评论。\n\n\n这是可以理解的，因为我们上面使用的字典非常简短，因此很可能漏掉了许多能表示正面情绪的词汇。\n\n\n我们可以尝试通过修改字典词列表并重新测试来提升这些分数，但接下来我们将使用朴素贝叶斯分类模型，让模型学习哪些词与正面或负面患者评论最密切相关。"
  },
  {
    "objectID": "Seminar/Seminar3.html#朴素贝叶斯naive-bayes",
    "href": "Seminar/Seminar3.html#朴素贝叶斯naive-bayes",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "使用你之前创建的训练集 DFM，通过 textmodel_nb() 函数来估计一个针对 review_positive 结果变量的朴素贝叶斯模型。\n\n该模型有三个主要参数：\n\n\n\n\n\n\n\n参数名\n说明\n\n\n\n\nx\n用于训练朴素贝叶斯模型的 DFM（文档-特征矩阵）\n\n\ny\n要预测的结果向量（例如，此处为 review_positive）\n\n\nprior\n对结果变量 y 的各类别的先验分布形式。通常设为 \"docfreq\"，即每个类别的先验概率将等于该类别在训练数据中的相对频率\n\n\n\n\n\nReveal Code\n\nnb_train &lt;- textmodel_nb(x = nhs_reviews_dfm_train, \n                         y = nhs_reviews_dfm_train$review_positive,\n                         prior = \"docfreq\")\n\n\n检查你所估计的朴素贝叶斯模型中的类别条件词概率（class-conditional word probabilities）。（此处使用 sort() 函数会很有帮助。）你可以使用 coef() 函数对估计得到的模型对象进行调用来访问这些概率。思考以下问题：在每个类别（正面 / 负面）下，哪些词的出现概率最高？请注意，如果你在研讨会一开始做了不同的特征选择决策，你得到的词列表可能会有所不同。特别是，如果你没有移除停用词（stopwords），你会发现两个类别下都对这些词赋予了高概率 —— 这也是合理的，因为这些词在所有患者评论中都很常见。\n\n\n\nReveal Code\n\nhead(sort(coef(nb_train)[,\"Positive\"], decreasing = T), 20)\n          .           ,       staff    practice     surgery appointment \n0.075829687 0.037050475 0.009988262 0.009241276 0.008921140 0.007619251 \n     always          gp     service           !        time     helpful \n0.006061253 0.006061253 0.005997225 0.005890513 0.005762459 0.005719774 \n      thank      doctor        care        well     doctors    friendly \n0.005463664 0.004652652 0.004545940 0.004481912 0.004183118 0.004076406 \n  reception         get \n0.003670900 0.003606872    \nhead(sort(coef(nb_train)[,\"Negative\"], decreasing = T), 20)\n           .            ,  appointment          get         call      surgery \n 0.067243057  0.032460640  0.012294357  0.011874226  0.009021758  0.008667964 \n           !         told        phone     practice         time           gp \n 0.008093048  0.007584468  0.007473908  0.006147178  0.006036618  0.005771272 \n      doctor          day         back          one          can         just \n 0.004908898  0.004555103  0.004333982  0.004245533  0.004134973  0.004134973 \nappointments            ? \n 0.004112860  0.004046524    \n\n这些词大体上是合理的。在正面和负面评论中，患者都可能使用与医疗相关的词汇，例如 “staff”（工作人员）、“practice”（诊所）、“surgery”（手术/诊所）、“appointment”（预约）、“doctor”（医生）等。但在 “Positive”（正面）类别中，语言模型还对许多具有正向情感的词赋予了更高的概率，如 “thank”（感谢）、“helpful”（有帮助的）、“friendly”（友好的）。\n\n\n相比之下，在 “Negative”（负面）类别中，模型对一些可能与不满意服务相关的词赋予了更高的概率，例如 “time”（时间）、“service”（服务）、“day”（日子），以及 “!”（感叹号）。\n\n\n\n估计来自朴素贝叶斯模型的正面评论预测概率。使用这些概率来检查 review_title 变量：哪些患者评论标题最有可能是正面评论？哪些最有可能是负面评论？这些看起来合理吗？\n\n\n\nReveal Code\n\nnhs_reviews_dfm_train$positive_nb_probability &lt;- predict(nb_train, type = \"probability\")[,2]\n\nnhs_reviews_dfm_train$review_title[order(nhs_reviews_dfm_train$positive_nb_probability, decreasing = T)[1:10]]\n [1] \"Good GP Practice\"                    \"Efficient and compassionate service\"\n [3] \"Fantastic practice\"                  \"Caring and professional service\"    \n [5] \"Nurse was amazing\"                   \"Fantastic Practice\"                 \n [7] \"Consistently good care\"              \"Exemplary Practice\"                 \n [9] \"Fantastic practice\"                  \"Fantastic Practice\"   \nnhs_reviews_dfm_train$review_title[order(nhs_reviews_dfm_train$positive_nb_probability, decreasing = F)[1:10]]\n [1] \"Shambles\"                                    \n [2] \"Unacceptable practice\"                       \n [3] \"Impossible to get a service\"                 \n [4] \"Unprofessional, Poor and Dangerous\"          \n [5] \"Impossible to get an appointment!!!\"         \n [6] \"Honestly, you’re better off not having a GP\" \n [7] \"Disappointed with appointments.\"             \n [8] \"Not so great practice!!\"                     \n [9] \"Very bad practice and arrogant receptionists\"\n[10] \"Extremely rude receptionist\"   \n\n是的！从这些列表可以非常清楚地看出，该模型在区分正面和负面评论方面表现得相当不错。但若要确切了解其表现如何，我们需要做的不仅仅是表面效度检验。\n\n\n\n使用你拟合好的朴素贝叶斯模型对训练数据中的每条观测进行类别预测（正面或负面）。然后使用 table() 函数创建一个混淆矩阵，用于比较预测出的正面评论与真实的正面评论编码（存储在 review_positive 变量中）。与字典分析一样，将 table() 函数的输出保存为一个新对象。\n\n\n\nReveal Code\n\n## 训练数据集准确率\nnhs_reviews_dfm_train$predicted_classification_nb &lt;- predict(nb_train, type = \"class\")\n\nconfusion_train &lt;- table(predicted_classification = nhs_reviews_dfm_train$predicted_classification_nb, \n                         true_classification = nhs_reviews_dfm_train$review_positive)\n\nconfusion_train\n                        true_classification\npredicted_classification Negative Positive\n                Negative      565       59\n                Positive       13      931  \n\n\n使用 confusionMatrix() 函数计算你的分类器的准确率（accuracy）、灵敏度（sensitivity） 和特异性（specificity）。将这些得分与你从字典分析中获得的结果进行比较。哪种方法表现更好？\n\n\n\nReveal Code\n\nconfusion_train_statistics &lt;- confusionMatrix(confusion_train, \n                                              positive = \"Positive\")\n\nconfusion_train_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      565       59\n                Positive       13      931\n                                          \n               Accuracy : 0.9541          \n                 95% CI : (0.9425, 0.9639)\n    No Information Rate : 0.6314          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.903           \n                                          \n Mcnemar's Test P-Value : 1.137e-07       \n                                          \n            Sensitivity : 0.9404          \n            Specificity : 0.9775          \n         Pos Pred Value : 0.9862          \n         Neg Pred Value : 0.9054          \n             Prevalence : 0.6314          \n         Detection Rate : 0.5938          \n   Detection Prevalence : 0.6020          \n      Balanced Accuracy : 0.9590          \n                                          \n       'Positive' Class : Positive   \n\n朴素贝叶斯模型显然优于字典分析。该分类器的灵敏度现在为 94%，准确率为 95%。使用朴素贝叶斯方法，我们在训练集中检测情感的表现要好得多。\n\n\n\n重复评估分类器的准确性，但现在要评估测试数据集。与训练数据集相比，分类器在这些数据上的表现如何？\n\n\n\nReveal Code\n\n## 测试数据集准确性\n\nnhs_reviews_dfm_test$predicted_classification_nb &lt;- predict(nb_train, \n                                                  newdata = nhs_reviews_dfm_test, \n                                                  type = \"class\")\n\nconfusion_test &lt;- table(predicted_classification = nhs_reviews_dfm_test$predicted_classification_nb, \n                        true_classification = nhs_reviews_dfm_test$review_positive)\n\nconfusion_test_statistics &lt;- confusionMatrix(confusion_test, \n                                             positive = \"Positive\")\n\nconfusion_test_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      170       35\n                Positive        6      207\n                                          \n               Accuracy : 0.9019          \n                 95% CI : (0.8693, 0.9287)\n    No Information Rate : 0.5789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8032          \n                                          \n Mcnemar's Test P-Value : 1.226e-05       \n                                          \n            Sensitivity : 0.8554          \n            Specificity : 0.9659          \n         Pos Pred Value : 0.9718          \n         Neg Pred Value : 0.8293          \n             Prevalence : 0.5789          \n         Detection Rate : 0.4952          \n   Detection Prevalence : 0.5096          \n      Balanced Accuracy : 0.9106          \n                                          \n       'Positive' Class : Positive \n\n测试数据集上的准确率为 90%，灵敏度为 86%，特异度为 97%。测试数据集上的表现比训练数据集上的表现要差一些，但仍明显优于字典分析。"
  },
  {
    "objectID": "Seminar/Seminar3.html#课后思考",
    "href": "Seminar/Seminar3.html#课后思考",
    "title": "研讨会3: 语言模型A：监督式文本分类学习",
    "section": "",
    "text": "从 nhs_reviews 数据中创建两个新的 DFM，每次都做出不同的特征选择决策（例如：词频修剪（trimming）、使用 n-gram、是否移除停用词等）。使用这些 DFM 训练两个新的朴素贝叶斯模型，并将它们在测试集数据上的表现与你之前创建的模型进行比较。 创建一个表格，比较你所估算模型的结果，和你为每个模型所做的特征选择决策。哪个模型在准确性、灵敏度和特异性方面表现最好？\n\n\n\nReveal Code\n\n## 无标点的 DFM\n\n# 转换为 DFM（移除标点符号）\nnhs_reviews_dfm_nopunct &lt;- nhs_reviews_corpus %&gt;% \n  tokens(remove_punct = T) %&gt;%\n  dfm() \n\n# 提取训练集中的观测值\nnhs_reviews_dfm_nopunct_train &lt;- dfm_subset(nhs_reviews_dfm_nopunct, train)\n\n# 提取测试集中的观测值\nnhs_reviews_dfm_nopunct_test &lt;- dfm_subset(nhs_reviews_dfm_nopunct, !train)\n\n# 训练一个新的朴素贝叶斯模型\nnb_train_nopunct &lt;- textmodel_nb(x = nhs_reviews_dfm_nopunct_train, \n                         y = nhs_reviews_dfm_nopunct_train$review_positive,\n                         prior = \"docfreq\")\n\n## 测试集准确率\n\n# 使用模型对测试集进行预测\nnhs_reviews_dfm_nopunct_test$predicted_classification &lt;- predict(nb_train_nopunct, \n                                                  newdata = nhs_reviews_dfm_nopunct_test, \n                                                  type = \"class\")\n\n# 创建混淆矩阵：预测类别 vs 真实类别\nconfusion_test_nopunct &lt;- table(predicted_classification = nhs_reviews_dfm_nopunct_test$predicted_classification, \n                        true_classification = nhs_reviews_dfm_nopunct_test$review_positive)\n\n# 计算混淆矩阵的性能统计（准确率、灵敏度、特异性等）\nconfusion_test_nopunct_statistics &lt;- confusionMatrix(confusion_test_nopunct, positive = \"Positive\")\n\n# 输出统计结果\nconfusion_test_nopunct_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      169       34\n                Positive        7      208\n                                          \n               Accuracy : 0.9019          \n                 95% CI : (0.8693, 0.9287)\n    No Information Rate : 0.5789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8029          \n                                          \n Mcnemar's Test P-Value : 4.896e-05       \n                                          \n            Sensitivity : 0.8595          \n            Specificity : 0.9602          \n         Pos Pred Value : 0.9674          \n         Neg Pred Value : 0.8325          \n             Prevalence : 0.5789          \n         Detection Rate : 0.4976          \n   Detection Prevalence : 0.5144          \n      Balanced Accuracy : 0.9099          \n                                          \n       'Positive' Class : Positive  \n## N-gram 模型\n\n# 转换为 DFM（使用 1 到 3 元的 n-gram 特征）\nnhs_reviews_dfm_ngram &lt;- nhs_reviews_corpus %&gt;% \n  tokens() %&gt;%\n  tokens_ngrams(1:3) %&gt;%\n  dfm()\n\n# 提取训练集中的观测值\nnhs_reviews_dfm_ngram_train &lt;- dfm_subset(nhs_reviews_dfm_ngram, train)\n\n# 提取测试集中的观测值\nnhs_reviews_dfm_ngram_test &lt;- dfm_subset(nhs_reviews_dfm_ngram, !train)\n\n# 训练一个新的朴素贝叶斯模型\nnb_train_ngram &lt;- textmodel_nb(x = nhs_reviews_dfm_ngram_train, \n                         y = nhs_reviews_dfm_ngram_train$review_positive,\n                         prior = \"docfreq\")\n\n## 测试集准确率\n\n# 使用模型对测试集进行预测\nnhs_reviews_dfm_ngram_test$predicted_classification &lt;- predict(nb_train_ngram, \n                                                  newdata = nhs_reviews_dfm_ngram_test, \n                                                  type = \"class\")\n\n# 创建混淆矩阵：预测类别 vs 真实类别\nconfusion_test_ngram &lt;- table(predicted_classification = nhs_reviews_dfm_ngram_test$predicted_classification, \n                        true_classification = nhs_reviews_dfm_ngram_test$review_positive)\n\n# 计算混淆矩阵的性能统计（准确率、灵敏度、特异性等）\nconfusion_test_ngram_statistics &lt;- confusionMatrix(confusion_test_ngram, positive = \"Positive\")\n\n# 输出统计结果\nconfusion_test_ngram_statistics\nConfusion Matrix and Statistics\n\n                        true_classification\npredicted_classification Negative Positive\n                Negative      169       35\n                Positive        7      207\n                                          \n               Accuracy : 0.8995          \n                 95% CI : (0.8666, 0.9266)\n    No Information Rate : 0.5789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7983          \n                                          \n Mcnemar's Test P-Value : 3.097e-05       \n                                          \n            Sensitivity : 0.8554          \n            Specificity : 0.9602          \n         Pos Pred Value : 0.9673          \n         Neg Pred Value : 0.8284          \n             Prevalence : 0.5789          \n         Detection Rate : 0.4952          \n   Detection Prevalence : 0.5120          \n      Balanced Accuracy : 0.9078          \n                                          \n       'Positive' Class : Positive       \naccuracy &lt;- c(confusion_test_statistics$overall[1],\n              confusion_test_nopunct_statistics$overall[1],\n              confusion_test_ngram_statistics$overall[1])\n\nsensitivity &lt;- c(confusion_test_statistics$byClass[1],\n                 confusion_test_nopunct_statistics$byClass[1],\n                 confusion_test_ngram_statistics$byClass[1])\n\nspecificity &lt;- c(confusion_test_statistics$byClass[2],\n                 confusion_test_nopunct_statistics$byClass[2],\n                 confusion_test_ngram_statistics$byClass[2])\n\nmodel_names &lt;- c(\"Unigram\", \"No punctuation\", \"N-gram\")\n\nresults &lt;- data.frame(model_names,\n                      accuracy,\n                      sensitivity,\n                      specificity)\n\nresults                                 \n     model_names  accuracy sensitivity specificity\n1        Unigram 0.9019139   0.8553719   0.9659091\n2 No punctuation 0.9019139   0.8595041   0.9602273\n3         N-gram 0.8995215   0.8553719   0.9602273  \n\n不同特征选择方法之间的差异很小。这主要是因为预测积极性是一项非常简单的任务！表示积极性的词语非常清晰，因此我们可以从这些数据中获得强烈的信号。需要注意的是，情况并非总是如此，尤其是在我们试图对一个更细微的概念或一组更精细的类别进行分类时。\n\n\n\n💪困难任务：在这个问题中，你应该实现 k 折交叉验证(k-fold cross-validation)，以评估你在上面估计的一个朴素贝叶斯模型的准确率、灵敏度和特异性。\n\n\n我在下面提供了一些起始代码，用于一个函数，该函数将接受一个逻辑值向量作为输入，该向量应命名为 held_out。该向量应对交叉验证中每一折的保留集观测为 TRUE，训练集观测为 FALSE。\n\n你的任务是填写代码中 “你的代码在这里！” 的部分。\n\n\nReveal Code\n\nget_performance_scores &lt;- function(held_out){\n  \n  # 为该折叠设置训练集和测试集\n  dfm_train &lt;- dfm_subset(nhs_reviews_dfm, !held_out)\n  dfm_test &lt;- dfm_subset(nhs_reviews_dfm, held_out)\n  \n  # 在非 held-out 部分训练模型\n  \n      # 你的代码在这里！\n  \n  # 对 held-out 部分进行预测\n  \n      # 你的代码在这里！\n  \n  # 创建混淆矩阵,计算准确性、特异性和灵敏度\n  \n      # 你的代码在这里！\n\n  # 将结果保存在 data.frame 中\n  \n  return(output)\n  \n}  \nget_performance_scores &lt;- function(held_out){\n  \n  # 为该折叠设置训练集和测试集\n  dfm_train &lt;- dfm_subset(nhs_reviews_dfm, !held_out)\n  dfm_test &lt;- dfm_subset(nhs_reviews_dfm, held_out)\n  \n    # 在非 held-out 部分训练模型\n  nb_train &lt;- textmodel_nb(x = dfm_train, \n                         y = dfm_train$review_positive,\n                         prior = \"docfreq\")\n  \n   # 对 held-out 部分进行预测\n  dfm_test$predicted_classification &lt;- predict(nb_train, \n                                             newdata = dfm_test, \n                                             type = \"class\")\n  \n  # 创建混淆矩阵,计算准确性、特异性和灵敏度\n  confusion_nb &lt;- table(predicted_classification = dfm_test$predicted_classification,\n                        true_classification = dfm_test$review_positive)\n  \n  confusion_nb_statistics &lt;- confusionMatrix(confusion_nb, positive = \"Positive\")\n  \n  accuracy &lt;- confusion_nb_statistics$overall[1]\n  sensitivity &lt;- confusion_nb_statistics$byClass[1]\n  specificity &lt;- confusion_nb_statistics$byClass[2]\n  \n  return(data.frame(accuracy, sensitivity, specificity))\n  \n}         \n\n\n完成这个函数之后，你需要创建一个向量，用来表示你将在交叉验证中测试数据所使用的各个折（K folds）。为此，你需要使用 sample() 函数生成一个向量，该向量的元素数量应与 nhs_reviews_dfm 对象中的行数相同。\n\n\n\nReveal Code\n\nK &lt;- 10\nfolds &lt;- sample(1:K, nrow(nhs_reviews_dfm), replace = T)\n\n\n最后，你需要将 get_performance_scores() 函数应用到每一个折（fold）上。为此，lapply() 函数可能会对你有所帮助。这个函数将一个向量作为输入，对该向量的每个值应用一个函数，并返回一个列表对象，其中包含你输入向量每个值所对应的函数运行结果。\n\n\n\nReveal Code\n\nall_folds &lt;- lapply(1:K, function(k) get_performance_scores(folds==k))\n\n\n输出你的结果！\n\n\n\nReveal Code\n\nall_folds &lt;- lapply(1:K, function(k) get_performance_scores(folds==k))\ncolMeans(bind_rows(all_folds))\n   accuracy sensitivity specificity \n  0.9122027   0.8881802   0.9504991"
  }
]